"""
Mod√®les de donn√©es pour le service Portfolio.
D√©finit les structures de donn√©es et les interactions avec la base de donn√©es.
"""

import contextlib
import logging
import os

# Importer les modules partag√©s
import sys
import time
from datetime import datetime
from typing import Any

import psycopg2
from psycopg2 import pool
from psycopg2.extras import RealDictCursor, execute_values

from shared.src.config import get_db_url
from shared.src.schemas import AssetBalance, PortfolioSummary

sys.path.append(
    os.path.abspath(
        os.path.join(
            os.path.dirname(__file__),
            "../../")))


# Configuration du logging
logger = logging.getLogger(__name__)


class DBManager:
    """
    Gestionnaire de connexion √† la base de donn√©es.
    Fournit des m√©thodes pour interagir avec la base de donn√©es PostgreSQL.
    Utilise un pool de connexions pour am√©liorer les performances.
    """

    # Singleton pattern pour le pool de connexions
    _pool = None

    @classmethod
    def get_pool(cls, db_url=None):
        """
        Obtient ou cr√©e le pool de connexions partag√©.

        Args:
            db_url: URL de connexion √† la base de donn√©es

        Returns:
            Pool de connexions
        """
        if cls._pool is None:
            db_url = db_url or get_db_url()
            try:
                # Cr√©er un pool avec min=5, max=20 connexions
                cls._pool = pool.ThreadedConnectionPool(
                    minconn=5, maxconn=20, dsn=db_url
                )
                logger.info("‚úÖ Pool de connexions DB initialis√©")
            except Exception:
                logger.exception("‚ùå Erreur lors de la cr√©ation du pool DB")
                raise
        return cls._pool

    def __init__(self, db_url: str | None = None):
        """
        Initialise le gestionnaire de base de donn√©es.

        Args:
            db_url: URL de connexion √† la base de donn√©es
        """
        self.db_url = db_url or get_db_url()
        self.conn = None
        self.pool = None

        # Utiliser le pool de connexions
        try:
            self.pool = self.get_pool(self.db_url)
            self.conn = self.pool.getconn()
            logger.debug(
                "‚úÖ Connexion obtenue depuis le pool DB"
            )  # Chang√© de INFO √† DEBUG
        except Exception:
            logger.exception("‚ùå Impossible d'obtenir une connexion du pool")
            # Fallback: connexion directe
            self._connect()

    def _connect(self) -> None:
        """
        √âtablit une connexion directe √† la base de donn√©es (fallback).
        """
        if self.conn is not None:
            return

        try:
            self.conn = psycopg2.connect(self.db_url)
            logger.info("‚úÖ Connexion directe √† la base de donn√©es √©tablie")
        except Exception:
            logger.exception(
                "‚ùå Erreur lors de la connexion √† la base de donn√©es: "
            )
            self.conn = None

    def _ensure_connection(self) -> bool:
        """
        S'assure que la connexion √† la base de donn√©es est active.
        Tente de r√©cup√©rer une nouvelle connexion du pool si n√©cessaire.

        Returns:
            True si la connexion est active, False sinon
        """
        if self.conn is None:
            if self.pool:
                try:
                    self.conn = self.pool.getconn()
                    return True
                except Exception:
                    logger.exception(
                        "‚ùå Impossible d'obtenir une connexion du pool: "
                    )
                    # Fallback: connexion directe
                    self._connect()
                    return self.conn is not None
            else:
                self._connect()
                return self.conn is not None

        try:
            # V√©rifier si la connexion est active avec un timeout
            with self.conn.cursor() as cursor:
                cursor.execute("SELECT 1")
                return True
        except Exception as e:
            # Reconnexion si n√©cessaire
            logger.warning(f"‚ö†Ô∏è Connexion √† la base de donn√©es perdue: {e!s}")
            try:
                if self.pool and self.conn:
                    # Marquer la connexion comme d√©fectueuse dans le pool
                    self.pool.putconn(self.conn, close=True)
                else:
                    with contextlib.suppress(Exception):
                        self.conn.close()
            except Exception:
                pass

            self.conn = None

            # Tenter d'obtenir une nouvelle connexion
            if self.pool:
                try:
                    self.conn = self.pool.getconn()
                    return True
                except Exception as e:
                    logger.exception(
                        "‚ùå Impossible d'obtenir une nouvelle connexion: "
                    )
                    # Fallback: connexion directe
                    self._connect()
            else:
                self._connect()

            return self.conn is not None

    def execute_query(
        self,
        query: str,
        params: tuple[Any, ...] | None = None,
        fetch_one: bool = False,
        fetch_all: bool = False,
        commit: bool = False,
        retry: int = 1,
    ) -> list[dict[str, Any]] | dict[str, Any] | bool | None:
        """
        Ex√©cute une requ√™te SQL avec retry.

        Args:
            query: Requ√™te SQL √† ex√©cuter
            params: Param√®tres de la requ√™te
            fetch_one: Si True, r√©cup√®re une seule ligne
            fetch_all: Si True, r√©cup√®re toutes les lignes
            commit: Si True, valide la transaction
            retry: Nombre de tentatives

        Returns:
            R√©sultat de la requ√™te ou True pour les mises √† jour r√©ussies, None en cas d'erreur
        """
        max_retries = max(1, retry)
        current_retry = 0
        last_error = None

        while current_retry < max_retries:
            if not self._ensure_connection():
                logger.error("‚ùå Pas de connexion √† la base de donn√©es")
                return None

            try:
                # Log plus l√©ger en production
                if len(query) > 200:
                    logger.debug(f"Ex√©cution de la requ√™te: {query[:200]}...")
                else:
                    logger.debug(f"Ex√©cution de la requ√™te: {query}")

                if params:
                    logger.debug(f"Param√®tres: {params}")

                if self.conn:
                    with self.conn.cursor(cursor_factory=RealDictCursor) as cursor:
                        cursor.execute(query, params)

                        if fetch_one:
                            result = cursor.fetchone()
                        elif fetch_all:
                            result = cursor.fetchall()
                        else:
                            # Pour les requ√™tes UPDATE/INSERT/DELETE, retourner
                            # True au lieu de None
                            result = True

                        if commit and self.conn:
                            self.conn.commit()

                        return result
                else:
                    return None

            except (psycopg2.InterfaceError, psycopg2.OperationalError) as e:
                # Erreurs de connexion, on peut r√©essayer
                current_retry += 1
                last_error = e
                logger.warning(
                    f"‚ö†Ô∏è Erreur de connexion DB (tentative {current_retry}/{max_retries}): {e!s}"
                )

                # Marquer la connexion comme d√©fectueuse
                try:
                    if self.pool and self.conn:
                        self.pool.putconn(self.conn, close=True)
                    else:
                        try:
                            if self.conn:
                                self.conn.close()
                        except Exception:
                            pass
                except Exception:
                    pass

                self.conn = None

                # Attendre avant de r√©essayer
                if current_retry < max_retries:
                    retry_delay = 2**current_retry  # Backoff exponentiel
                    time.sleep(retry_delay)

            except Exception:
                # Autres erreurs, rollback et log
                logger.exception("‚ùå Erreur lors de l'ex√©cution de la requ√™te")
                if len(query) > 200:
                    logger.exception(f"Query: {query[:200]}...")
                else:
                    logger.exception("Query: ")
                logger.exception("Params: ")
                import traceback

                logger.exception(traceback.format_exc())
                try:
                    if self.conn:
                        self.conn.rollback()
                except Exception:
                    pass
                return None

        if last_error:
            logger.error(
                f"‚ùå √âchec apr√®s {max_retries} tentatives: {last_error!s}")
        return None

    def execute_batch(
        self,
        query: str,
        params_list: list[tuple],
        page_size: int = 100,
        commit: bool = True,
    ) -> bool:
        """
        Ex√©cute une requ√™te SQL par lots pour am√©liorer les performances.

        Args:
            query: Requ√™te SQL avec placeholders
            params_list: Liste des param√®tres pour chaque ex√©cution
            page_size: Taille de chaque lot
            commit: Si True, valide la transaction

        Returns:
            True si l'ex√©cution a r√©ussi, False sinon
        """
        if not params_list:
            return True

        if not self._ensure_connection():
            logger.error("‚ùå Pas de connexion √† la base de donn√©es")
            return False

        try:
            if self.conn:
                with self.conn.cursor() as cursor:
                    # Utiliser execute_values pour les performances
                    execute_values(
                        cursor, query, params_list, page_size=page_size)

                    if commit and self.conn:
                        self.conn.commit()

                    return True
            else:
                return False

        except Exception:
            logger.exception("‚ùå Erreur lors de l'ex√©cution par lots")
            try:
                if self.conn:
                    self.conn.rollback()
            except Exception:
                pass
            return False

    def execute_many(
        self, query: str, params_list: list[tuple], commit: bool = True
    ) -> bool:
        """
        Ex√©cute une requ√™te SQL avec plusieurs ensembles de param√®tres.

        Args:
            query: Requ√™te SQL √† ex√©cuter
            params_list: Liste des param√®tres pour chaque ex√©cution
            commit: Si True, valide la transaction

        Returns:
            True si l'ex√©cution a r√©ussi, False sinon
        """
        if not params_list:
            return True

        # Pour les petites listes, utiliser executemany standard
        if len(params_list) < 50:
            if not self._ensure_connection():
                logger.error("‚ùå Pas de connexion √† la base de donn√©es")
                return False

            try:
                if self.conn:
                    with self.conn.cursor() as cursor:
                        cursor.executemany(query, params_list)

                        if commit and self.conn:
                            self.conn.commit()

                        return True
                else:
                    return False

            except Exception:
                logger.exception("‚ùå Erreur lors de l'ex√©cution multiple")
                try:
                    if self.conn:
                        self.conn.rollback()
                except Exception:
                    pass
                return False
        else:
            # Pour les grandes listes, utiliser execute_batch avec pagination
            return self.execute_batch(query, params_list, commit=commit)

    def close(self) -> None:
        """
        Ferme la connexion √† la base de donn√©es.
        Si un pool est utilis√©, retourne la connexion au pool.
        """
        if self.conn:
            try:
                if self.pool:
                    # Retourner la connexion au pool
                    self.pool.putconn(self.conn)
                    logger.debug("‚úÖ Connexion retourn√©e au pool DB")
                else:
                    # Fermer la connexion
                    self.conn.close()
                    logger.debug("‚úÖ Connexion √† la base de donn√©es ferm√©e")
            except Exception:
                logger.exception(
                    "‚ùå Erreur lors de la fermeture de la connexion: "
                )
            finally:
                self.conn = None


# Cache √† m√©moire partag√©e
class SharedCache:
    """Cache √† m√©moire partag√©e pour stocker des r√©sultats fr√©quemment demand√©s."""

    _cache: dict[str, tuple[float, Any]] = {}
    _locks: dict[str, Any] = {}

    @classmethod
    def get(cls, key: str, max_age: int = 5):
        """
        R√©cup√®re une valeur du cache si elle existe et n'est pas expir√©e.

        Args:
            key: Cl√© du cache
            max_age: √Çge maximum en secondes

        Returns:
            Valeur mise en cache ou None
        """
        current_time = time.time()

        if key in cls._cache:
            cache_time, cache_data = cls._cache[key]
            if current_time - cache_time < max_age:
                return cache_data

        return None

    @classmethod
    def set(cls, key: str, data: Any):
        """
        Stocke une valeur dans le cache.

        Args:
            key: Cl√© du cache
            data: Donn√©es √† mettre en cache
        """
        current_time = time.time()
        cls._cache[key] = (current_time, data)

    @classmethod
    def clear(cls, prefix: str | None = None):
        """
        Efface le cache ou une partie du cache.

        Args:
            prefix: Pr√©fixe des cl√©s √† effacer
        """
        if prefix:
            # Effacer les cl√©s commen√ßant par le pr√©fixe
            keys_to_remove = [k for k in cls._cache if k.startswith(prefix)]
            for k in keys_to_remove:
                del cls._cache[k]
        else:
            # Effacer tout le cache
            cls._cache.clear()


class PortfolioModel:
    """
    Mod√®le pour la gestion du portefeuille.
    Fournit des m√©thodes pour acc√©der et manipuler les donn√©es du portefeuille.
    """

    def __init__(self, db_manager: DBManager | None = None):
        """
        Initialise le mod√®le de portefeuille.

        Args:
            db_manager: Gestionnaire de base de donn√©es pr√©existant (optionnel)
        """
        self.db = db_manager or DBManager()

        logger.info("‚úÖ PortfolioModel initialis√©")

    def get_latest_balances(self) -> list[AssetBalance]:
        """
        R√©cup√®re les derniers soldes du portefeuille.
        Utilise un cache partag√© pour am√©liorer les performances.

        Returns:
            Liste des soldes par actif
        """
        # V√©rifier le cache partag√©
        cache_key = "latest_balances"
        cached_data = SharedCache.get(cache_key, max_age=5)

        if cached_data:
            return cached_data

        # Si pas en cache, ex√©cuter la requ√™te
        query = """
        WITH latest_balances AS (
            SELECT
                asset,
                MAX(timestamp) as latest_timestamp
            FROM
                portfolio_balances
            GROUP BY
                asset
        )
        SELECT
            pb.asset,
            pb.free,
            pb.locked,
            pb.total,
            pb.value_usdc
        FROM
            portfolio_balances pb
        JOIN
            latest_balances lb ON pb.asset = lb.asset AND pb.timestamp = lb.latest_timestamp
        ORDER BY
            pb.value_usdc DESC NULLS LAST,
            pb.total DESC
        """

        result = self.db.execute_query(query, fetch_all=True, retry=2)

        if not result or not isinstance(result, list):
            return []

        # Convertir en objets AssetBalance
        balances = []
        for row in result:
            balance = AssetBalance(
                asset=row["asset"],
                free=float(row["free"]),
                locked=float(row["locked"]),
                total=float(row["total"]),
                value_usdc=(
                    float(row["value_usdc"]) if row["value_usdc"] is not None else None
                ),
            )
            balances.append(balance)

        # Mettre en cache
        SharedCache.set(cache_key, balances)

        return balances

    def get_portfolio_summary(self) -> PortfolioSummary:
        """
        R√©cup√®re un r√©sum√© du portefeuille.
        Utilise un cache partag√© pour am√©liorer les performances.

        Returns:
            R√©sum√© du portefeuille
        """
        # V√©rifier le cache
        cache_key = "portfolio_summary"
        cached_data = SharedCache.get(cache_key, max_age=5)

        if cached_data:
            return cached_data

        try:
            # R√©cup√©rer les soldes r√©cents
            balances = self.get_latest_balances()

            if not balances:
                balances = []

            # Calculer la valeur totale
            total_value = sum(b.value_usdc or 0 for b in balances)

            # Obtenir les performances
            performance_24h = (
                self._calculate_performance(days=1)
                if hasattr(self, "_calculate_performance")
                else None
            )
            performance_7d = (
                self._calculate_performance(days=7)
                if hasattr(self, "_calculate_performance")
                else None
            )

            # Compter les trades actifs
            active_trades = (
                self._count_active_trades()
                if hasattr(self, "_count_active_trades")
                else 0
            )

            # Cr√©er le r√©sum√©
            summary = PortfolioSummary(
                balances=balances,
                total_value=total_value,
                performance_24h=performance_24h,
                performance_7d=performance_7d,
                active_trades=active_trades,
                timestamp=datetime.now(tz=timezone.utc),
            )

            # Mettre en cache
            SharedCache.set(cache_key, summary)

            return summary

        except Exception:
            logger.exception(
                "‚ùå Erreur lors de la r√©cup√©ration du r√©sum√© du portefeuille: "
            )
            import traceback

            logger.exception(traceback.format_exc())

            # Retourner un r√©sum√© vide en cas d'erreur
            return PortfolioSummary(
                balances=[],
                total_value=0,
                active_trades=0,
                timestamp=datetime.now(tz=timezone.utc))

    def update_balances(self, balances: list[AssetBalance | dict]) -> bool:
        """
        Met √† jour les soldes du portefeuille.

        Args:
            balances: Liste des nouveaux soldes (AssetBalance ou dictionnaires)

        Returns:
            True si la mise √† jour a r√©ussi, False sinon
        """
        if not balances:
            return False

        now = datetime.now(tz=timezone.utc)
        values = []
        assets_from_binance = set()

        # Pr√©parer les actifs pr√©sents sur Binance
        for balance in balances:
            # Accepter les objets AssetBalance ou les dictionnaires
            if isinstance(balance, dict):
                asset = balance.get("asset")
                free = float(balance.get("free", 0))
                locked = float(balance.get("locked", 0))
                total = float(balance.get("total", free + locked))
                value_usdc = balance.get("value_usdc")
                if value_usdc is not None:
                    value_usdc = float(value_usdc)
            else:
                asset = balance.asset
                free = balance.free
                locked = balance.locked
                total = balance.total
                value_usdc = balance.value_usdc

            assets_from_binance.add(asset)
            values.append((asset, free, locked, total, value_usdc, now))

        # NOUVEAU: R√©cup√©rer tous les actifs connus (ayant eu une balance > 0
        # r√©cemment)
        known_assets_query = """
        SELECT DISTINCT asset
        FROM portfolio_balances
        WHERE total > 0
        AND timestamp > NOW() - INTERVAL '7 days'
        """
        known_assets_result = self.db.execute_query(
            known_assets_query, fetch_all=True)

        if known_assets_result and isinstance(known_assets_result, list):
            # Pour chaque actif connu mais absent de Binance, ajouter une
            # entr√©e √† 0
            for row in known_assets_result:
                asset = row["asset"]
                if asset not in assets_from_binance:
                    # Ajouter cet actif avec une valeur de 0
                    values.append((asset, 0.0, 0.0, 0.0, 0.0, now))
                    logger.info(f"üìâ Actif {asset} absent de Binance, mis √† 0")

        # Ins√©rer toutes les valeurs (pr√©sentes et √† 0)
        success = self.db.execute_batch(
            "INSERT INTO portfolio_balances (asset, free, locked, total, value_usdc, timestamp) VALUES %s",
            values,
        )

        if success:
            logger.info(
                f"‚úÖ Soldes mis √† jour: {len(assets_from_binance)} actifs actifs, {len(values) - len(assets_from_binance)} mis √† 0"
            )
            # Invalider le cache
            SharedCache.clear("latest_balances")
            SharedCache.clear("portfolio_summary")

        # Nettoyer les anciens enregistrements (garder seulement les 24
        # derni√®res heures)
        self._cleanup_old_records()

        return success

    def get_trades_history(
        self,
        limit: int = 50,
        offset: int = 0,
        symbol: str | None = None,
        strategy: str | None = None,
        start_date: datetime | None = None,
        end_date: datetime | None = None,
    ) -> list[dict[str, Any]]:
        """
        R√©cup√®re l'historique des trades.

        Args:
            limit: Nombre maximum de r√©sultats
            offset: D√©calage pour la pagination
            symbol: Filtrer par symbole
            strategy: Filtrer par strat√©gie
            start_date: Date de d√©but
            end_date: Date de fin

        Returns:
            Liste des trades
        """
        # Construire la requ√™te avec les filtres
        query = """
        SELECT
            tc.id,
            tc.symbol,
            tc.strategy,
            tc.status,
            tc.entry_price,
            tc.exit_price,
            tc.quantity,
            tc.profit_loss,
            tc.profit_loss_percent,
            tc.created_at,
            tc.completed_at,
            tc.demo
        FROM
            trade_cycles tc
        WHERE 1=1
        """

        params: list[Any] = []

        # Ajouter les filtres si sp√©cifi√©s
        if symbol:
            query += " AND tc.symbol = %s"
            params.append(symbol)

        if strategy:
            query += " AND tc.strategy = %s"
            params.append(strategy)

        if start_date:
            query += " AND tc.created_at >= %s"
            params.append(start_date)

        if end_date:
            query += " AND tc.created_at <= %s"
            params.append(end_date)

        # Ajouter l'ordre et la pagination
        query += " ORDER BY tc.created_at DESC LIMIT %s OFFSET %s"
        params.extend([limit, offset])

        # Ex√©cuter la requ√™te
        result = self.db.execute_query(query, tuple(params), fetch_all=True)

        return result if isinstance(result, list) else []

    def get_performance_stats(
        self, period: str = "daily", limit: int = 30
    ) -> list[dict[str, Any]]:
        """
        R√©cup√®re les statistiques de performance.

        Args:
            period: P√©riode ('daily', 'weekly', 'monthly')
            limit: Nombre de p√©riodes √† r√©cup√©rer

        Returns:
            Liste des statistiques de performance
        """
        # V√©rifier si c'est en cache
        cache_key = f"performance_stats_{period}_{limit}"
        cached_data = SharedCache.get(
            cache_key, max_age=30
        )  # Cache plus long pour les stats

        if cached_data:
            return cached_data

        query = """
        SELECT
            symbol,
            strategy,
            period,
            start_date,
            end_date,
            total_trades,
            winning_trades,
            losing_trades,
            break_even_trades,
            profit_loss,
            profit_loss_percent
        FROM
            performance_stats
        WHERE
            period = %s
        ORDER BY
            start_date DESC
        LIMIT %s
        """

        result = self.db.execute_query(query, (period, limit), fetch_all=True)

        if result:
            SharedCache.set(cache_key, result)

        return result if isinstance(result, list) else []

    def get_strategy_performance(self) -> list[dict[str, Any]]:
        """
        R√©cup√®re les performances par strat√©gie.

        Returns:
            Liste des performances par strat√©gie
        """
        # V√©rifier si c'est en cache
        cache_key = "strategy_performance"
        cached_data = SharedCache.get(cache_key, max_age=30)

        if cached_data:
            return cached_data

        query = """
        SELECT * FROM strategy_performance
        """

        result = self.db.execute_query(query, fetch_all=True)

        if result:
            SharedCache.set(cache_key, result)

        return result if isinstance(result, list) else []

    def get_symbol_performance(self) -> list[dict[str, Any]]:
        """
        R√©cup√®re les performances par symbole.

        Returns:
            Liste des performances par symbole
        """
        # V√©rifier si c'est en cache
        cache_key = "symbol_performance"
        cached_data = SharedCache.get(cache_key, max_age=30)

        if cached_data:
            return cached_data

        query = """
        SELECT * FROM symbol_performance
        """

        result = self.db.execute_query(query, fetch_all=True)

        if result:
            SharedCache.set(cache_key, result)

        return result if isinstance(result, list) else []

    def _cleanup_old_records(self) -> None:
        """
        Nettoie les anciens enregistrements de portfolio_balances.
        Garde seulement les 24 derni√®res heures de donn√©es pour chaque actif,
        mais conserve toujours au moins un enregistrement par actif.
        """
        try:
            # Compter d'abord combien d'enregistrements vont √™tre supprim√©s
            count_query = """
            SELECT COUNT(*) as count_to_delete
            FROM portfolio_balances
            WHERE timestamp < NOW() - INTERVAL '24 hours'
            AND (asset, timestamp) NOT IN (
                SELECT asset, MAX(timestamp)
                FROM portfolio_balances
                GROUP BY asset
            )
            """
            count_result = self.db.execute_query(count_query, fetch_one=True)
            count_to_delete = (
                count_result.get("count_to_delete", 0)
                if count_result and isinstance(count_result, dict)
                else 0
            )

            if count_to_delete > 0:
                # Maintenant supprimer les enregistrements
                cleanup_query = """
                DELETE FROM portfolio_balances
                WHERE timestamp < NOW() - INTERVAL '24 hours'
                AND (asset, timestamp) NOT IN (
                    SELECT asset, MAX(timestamp)
                    FROM portfolio_balances
                    GROUP BY asset
                )
                """

                result = self.db.execute_query(cleanup_query, commit=True)
                if result:
                    logger.info(
                        f"üßπ Nettoyage: {count_to_delete} anciens enregistrements supprim√©s"
                    )

        except Exception as e:
            logger.warning(
                f"‚ö†Ô∏è Erreur lors du nettoyage des anciens enregistrements: {e}"
            )

    def get_strategy_configs(
            self, name: str | None = None) -> list[dict[str, Any]]:
        """
        R√©cup√®re les configurations de strat√©gie.

        Args:
            name: Filtrer par nom de strat√©gie

        Returns:
            Liste des configurations de strat√©gie
        """
        query = """
        SELECT name, mode, symbols, params, max_simultaneous_trades, enabled
        FROM strategy_configs
        WHERE 1=1
        """
        params: list[Any] = []

        if name:
            query += " AND name = %s"
            params.append(name)

        result = self.db.execute_query(query, tuple(params), fetch_all=True)
        return result if isinstance(result, list) else []

    def close(self) -> None:
        """
        Ferme la connexion √† la base de donn√©es.
        """
        if self.db:
            self.db.close()
