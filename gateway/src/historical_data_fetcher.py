"""
Module de r√©cup√©ration des donn√©es historiques de Binance.
Permet d'initialiser le syst√®me avec suffisamment de donn√©es historiques.
"""
import logging
import time
from typing import List, Dict, Any
import requests
from datetime import datetime, timedelta
import asyncio

# Configuration du logging
logger = logging.getLogger(__name__)

class HistoricalDataFetcher:
    """
    R√©cup√®re les donn√©es historiques de Binance pour initialiser l'Analyzer.
    """
    
    def __init__(self, kafka_producer=None):
        """
        Initialise le r√©cup√©rateur de donn√©es historiques.
        
        Args:
            kafka_producer: Producteur Kafka pour publier les donn√©es
        """
        self.kafka_producer = kafka_producer
        self.base_url = "https://api.binance.com/api/v3"
        
        # Limites de l'API
        self.rate_limit_per_second = 20
        self.last_request_time = 0
        
        logger.info("‚úÖ HistoricalDataFetcher initialis√©")
    
    async def _respect_rate_limit(self):
        """
        Respecte les limites de taux de l'API Binance.
        """
        current_time = time.time()
        elapsed = current_time - self.last_request_time
        
        # S'assurer d'au moins 50ms entre les requ√™tes (20 requ√™tes/s)
        if elapsed < 0.05:
            await asyncio.sleep(0.05 - elapsed)
        
        self.last_request_time = time.time()
    
    async def get_klines(self, symbol: str, interval: str, limit: int = 1000, start_time: int = None) -> List[Dict[str, Any]]:
        """
        R√©cup√®re les chandeliers historiques de Binance.
        
        Args:
            symbol: Symbole (ex: 'BTCUSDC')
            interval: Intervalle (ex: '1m', '5m', '1h')
            limit: Nombre max de chandeliers √† r√©cup√©rer (max 1000)
            start_time: Timestamp de d√©but en millisecondes
            
        Returns:
            Liste des chandeliers
        """
        await self._respect_rate_limit()
        
        url = f"{self.base_url}/klines"
        params = {
            "symbol": symbol,
            "interval": interval,
            "limit": min(limit, 1000)  # Max 1000 par requ√™te
        }
        
        if start_time:
            params["startTime"] = start_time
        
        try:
            response = requests.get(url, params=params)
            response.raise_for_status()
            
            # Formater les donn√©es
            klines = []
            for k in response.json():
                kline = {
                    "symbol": symbol,
                    "start_time": k[0],  # Open time
                    "close_time": k[6],  # Close time
                    "open": float(k[1]),
                    "high": float(k[2]),
                    "low": float(k[3]),
                    "close": float(k[4]),
                    "volume": float(k[5]),
                    "is_closed": True,  # Toutes ces donn√©es sont des chandeliers ferm√©s
                    "timestamp": datetime.fromtimestamp(k[0] / 1000).isoformat()
                }
                klines.append(kline)
            
            return klines
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la r√©cup√©ration des chandeliers: {str(e)}")
            return []
    
    async def fetch_and_publish_history(self, symbols: List[str], interval: str, days_back: int = 3) -> int:
        """
        R√©cup√®re l'historique et publie les donn√©es via Kafka.
        
        Args:
            symbols: Liste des symboles
            interval: Intervalle des chandeliers
            days_back: Nombre de jours d'historique √† r√©cup√©rer
            
        Returns:
            Nombre total de chandeliers r√©cup√©r√©s
        """
        if not self.kafka_producer:
            logger.error("‚ùå Kafka Producer non configur√©")
            return 0
        
        total_klines = 0
        
        # Calculer le timestamp de d√©but
        start_time = int((datetime.now() - timedelta(days=days_back)).timestamp() * 1000)
        
        for symbol in symbols:
            logger.info(f"üìà R√©cup√©ration de l'historique pour {symbol} @ {interval} (depuis {days_back} jours)...")
            
            # R√©cup√©rer les donn√©es par lots
            current_start = start_time
            all_klines = []
            
            while True:
                klines = await self.get_klines(symbol, interval, 1000, current_start)
                
                if not klines:
                    break
                
                all_klines.extend(klines)
                
                # Mettre √† jour le timestamp de d√©but pour la prochaine requ√™te
                current_start = klines[-1]["close_time"] + 1
                
                # Si on a atteint le pr√©sent, arr√™ter
                if current_start > int(time.time() * 1000):
                    break
                
                # Petite pause pour respecter les limites
                await asyncio.sleep(0.1)
            
            # Publier les donn√©es
            logger.info(f"üìä Publication de {len(all_klines)} chandeliers pour {symbol}...")
            
            for kline in all_klines:
                # Publier via Kafka
                topic = f"market.data.{symbol.lower()}"
                self.kafka_producer.publish_market_data(kline)
            
            total_klines += len(all_klines)
            
            # Pause entre les symboles
            await asyncio.sleep(1)
        
        logger.info(f"‚úÖ Total: {total_klines} chandeliers historiques r√©cup√©r√©s et publi√©s")
        return total_klines