"""
Simple Data Fetcher - Service de r√©cup√©ration de donn√©es OHLCV brutes depuis Binance
Ne fait AUCUN calcul d'indicateur - transmet uniquement les donn√©es brutes
"""

import asyncio
import logging
import os
import sys
from datetime import datetime

import aiohttp
from aiohttp import ClientTimeout

from gateway.src.kafka_producer import get_producer
from shared.src.config import SYMBOLS
from shared.src.redis_client import RedisClient

# Ajouter le r√©pertoire parent au path pour les imports
sys.path.append(
    os.path.abspath(
        os.path.join(
            os.path.dirname(__file__),
            "../../")))


logger = logging.getLogger(__name__)


class SimpleDataFetcher:
    """
    R√©cup√©rateur de donn√©es OHLCV brutes multi-timeframes depuis Binance.
    Architecture propre : AUCUN calcul d'indicateur, transmission de donn√©es brutes uniquement.
    """

    def __init__(self):
        self.symbols = SYMBOLS
        self.timeframes = ["1m", "3m", "5m", "15m", "1h", "1d"]
        self.redis_client = RedisClient()
        self.kafka_producer = get_producer()
        self.running = False

        # URLs Binance API
        self.base_url = "https://api.binance.com"
        self.klines_endpoint = "/api/v3/klines"

        # Configuration des timeouts
        self.timeout = ClientTimeout(total=30)

        # Limits de r√©cup√©ration par timeframe (optimis√©es pour EMA 99 stable)
        self.limits = {
            "1m": 300,  # 300 minutes = 5h (suffisant pour EMA 99)
            "3m": 300,  # 900 minutes = 15h (bon √©quilibre)
            "5m": 300,  # 1500 minutes = 25h (journ√©e + 1h)
            "15m": 300,  # 4500 minutes = 75h (3+ jours)
            "1h": 300,  # 18000 minutes = 300h (12+ jours)
            "1d": 300,  # 300 jours = 10 mois (EMA 99 tr√®s stable)
        }

        logger.info(
            "üì° SimpleDataFetcher initialis√© - donn√©es brutes uniquement")

    async def start(self):
        """D√©marre le service de r√©cup√©ration de donn√©es."""
        self.running = True
        logger.info("üöÄ SimpleDataFetcher d√©marr√©")

        try:
            # R√©cup√©ration initiale des donn√©es historiques pour tous les
            # symboles/timeframes
            await self._fetch_initial_data()

            # Ensuite, lancer la surveillance en continu
            await self._continuous_fetch()

        except Exception:
            logger.exception("‚ùå Erreur dans SimpleDataFetcher")
        finally:
            self.running = False

    async def _fetch_initial_data(self):
        """R√©cup√®re les donn√©es historiques initiales pour tous les symboles/timeframes."""
        logger.info("üìö R√©cup√©ration des donn√©es historiques initiales...")

        tasks = []
        for symbol in self.symbols:
            for timeframe in self.timeframes:
                task = self._fetch_symbol_timeframe_data(symbol, timeframe)
                tasks.append(task)

        # Ex√©cuter toutes les t√¢ches en parall√®le
        results = await asyncio.gather(*tasks, return_exceptions=True)

        success_count = sum(1 for r in results if not isinstance(r, Exception))
        total_count = len(tasks)

        logger.info(
            f"‚úÖ Donn√©es initiales r√©cup√©r√©es: {success_count}/{total_count}")

    async def _fetch_symbol_timeframe_data(self, symbol: str, timeframe: str):
        """R√©cup√®re les donn√©es pour un symbole/timeframe sp√©cifique."""
        try:
            limit = self.limits.get(timeframe, 200)

            # URL de requ√™te Binance
            url = f"{self.base_url}{self.klines_endpoint}"
            params = {
                "symbol": symbol,
                "interval": timeframe,
                "limit": str(limit)}

            async with aiohttp.ClientSession(timeout=self.timeout) as session:
                async with session.get(url, params=params) as response:
                    if response.status == 200:
                        klines = await response.json()

                        # Traiter les donn√©es OHLCV brutes
                        processed_data = self._process_raw_klines(
                            klines, symbol, timeframe
                        )

                        # Publier sur Kafka via Redis
                        await self._publish_to_kafka(processed_data, symbol, timeframe)

                        logger.debug(
                            f"‚úÖ Donn√©es r√©cup√©r√©es: {symbol} {timeframe} ({len(klines)} bougies)"
                        )
                        return True
                    logger.error(
                        f"‚ùå Erreur API Binance {response.status} pour {symbol} {timeframe}"
                    )
                    return False

        except Exception:
            logger.exception("‚ùå Erreur r√©cup√©ration {symbol} {timeframe}")
            return False

    def _process_raw_klines(
        self, klines: list, symbol: str, timeframe: str
    ) -> list[dict]:
        """
        Traite les klines brutes pour en extraire uniquement les donn√©es OHLCV.
        AUCUN calcul d'indicateur technique.
        """
        processed_candles = []

        for kline in klines:
            # Extraire uniquement les donn√©es OHLCV
            candle_data = {
                "time": datetime.fromtimestamp(kline[0] / 1000, tz=timezone.utc).isoformat(),
                "symbol": symbol,
                "timeframe": timeframe,
                "open": float(kline[1]),
                "high": float(kline[2]),
                "low": float(kline[3]),
                "close": float(kline[4]),
                "volume": float(kline[5]),
                "close_time": kline[6],
                "quote_asset_volume": float(kline[7]),
                "number_of_trades": kline[8],
                "taker_buy_base_asset_volume": float(kline[9]),
                "taker_buy_quote_asset_volume": float(kline[10]),
                "is_closed": True,  # Donn√©es historiques sont ferm√©es
                "source": "binance_historical",
            }

            processed_candles.append(candle_data)

        return processed_candles

    async def _publish_to_kafka(
            self,
            candles: list[dict],
            symbol: str,
            timeframe: str):
        """Publie les donn√©es brutes sur Kafka via KafkaProducer."""
        try:
            for candle in candles:
                # Utiliser le KafkaProducer pour publier
                self.kafka_producer.publish_market_data(candle, key=symbol)

                logger.debug(
                    f"üì§ Donn√©es historiques publi√©es: {symbol} {timeframe}")

        except Exception:
            logger.exception("‚ùå Erreur publication Kafka")

    async def _continuous_fetch(self):
        """Surveillance continue des nouvelles donn√©es."""
        logger.info("üîÑ D√©marrage de la surveillance continue...")

        while self.running:
            try:
                # Attendre 60 secondes entre les v√©rifications (moins de
                # pression sur l'API)
                await asyncio.sleep(60)

                # R√©cup√©rer les derni√®res donn√©es pour tous les symboles avec
                # un petit d√©lai
                for symbol in self.symbols:
                    for timeframe in self.timeframes:
                        await self._fetch_latest_data(symbol, timeframe)
                        # Petit d√©lai entre chaque requ√™te pour √©viter les rate
                        # limits
                        await asyncio.sleep(0.1)

            except Exception:
                logger.exception("‚ùå Erreur surveillance continue")
                # Attendre plus longtemps en cas d'erreur
                await asyncio.sleep(60)

    async def _fetch_latest_data(self, symbol: str, timeframe: str):
        """R√©cup√®re uniquement les derni√®res donn√©es pour un symbole/timeframe."""
        try:
            # R√©cup√©rer les 5 derni√®res bougies pour √©viter les gaps
            url = f"{self.base_url}{self.klines_endpoint}"
            params = {"symbol": symbol, "interval": timeframe, "limit": "5"}

            async with aiohttp.ClientSession(timeout=self.timeout) as session:
                async with session.get(url, params=params) as response:
                    if response.status == 200:
                        klines = await response.json()

                        # Traiter toutes les bougies ferm√©es (pas celle en
                        # cours)
                        if len(klines) >= 2:
                            closed_klines = klines[
                                :-1
                            ]  # Toutes sauf la derni√®re (en cours)

                            processed_data = self._process_raw_klines(
                                closed_klines, symbol, timeframe
                            )
                            await self._publish_to_kafka(
                                processed_data, symbol, timeframe
                            )

        except Exception:
            logger.exception(
                "‚ùå Erreur r√©cup√©ration latest {symbol} {timeframe}")

    async def _fetch_period_data(
        self, symbol: str, timeframe: str, start_time, end_time
    ):
        """R√©cup√®re les donn√©es pour une p√©riode sp√©cifique (utilis√© par SmartDataFetcher)."""
        try:
            # Convertir les timestamps en millisecondes pour Binance API
            start_ts = int(start_time.timestamp() * 1000)
            end_ts = int(end_time.timestamp() * 1000)

            url = f"{self.base_url}{self.klines_endpoint}"
            params = {
                "symbol": symbol,
                "interval": timeframe,
                "startTime": str(start_ts),
                "endTime": str(end_ts),
                "limit": "1000",
            }

            async with aiohttp.ClientSession(timeout=self.timeout) as session:
                async with session.get(url, params=params) as response:
                    if response.status == 200:
                        klines = await response.json()

                        if klines:
                            processed_data = self._process_raw_klines(
                                klines, symbol, timeframe
                            )
                            await self._publish_to_kafka(
                                processed_data, symbol, timeframe
                            )

                            logger.debug(
                                f"‚úÖ P√©riode remplie: {symbol} {timeframe} ({len(klines)} bougies)"
                            )
                            return True
                    else:
                        logger.error(
                            f"‚ùå Erreur API Binance {response.status} pour {symbol} {timeframe}"
                        )
                        return False

        except Exception:
            logger.exception("‚ùå Erreur fetch p√©riode {symbol} {timeframe}")
            return False

    async def stop(self):
        """Arr√™te le service."""
        self.running = False
        logger.info("üõë SimpleDataFetcher arr√™t√©")


async def main():
    """Point d'entr√©e principal."""
    fetcher = SimpleDataFetcher()

    try:
        await fetcher.start()
    except KeyboardInterrupt:
        logger.info("üõë Arr√™t demand√© par l'utilisateur")
    finally:
        await fetcher.stop()


if __name__ == "__main__":
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    )

    asyncio.run(main())
