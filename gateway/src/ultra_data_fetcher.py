"""
Ultra Data Fetcher - Service de r√©cup√©ration de donn√©es enrichies multi-timeframes
Compatible avec le syst√®me UltraConfluence pour signaux de qualit√© institutionnelle
"""

import asyncio
import logging
import json
import time
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
import aiohttp
import sys
import os

# Ajouter le r√©pertoire parent au path pour les imports
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "../../")))

from shared.src.config import SYMBOLS
from shared.src.redis_client import RedisClient
from shared.src.technical_indicators import indicators

logger = logging.getLogger(__name__)

class UltraDataFetcher:
    """
    R√©cup√©rateur ultra-avanc√© de donn√©es multi-timeframes avec 20+ indicateurs techniques.
    Fournit les donn√©es n√©cessaires pour l'UltraConfluence et le scoring institutionnel.
    """
    
    def __init__(self):
        self.symbols = SYMBOLS
        self.timeframes = ['1m', '5m', '15m', '1h', '4h']  # Multi-timeframes pour confluence
        self.redis_client = RedisClient()
        self.running = False
        
        # URLs Binance API
        self.base_url = "https://api.binance.com"
        self.klines_endpoint = "/api/v3/klines"
        self.depth_endpoint = "/api/v3/depth"
        self.ticker_endpoint = "/api/v3/ticker/24hr"
        
        # Buffers pour calculs techniques en temps r√©el (comme dans binance_ws.py)
        self.price_buffers = {}
        self.volume_buffers = {}
        
        # Initialiser les buffers pour chaque symbole/timeframe
        for symbol in self.symbols:
            self.price_buffers[symbol] = {}
            self.volume_buffers[symbol] = {}
            for tf in self.timeframes:
                self.price_buffers[symbol][tf] = []
                self.volume_buffers[symbol][tf] = []
        
        # Limites API
        self.rate_limit_delay = 0.1  # 100ms entre requ√™tes
        self.last_request_time = 0
        
        logger.info(f"üî• UltraDataFetcher initialis√© pour {len(self.symbols)} symboles x {len(self.timeframes)} timeframes")
    
    async def start(self):
        """D√©marre la r√©cup√©ration ultra-enrichie p√©riodique"""
        self.running = True
        logger.info("üöÄ D√©marrage de l'UltraDataFetcher")
        
        while self.running:
            try:
                # R√©cup√©ration multi-timeframes pour tous les symboles
                for symbol in self.symbols:
                    # Donn√©es klines multi-timeframes
                    for timeframe in self.timeframes:
                        await self._fetch_ultra_enriched_data(symbol, timeframe)
                        await self._respect_rate_limit()
                    
                    # Donn√©es orderbook pour sentiment
                    await self._fetch_orderbook_sentiment(symbol)
                    await self._respect_rate_limit()
                    
                    # Donn√©es ticker 24h pour contexte
                    await self._fetch_ticker_data(symbol)
                    await self._respect_rate_limit()
                
                # Cycle complet toutes les 60 secondes (align√© avec bougies 1min)
                logger.info("‚úÖ Cycle ultra-enrichi termin√©, pause 60s")
                await asyncio.sleep(60)
                
            except Exception as e:
                logger.error(f"‚ùå Erreur dans la boucle ultra-enrichie: {e}")
                await asyncio.sleep(30)
    
    async def stop(self):
        """Arr√™te la r√©cup√©ration"""
        self.running = False
        logger.info("‚èπÔ∏è Arr√™t de l'UltraDataFetcher")
    
    async def _fetch_initialization_data(self):
        """
        R√©cup√®re les donn√©es initiales pour remplir les caches Redis.
        Utilis√© au d√©marrage pour √©viter d'attendre le premier cycle complet.
        """
        logger.info("üîÑ Initialisation des donn√©es ultra-enrichies...")
        
        try:
            for symbol in self.symbols:
                logger.info(f"üìä Initialisation {symbol}...")
                
                # Donn√©es multi-timeframes
                for timeframe in self.timeframes:
                    await self._fetch_ultra_enriched_data(symbol, timeframe)
                    await self._respect_rate_limit()
                
                # Donn√©es de sentiment et ticker
                await self._fetch_orderbook_sentiment(symbol)
                await self._respect_rate_limit()
                
                await self._fetch_ticker_data(symbol)
                await self._respect_rate_limit()
            
            logger.info("‚úÖ Initialisation ultra-enrichie termin√©e")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de l'initialisation: {e}")
            raise
    
    async def _respect_rate_limit(self):
        """Respecte les limites de taux API"""
        current_time = time.time()
        elapsed = current_time - self.last_request_time
        
        if elapsed < self.rate_limit_delay:
            await asyncio.sleep(self.rate_limit_delay - elapsed)
        
        self.last_request_time = time.time()
    
    async def _fetch_ultra_enriched_data(self, symbol: str, timeframe: str):
        """
        R√©cup√®re et traite les donn√©es ultra-enrichies pour un symbole/timeframe.
        Calcule TOUS les indicateurs n√©cessaires pour UltraConfluence.
        """
        try:
            # R√©cup√©rer BEAUCOUP plus de klines pour indicateurs ultra-pr√©cis
            timeframe_limits = {
                '1m': 1000,   # 16.7 heures ‚Üí EMA/SMA 200 ultra-pr√©cis
                '5m': 1000,   # 3.5 jours ‚Üí Tendances moyennes parfaites
                '15m': 800,   # 8.3 jours ‚Üí Indicateurs long terme solides
                '1h': 500,    # 20.8 jours ‚Üí Tendance mensuelle
                '4h': 300     # 50 jours ‚Üí Tendance long terme
            }
            limit = timeframe_limits.get(timeframe, 100)
            klines = await self._fetch_klines(symbol, timeframe, limit=limit)
            
            if not klines:
                logger.warning(f"Aucune kline pour {symbol} {timeframe}")
                return
            
            # Traitement ultra-enrichi avec TOUS les indicateurs
            enriched_data = await self._process_ultra_enriched_klines(klines, symbol, timeframe)
            
            # Mettre √† jour les buffers
            self._update_price_buffers(symbol, timeframe, enriched_data)
            
            # Stocker dans Redis avec format standardis√©
            redis_key = f"market_data:{symbol}:{timeframe}"
            await self._store_in_redis(redis_key, enriched_data)
            
            logger.debug(f"üî• {symbol} {timeframe}: {len(enriched_data)} indicateurs calcul√©s")
            
        except Exception as e:
            error_msg = str(e).replace('{', '{{').replace('}', '}}') if e else "Exception vide"
            logger.error(f"‚ùå Erreur donn√©es ultra-enrichies {symbol} {timeframe}: {error_msg}")
    
    async def _fetch_klines(self, symbol: str, timeframe: str, limit: int = 100) -> List:
        """R√©cup√®re les klines depuis Binance"""
        try:
            params = {
                'symbol': symbol,
                'interval': timeframe,
                'limit': limit
            }
            
            url = f"{self.base_url}{self.klines_endpoint}"
            
            async with aiohttp.ClientSession() as session:
                async with session.get(url, params=params, timeout=15) as response:
                    if response.status == 200:
                        return await response.json()
                    else:
                        logger.error(f"‚ùå API Binance error {response.status} pour {symbol} {timeframe}")
                        return []
                        
        except Exception as e:
            error_msg = str(e).replace('{', '{{').replace('}', '}}') if e else "Exception vide"
            logger.error(f"‚ùå Erreur fetch klines {symbol} {timeframe}: {error_msg}")
            return []
    
    async def _process_ultra_enriched_klines(self, klines: List, symbol: str, timeframe: str) -> Dict:
        """
        Traite les klines avec TOUS les indicateurs ultra-confluents.
        Utilise le module partag√© technical_indicators pour les calculs optimis√©s.
        """
        try:
            # Extraire les prix et volumes
            prices = []
            volumes = []
            highs = []
            lows = []
            opens = []
            
            for kline in klines:
                opens.append(float(kline[1]))
                highs.append(float(kline[2]))
                lows.append(float(kline[3]))
                prices.append(float(kline[4]))  # close
                volumes.append(float(kline[5]))
            
            if not prices:
                return {}
            
            # Donn√©es de base
            enriched_data = {
                'symbol': symbol,
                'timeframe': timeframe,
                'enhanced': True,
                'ultra_enriched': True,
                'close': prices[-1],
                'volume': volumes[-1],
                'timestamp': time.time(),
                'last_update': time.time()
            }
            
            # üöÄ HYBRIDE : Calcul incr√©mental pour EMA/MACD + traditionnel pour le reste
            logger.debug(f"üîß Calcul HYBRIDE indicateurs pour {symbol} {timeframe}")
            
            # üöÄ NOUVEAU : Calcul incr√©mental pour EMA/MACD (√©vite dents de scie)
            incremental_indicators = self._calculate_smooth_indicators_ultra(symbol, timeframe, prices, highs, lows, volumes)
            
            # üìä TRADITIONNEL : Calcul normal pour les autres indicateurs
            calculated_indicators = indicators.calculate_all_indicators(highs, lows, prices, volumes)
            
            # FUSION : Priorit√© aux incr√©mentaux (EMA/MACD lisses)
            final_indicators = calculated_indicators.copy()
            final_indicators.update(incremental_indicators)  # Override EMA/MACD with smooth versions
            
            # Ajouter tous les indicateurs calcul√©s
            enriched_data.update(final_indicators)
            
            # **CRITIQUE**: Sauvegarder les indicateurs dans le cache persistant IMM√âDIATEMENT
            from shared.src.technical_indicators import indicator_cache
            saved_indicators = 0
            for indicator_name, indicator_value in final_indicators.items():
                # Sauvegarder seulement les indicateurs num√©riques valides
                if isinstance(indicator_value, (int, float)) and not (isinstance(indicator_value, float) and indicator_value != indicator_value):
                    indicator_cache.set(symbol, timeframe, indicator_name, indicator_value)
                    saved_indicators += 1
            
            if saved_indicators > 0:
                logger.debug(f"üíæ {saved_indicators} indicateurs sauvegard√©s dans cache persistant {symbol} {timeframe}")
            
            if incremental_indicators:
                logger.debug(f"‚úÖ {len(final_indicators)} indicateurs calcul√©s et sauvegard√©s pour {symbol} (üöÄ EMA/MACD incr√©mentaux)")
            
            # Indicateurs additionnels non couverts par le module partag√©
            additional_indicators = {}
            if len(prices) >= 20:
                # Stochastic RSI (pas dans le module partag√©)
                additional_indicators['stoch_rsi'] = self._calculate_stoch_rsi(prices, 14)
                
                # ADX d√©j√† calcul√© par calculate_all_indicators avec plus_di/minus_di
                # Pas besoin de recalculer individuellement
                
                # Williams %R
                additional_indicators['williams_r'] = self._calculate_williams_r(highs, lows, prices, 14)
                
                # CCI
                additional_indicators['cci_20'] = self._calculate_cci(highs, lows, prices, 20)
            
            # VWAP
            if len(prices) >= 10:
                additional_indicators['vwap_10'] = self._calculate_vwap(prices[-10:], volumes[-10:])
            
            # Ajouter et sauvegarder les indicateurs additionnels
            enriched_data.update(additional_indicators)
            for indicator_name, indicator_value in additional_indicators.items():
                if isinstance(indicator_value, (int, float)) and not (isinstance(indicator_value, float) and indicator_value != indicator_value):
                    indicator_cache.set(symbol, timeframe, indicator_name, indicator_value)
            
            logger.debug(f"‚úÖ {symbol} {timeframe}: {len(enriched_data)} indicateurs calcul√©s et sauvegard√©s (module partag√© + additionnels)")
            return enriched_data
            
        except Exception as e:
            logger.error(f"‚ùå Erreur traitement ultra-enrichi {symbol} {timeframe}: {e}")
            return {}
    
    async def _fetch_orderbook_sentiment(self, symbol: str):
        """R√©cup√®re et analyse le sentiment orderbook"""
        try:
            params = {
                'symbol': symbol,
                'limit': 20  # Top 20 niveaux
            }
            
            url = f"{self.base_url}{self.depth_endpoint}"
            
            async with aiohttp.ClientSession() as session:
                async with session.get(url, params=params, timeout=10) as response:
                    if response.status == 200:
                        orderbook = await response.json()
                        sentiment_data = self._analyze_orderbook_sentiment(orderbook)
                        
                        # Stocker dans Redis
                        redis_key = f"orderbook_sentiment:{symbol}"
                        await self._store_in_redis(redis_key, sentiment_data)
                        
                        logger.debug(f"üìä Sentiment orderbook {symbol} mis √† jour")
                        
        except Exception as e:
            logger.error(f"‚ùå Erreur sentiment orderbook {symbol}: {e}")
    
    async def _fetch_ticker_data(self, symbol: str):
        """R√©cup√®re les donn√©es ticker 24h"""
        try:
            params = {'symbol': symbol}
            url = f"{self.base_url}{self.ticker_endpoint}"
            
            async with aiohttp.ClientSession() as session:
                async with session.get(url, params=params, timeout=10) as response:
                    if response.status == 200:
                        ticker = await response.json()
                        
                        # Stocker dans Redis
                        redis_key = f"ticker_24h:{symbol}"
                        await self._store_in_redis(redis_key, ticker)
                        
                        logger.debug(f"üìà Ticker 24h {symbol} mis √† jour")
                        
        except Exception as e:
            logger.error(f"‚ùå Erreur ticker 24h {symbol}: {e}")
    
    def _update_price_buffers(self, symbol: str, timeframe: str, data: Dict):
        """Met √† jour les buffers de prix pour calculs en temps r√©el"""
        try:
            if 'close' in data:
                self.price_buffers[symbol][timeframe].append(data['close'])
                # Garder seulement les 100 derniers prix
                if len(self.price_buffers[symbol][timeframe]) > 100:
                    self.price_buffers[symbol][timeframe] = self.price_buffers[symbol][timeframe][-100:]
            
            if 'volume' in data:
                self.volume_buffers[symbol][timeframe].append(data['volume'])
                if len(self.volume_buffers[symbol][timeframe]) > 100:
                    self.volume_buffers[symbol][timeframe] = self.volume_buffers[symbol][timeframe][-100:]
                    
        except Exception as e:
            logger.error(f"‚ùå Erreur mise √† jour buffers {symbol} {timeframe}: {e}")
    
    async def _store_in_redis(self, key: str, data: Dict):
        """Stocke les donn√©es dans Redis avec TTL √©tendu pour continuit√©"""
        try:
            # TTL √©tendu : 48h pour donn√©es historiques, 24h pour sentiment/ticker
            if "market_data:" in key:
                ttl = 48 * 3600  # 48 heures pour donn√©es de march√©
            elif "orderbook_sentiment:" in key or "ticker:" in key:
                ttl = 24 * 3600  # 24 heures pour sentiment et ticker
            else:
                ttl = 12 * 3600  # 12 heures par d√©faut
            
            self.redis_client.set(key, json.dumps(data), expiration=ttl)
            logger.debug(f"üíæ Donn√©es stock√©es Redis {key} (TTL: {ttl/3600}h)")
        except Exception as e:
            logger.error(f"‚ùå Erreur stockage Redis {key}: {e}")
    
    # =================== M√âTHODES DE CALCUL D'INDICATEURS ===================
    # (Copies des m√©thodes de binance_ws.py pour coh√©rence)
    
    def _calculate_rsi(self, prices: List[float], period: int = 14) -> Optional[float]:
        """Calcule le RSI via le module centralis√©"""
        from shared.src.technical_indicators import calculate_rsi
        return calculate_rsi(prices, period)
    
    def _calculate_stoch_rsi(self, prices: List[float], period: int = 14) -> Optional[float]:
        """Calcule le Stochastic RSI"""
        if len(prices) < period * 2:
            return None
        
        # Calculer RSI pour chaque point
        rsi_values = []
        for i in range(period, len(prices)):
            subset = prices[i-period:i+1]
            rsi = self._calculate_rsi(subset, period)
            if rsi is not None:
                rsi_values.append(rsi)
        
        if len(rsi_values) < period:
            return None
        
        # Calculer Stochastic RSI
        recent_rsi = rsi_values[-period:]
        min_rsi = min(recent_rsi)
        max_rsi = max(recent_rsi)
        
        if max_rsi == min_rsi:
            return 50
        
        stoch_rsi = ((rsi_values[-1] - min_rsi) / (max_rsi - min_rsi)) * 100
        return round(stoch_rsi, 2)
    
    def _calculate_ema(self, prices: List[float], period: int) -> float:
        """Calcule EMA via le module centralis√©"""
        from shared.src.technical_indicators import calculate_ema
        result = calculate_ema(prices, period)
        return result if result is not None else (prices[-1] if prices else 0)
    
    def _calculate_macd(self, prices: List[float]) -> Dict:
        """Calcule MACD complet via le module centralis√©"""
        from shared.src.technical_indicators import calculate_macd
        
        result = calculate_macd(prices)
        if result is None or any(v is None for v in result.values()):
            return {}
            
        return {
            'macd_line': round(result['macd_line'], 6),
            'macd_signal': round(result['macd_signal'], 6),
            'macd_histogram': round(result['macd_histogram'], 6)
        }
    
    def _calculate_bollinger_bands(self, prices: List[float], period: int, std_dev: float) -> Dict:
        """Calcule les Bollinger Bands via le module centralis√©"""
        from shared.src.technical_indicators import calculate_bollinger_bands
        
        result = calculate_bollinger_bands(prices, period, std_dev)
        if result is None or any(v is None for v in result.values()):
            return {}
            
        return {
            'bb_upper': round(result['bb_upper'], 6),
            'bb_middle': round(result['bb_middle'], 6),
            'bb_lower': round(result['bb_lower'], 6),
            'bb_position': round(result['bb_position'], 3),
            'bb_width': round(result['bb_width'], 2)
        }
    
    def _calculate_adx(self, highs: List[float], lows: List[float], closes: List[float], period: int = 14) -> Optional[float]:
        """Calcule ADX en utilisant le module partag√©"""
        from shared.src.technical_indicators import calculate_adx
        
        try:
            adx_result = calculate_adx(highs, lows, closes, period)
            # calculate_adx retourne (ADX, DI+, DI-)
            adx_value = adx_result[0] if adx_result[0] is not None else None
            return round(adx_value, 2) if adx_value is not None else None
        except Exception as e:
            logger.warning(f"Erreur calcul ADX: {e}")
            return None
    
    def _calculate_atr(self, highs: List[float], lows: List[float], closes: List[float], period: int = 14) -> Optional[float]:
        """Calcule ATR via le module centralis√©"""
        from shared.src.technical_indicators import calculate_atr
        result = calculate_atr(highs, lows, closes, period)
        return round(result, 6) if result is not None else None
    
    def _calculate_williams_r(self, highs: List[float], lows: List[float], closes: List[float], period: int = 14) -> Optional[float]:
        """Calcule Williams %R"""
        if len(closes) < period:
            return None
        
        highest_high = max(highs[-period:])
        lowest_low = min(lows[-period:])
        current_close = closes[-1]
        
        if highest_high == lowest_low:
            return -50
        
        williams_r = ((highest_high - current_close) / (highest_high - lowest_low)) * -100
        return round(williams_r, 2)
    
    def _calculate_cci(self, highs: List[float], lows: List[float], closes: List[float], period: int = 20) -> Optional[float]:
        """Calcule CCI"""
        if len(closes) < period:
            return None
        
        # Typical Price
        typical_prices = []
        for i in range(len(closes)):
            tp = (highs[i] + lows[i] + closes[i]) / 3
            typical_prices.append(tp)
        
        if len(typical_prices) < period:
            return None
        
        # SMA des typical prices
        sma_tp = sum(typical_prices[-period:]) / period
        
        # D√©viation moyenne
        mean_deviation = sum(abs(tp - sma_tp) for tp in typical_prices[-period:]) / period
        
        if mean_deviation == 0:
            return 0
        
        current_tp = typical_prices[-1]
        cci = (current_tp - sma_tp) / (0.015 * mean_deviation)
        
        return round(cci, 2)
    
    def _calculate_vwap(self, prices: List[float], volumes: List[float]) -> Optional[float]:
        """Calcule VWAP"""
        if not prices or not volumes or len(prices) != len(volumes):
            return None
        
        total_volume = sum(volumes)
        if total_volume == 0:
            return None
        
        vwap = sum(p * v for p, v in zip(prices, volumes)) / total_volume
        return round(vwap, 6)
    
    def _calculate_momentum(self, prices: List[float], period: int) -> Optional[float]:
        """Calcule le momentum"""
        if len(prices) < period + 1:
            return None
        
        momentum = ((prices[-1] / prices[-period-1]) - 1) * 100
        return round(momentum, 3)
    
    def _analyze_volume(self, volumes: List[float]) -> Dict:
        """Analyse du volume"""
        if len(volumes) < 20:
            return {}
        
        recent_vol = volumes[-10:]
        avg_vol = sum(volumes[-20:]) / 20
        
        current_vol = volumes[-1]
        vol_ratio = current_vol / avg_vol if avg_vol > 0 else 1
        
        # D√©tection du spike
        volume_spike = vol_ratio > 2.0
        
        # Tendance du volume
        if len(recent_vol) >= 5:
            early_avg = sum(recent_vol[:5]) / 5
            late_avg = sum(recent_vol[5:]) / 5
            
            if late_avg > early_avg * 1.1:
                volume_trend = 'increasing'
            elif late_avg < early_avg * 0.9:
                volume_trend = 'decreasing'
            else:
                volume_trend = 'stable'
        else:
            volume_trend = 'stable'
        
        return {
            'volume_ratio': round(vol_ratio, 2),
            'volume_spike': volume_spike,
            'volume_trend': volume_trend,
            'avg_volume_20': round(avg_vol, 2)
        }
    
    def _analyze_orderbook_sentiment(self, orderbook: Dict) -> Dict:
        """Analyse le sentiment de l'orderbook"""
        try:
            bids = orderbook.get('bids', [])
            asks = orderbook.get('asks', [])
            
            if not bids or not asks:
                return {}
            
            # Calculs de base
            bid_price = float(bids[0][0])
            ask_price = float(asks[0][0])
            spread = ask_price - bid_price
            spread_pct = (spread / bid_price) * 100
            
            # Volumes totaux
            total_bid_volume = sum(float(bid[1]) for bid in bids[:10])
            total_ask_volume = sum(float(ask[1]) for ask in asks[:10])
            
            # Ratio et imbalance
            total_volume = total_bid_volume + total_ask_volume
            if total_volume > 0:
                bid_ask_ratio = total_bid_volume / total_ask_volume
                imbalance = (total_bid_volume - total_ask_volume) / total_volume
            else:
                bid_ask_ratio = 1.0
                imbalance = 0.0
            
            # Signal de sentiment
            if bid_ask_ratio > 1.5:
                sentiment_signal = 'VERY_BULLISH'
            elif bid_ask_ratio > 1.2:
                sentiment_signal = 'BULLISH'
            elif bid_ask_ratio < 0.67:
                sentiment_signal = 'VERY_BEARISH'
            elif bid_ask_ratio < 0.83:
                sentiment_signal = 'BEARISH'
            else:
                sentiment_signal = 'NEUTRAL'
            
            return {
                'bid_price': bid_price,
                'ask_price': ask_price,
                'spread_pct': round(spread_pct, 4),
                'bid_ask_ratio': round(bid_ask_ratio, 3),
                'imbalance': round(imbalance, 3),
                'sentiment_signal': sentiment_signal,
                'sentiment_score': round(imbalance, 3),
                'total_bid_volume': round(total_bid_volume, 2),
                'total_ask_volume': round(total_ask_volume, 2),
                'timestamp': time.time()
            }
            
        except Exception as e:
            logger.error(f"‚ùå Erreur analyse sentiment: {e}")
            return {}

    async def load_historical_data(self, days: int = 5, use_gap_detection: bool = True) -> None:
        """
        Charge les donn√©es historiques pour tous les symboles et timeframes.
        Publie aussi les donn√©es sur Kafka pour persistance en DB.
        
        Args:
            days: Nombre de jours d'historique √† charger (d√©faut: 5)
            use_gap_detection: Si True, d√©tecte et charge uniquement les donn√©es manquantes
        """
        logger.info(f"üîÑ Chargement de {days} jours de donn√©es historiques...")
        
        # Importer le producteur Kafka pour publier les donn√©es historiques
        try:
            from kafka_producer import get_producer
            kafka_producer = get_producer()
        except ImportError:
            logger.warning("‚ö†Ô∏è Producteur Kafka non disponible, donn√©es historiques non persist√©es en DB")
            kafka_producer = None
            
        # Utiliser la d√©tection de gaps si activ√©e
        gap_filling_plan = None
        if use_gap_detection:
            try:
                from gap_detector import GapDetector
                detector = GapDetector()
                await detector.initialize()
                
                # D√©tecter les gaps sur la p√©riode demand√©e
                lookback_hours = days * 24
                all_gaps = await detector.detect_all_gaps(self.symbols, lookback_hours)
                
                # G√©n√©rer le plan de remplissage optimis√©
                gap_filling_plan = detector.generate_gap_filling_plan(all_gaps)
                
                if gap_filling_plan:
                    # Estimer le temps de remplissage
                    estimated_time = detector.estimate_fill_time(gap_filling_plan)
                    logger.info(f"üéØ Mode intelligent: Remplissage cibl√© des gaps uniquement")
                    logger.info(f"‚è±Ô∏è Temps estim√©: {estimated_time:.1f}s ({estimated_time/60:.1f} minutes)")
                else:
                    logger.info("‚úÖ Aucun gap d√©tect√© - donn√©es d√©j√† compl√®tes")
                    await detector.close()
                    return
                    
                await detector.close()
                
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Erreur d√©tection gaps: {e} - Fallback sur chargement complet")
                gap_filling_plan = None
        
        # Calculer les limites optimis√©es pour indicateurs ULTRA-PR√âCIS
        timeframe_limits = {
            '1m': 2000,   # 33.3 heures ‚Üí Intraday ultra-pr√©cis
            '5m': 1500,   # 5.2 jours ‚Üí Pattern detection parfait
            '15m': 1000,  # 10.4 jours ‚Üí Swing trading optimal  
            '1h': 720,    # 30 jours ‚Üí Tendance mensuelle compl√®te
            '4h': 500     # 83.3 jours ‚Üí Cycle long terme
        }
        
        total_expected = 0
        total_loaded = 0
        total_published = 0
        
        # Si on a un plan de remplissage de gaps, l'utiliser
        if gap_filling_plan:
            for symbol, timeframe_periods in gap_filling_plan.items():
                for timeframe, periods in timeframe_periods.items():
                    for start_time, end_time in periods:
                        try:
                            # Charger uniquement les donn√©es pour cette p√©riode de gap
                            historical_data = await self._fetch_historical_klines_for_period(
                                symbol, timeframe, start_time, end_time
                            )
                            
                            if historical_data:
                                loaded_count = len(historical_data)
                                total_loaded += loaded_count
                                logger.info(f"üìä {symbol} {timeframe}: Gap {start_time} ‚Üí {end_time} rempli ({loaded_count} points)")
                                
                                # Enrichir et publier sur Kafka
                                if kafka_producer and historical_data:
                                    try:
                                        enriched_historical = await self._enrich_historical_batch(
                                            historical_data, symbol, timeframe
                                        )
                                        
                                        for enriched_point in enriched_historical:
                                            enriched_point['symbol'] = symbol
                                            enriched_point['timeframe'] = timeframe
                                            kafka_producer.publish_market_data(enriched_point, symbol)
                                            total_published += 1
                                            
                                    except Exception as e:
                                        logger.error(f"‚ùå Erreur enrichissement gap {symbol} {timeframe}: {e}")
                                        
                        except Exception as e:
                            logger.error(f"‚ùå Erreur remplissage gap {symbol} {timeframe} {start_time}: {e}")
        else:
            # Mode classique: charger toutes les donn√©es
            for symbol in self.symbols:
                for timeframe in self.timeframes:
                    expected_count = timeframe_limits[timeframe]
                    total_expected += expected_count
                    
                    try:
                        # Charger les donn√©es historiques avec pagination si n√©cessaire
                        historical_data = await self._fetch_historical_klines(
                            symbol, timeframe, expected_count
                        )
                        
                        if historical_data:
                            loaded_count = len(historical_data)
                            total_loaded += loaded_count
                            
                            # **NOUVEAU**: Enrichir et publier chaque kline historique sur Kafka
                            if kafka_producer and historical_data:
                                try:
                                    # Enrichir toutes les donn√©es historiques avec indicateurs techniques
                                    enriched_historical = await self._enrich_historical_batch(
                                        historical_data, symbol, timeframe
                                    )
                                    
                                    # Publier chaque point enrichi sur Kafka
                                    for i, enriched_point in enumerate(enriched_historical):
                                        try:
                                            # Assurer que les champs symbol et timeframe sont pr√©sents pour la g√©n√©ration automatique du topic
                                            enriched_point['symbol'] = symbol  
                                            enriched_point['timeframe'] = timeframe
                                            
                                            # Debug: Log d'un √©chantillon pour voir ce qui est publi√©
                                            if i == len(enriched_historical) - 1:  # Dernier point
                                                indicators_in_point = [k for k in enriched_point.keys() if k not in ['symbol', 'interval', 'start_time', 'open_time', 'close_time', 'open', 'high', 'low', 'close', 'volume', 'is_closed', 'is_historical', 'enhanced', 'ultra_enriched', 'timeframe']]
                                                logger.error(f"üîç KAFKA PUBLISH {symbol} {timeframe}: {len(indicators_in_point)} indicateurs dans le point")
                                                logger.error(f"üîç Indicateurs dans le point: {indicators_in_point}")
                                            
                                            kafka_producer.publish_market_data(enriched_point, symbol)
                                            total_published += 1
                                            
                                        except Exception as e:
                                            logger.warning(f"Erreur publication Kafka point enrichi: {e}")
                                            
                                    logger.info(f"üìä {symbol} {timeframe}: {len(enriched_historical)} points enrichis publi√©s")
                                    
                                except Exception as e:
                                    logger.error(f"‚ùå Erreur enrichissement historique {symbol} {timeframe}: {e}")
                            
                            # Traiter et enrichir la derni√®re donn√©e pour Redis (indicateurs actuels)
                            enriched_data = await self._process_historical_klines(
                                historical_data, symbol, timeframe
                            )
                            
                            # Stocker dans Redis
                            redis_key = f"market_data:{symbol}:{timeframe}_history"
                            await self._store_in_redis(redis_key, enriched_data)
                            
                            logger.info(f"üìä {symbol} {timeframe}: {loaded_count}/{expected_count} points charg√©s")
                        else:
                            logger.warning(f"‚ö†Ô∏è Aucune donn√©e historique pour {symbol} {timeframe}")
                            
                    except Exception as e:
                        logger.error(f"‚ùå Erreur chargement historique {symbol} {timeframe}: {e}")
        
        # Attendre que tous les messages Kafka soient envoy√©s
        if kafka_producer:
            kafka_producer.flush()
            logger.info(f"üì§ {total_published} points historiques publi√©s sur Kafka pour persistance DB")
        
        success_rate = (total_loaded / total_expected * 100) if total_expected > 0 else 0
        logger.info(f"‚úÖ Chargement historique termin√©: {total_loaded}/{total_expected} points ({success_rate:.1f}%)")
    
    async def _fetch_historical_klines_for_period(self, 
                                                  symbol: str, 
                                                  timeframe: str, 
                                                  start_time: datetime, 
                                                  end_time: datetime) -> List:
        """
        R√©cup√®re les klines historiques pour une p√©riode sp√©cifique (gap filling)
        """
        try:
            # Convertir les datetime en timestamps milliseconds
            start_timestamp = int(start_time.timestamp() * 1000)
            end_timestamp = int(end_time.timestamp() * 1000)
            
            # Calculer combien de klines maximum pour cette p√©riode
            interval_seconds = {
                '1m': 60, '5m': 300, '15m': 900, '1h': 3600, '4h': 14400
            }.get(timeframe, 60)
            
            duration_seconds = (end_time - start_time).total_seconds()
            max_klines = min(1000, int(duration_seconds / interval_seconds) + 10)  # +10 de marge
            
            params = {
                'symbol': symbol,
                'interval': timeframe,
                'startTime': start_timestamp,
                'endTime': end_timestamp,
                'limit': max_klines
            }
            
            url = f"{self.base_url}{self.klines_endpoint}"
            
            async with aiohttp.ClientSession() as session:
                async with session.get(url, params=params, timeout=15) as response:
                    if response.status == 200:
                        klines = await response.json()
                        logger.debug(f"üìö Gap fill {symbol} {timeframe}: {len(klines)} klines r√©cup√©r√©es "
                                   f"pour {start_time} ‚Üí {end_time}")
                        return klines
                    else:
                        logger.error(f"‚ùå API Binance error {response.status} pour gap {symbol} {timeframe}")
                        return []
                        
        except Exception as e:
            logger.error(f"‚ùå Erreur fetch gap {symbol} {timeframe} {start_time}: {e}")
            return []

    async def _fetch_historical_klines(self, symbol: str, timeframe: str, limit: int) -> List:
        """
        R√©cup√®re les klines historiques avec pagination pour les gros volumes.
        Binance limite √† 1000 klines par requ√™te.
        """
        all_klines = []
        max_per_request = 1000
        
        try:
            while len(all_klines) < limit:
                # Calculer combien r√©cup√©rer dans cette requ√™te
                remaining = limit - len(all_klines)
                current_limit = min(remaining, max_per_request)
                
                # Calculer l'endTime pour r√©cup√©rer les donn√©es plus anciennes
                if all_klines:
                    # Utiliser le timestamp de la premi√®re kline comme endTime
                    end_time = all_klines[0][0] - 1  # -1ms pour √©viter les doublons
                else:
                    # Premi√®re requ√™te, pas d'endTime
                    end_time = None
                
                # Faire la requ√™te
                batch_klines = await self._fetch_klines_batch(
                    symbol, timeframe, current_limit, end_time
                )
                
                if not batch_klines:
                    logger.warning(f"Pas de donn√©es re√ßues pour {symbol} {timeframe}")
                    break
                
                # Ajouter au d√©but (ordre chronologique inverse)
                all_klines = batch_klines + all_klines
                
                # Log de progression
                logger.debug(f"üìä {symbol} {timeframe}: {len(all_klines)}/{limit} points r√©cup√©r√©s")
                
                # Pause pour √©viter la limite de taux
                await asyncio.sleep(0.1)
                
                # Si on a re√ßu moins que demand√©, on a atteint la fin
                if len(batch_klines) < current_limit:
                    break
            
            # Trier par timestamp pour √™tre s√ªr de l'ordre chronologique
            all_klines.sort(key=lambda x: x[0])
            
            logger.info(f"üìö {symbol} {timeframe}: {len(all_klines)} klines historiques r√©cup√©r√©es")
            return all_klines
            
        except Exception as e:
            logger.error(f"‚ùå Erreur r√©cup√©ration historique {symbol} {timeframe}: {e}")
            return []
    
    async def _fetch_klines_batch(self, symbol: str, timeframe: str, limit: int, end_time: Optional[int] = None) -> List:
        """R√©cup√®re un batch de klines avec gestion de l'endTime"""
        try:
            params = {
                'symbol': symbol,
                'interval': timeframe,
                'limit': limit
            }
            
            if end_time:
                params['endTime'] = end_time
            
            url = f"{self.base_url}{self.klines_endpoint}"
            
            async with aiohttp.ClientSession() as session:
                async with session.get(url, params=params, timeout=15) as response:
                    if response.status == 200:
                        return await response.json()
                    else:
                        logger.error(f"‚ùå API Binance error {response.status} pour {symbol} {timeframe}")
                        return []
                        
        except Exception as e:
            logger.error(f"‚ùå Erreur fetch batch {symbol} {timeframe}: {e}")
            return []
    
    async def _enrich_historical_batch(self, klines: List, symbol: str, timeframe: str) -> List[Dict]:
        """
        Enrichit les donn√©es historiques avec calcul correct des indicateurs.
        APPROCHE CORRIG√âE: Calcule les indicateurs progressivement pour chaque point.
        
        Args:
            klines: Liste des klines historiques brutes (tri√©es chronologiquement)
            symbol: Symbole de trading
            timeframe: Timeframe des donn√©es
            
        Returns:
            Liste des points enrichis avec indicateurs calcul√©s correctement
        """
        from shared.src.technical_indicators import indicator_cache, indicators, TechnicalIndicators
        import numpy as np
        
        enriched_points = []
        
        try:
            logger.info(f"üîÑ Enrichissement de {len(klines)} points historiques pour {symbol} {timeframe}")
            
            # Extraire TOUTES les donn√©es OHLCV
            prices = []
            highs = []
            lows = []
            volumes = []
            timestamps = []
            
            for kline in klines:
                prices.append(float(kline[4]))    # close
                highs.append(float(kline[2]))     # high
                lows.append(float(kline[3]))      # low
                volumes.append(float(kline[5]))   # volume
                timestamps.append(kline[0])       # timestamp
            
            logger.info(f"üìä Calcul arrays d'indicateurs sur {len(prices)} points pour {symbol} {timeframe}")
            
            # Calculer TOUS les indicateurs en une fois avec les arrays complets
            tech_indicators = TechnicalIndicators()
            indicators_arrays = tech_indicators.calculate_all_indicators_array(highs, lows, prices, volumes)
            
            logger.info(f"‚úÖ {len(indicators_arrays)} indicateurs calcul√©s en arrays")
            
            # Traiter chaque kline avec les indicateurs correspondants
            for i, kline in enumerate(klines):
                try:
                    # Extraire les donn√©es OHLCV
                    timestamp = kline[0]
                    open_price = float(kline[1])
                    high_price = float(kline[2])
                    low_price = float(kline[3])
                    close_price = float(kline[4])
                    volume = float(kline[5])
                    close_time = kline[6]
                    
                    # Cr√©er le point enrichi de base
                    enriched_point = {
                        'symbol': symbol,
                        'interval': timeframe,
                        'start_time': timestamp,
                        'open_time': timestamp,
                        'close_time': close_time,
                        'open': open_price,
                        'high': high_price,
                        'low': low_price,
                        'close': close_price,
                        'volume': volume,
                        'is_closed': True,
                        'is_historical': True,
                        'enhanced': True,
                        'ultra_enriched': True
                    }
                    
                    # Ajouter les indicateurs correspondant √† l'index i
                    indicators_count = 0
                    if indicators_arrays:
                        for indicator_name, indicator_array in indicators_arrays.items():
                            if i < len(indicator_array):
                                value = indicator_array[i]
                                # V√©rifier que la valeur est valide (pas NaN ou inf)
                                if not (np.isnan(value) or np.isinf(value)):
                                    enriched_point[indicator_name] = float(value)
                                    indicators_count += 1
                                    # Sauvegarder le dernier indicateur dans le cache pour continuit√©
                                    if i == len(klines) - 1:  # Derni√®re kline
                                        indicator_cache.set(symbol, timeframe, indicator_name, float(value))
                    
                    enriched_points.append(enriched_point)
                    
                    # Log de progression
                    if (i + 1) % max(1, len(klines) // 10) == 0 or i == len(klines) - 1:
                        logger.debug(f"  ‚ö° Point {i+1}/{len(klines)}: {indicators_count} indicateurs ajout√©s")
                        
                except Exception as e:
                    logger.warning(f"Erreur traitement point {i} pour {symbol} {timeframe}: {e}")
                    # En cas d'erreur sur un point, ajouter au moins les donn√©es de base
                    basic_point = {
                        'symbol': symbol,
                        'interval': timeframe,
                        'start_time': kline[0],
                        'open_time': kline[0],
                        'close_time': kline[6],
                        'open': float(kline[1]),
                        'high': float(kline[2]),
                        'low': float(kline[3]),
                        'close': float(kline[4]),
                        'volume': float(kline[5]),
                        'is_closed': True,
                        'is_historical': True
                    }
                    enriched_points.append(basic_point)
            
            # Log final avec statistiques d√©taill√©es
            if enriched_points:
                sample_indicators = len([k for k in enriched_points[-1].keys() 
                                       if k not in ['symbol', 'interval', 'start_time', 'open_time', 'close_time', 
                                                   'open', 'high', 'low', 'close', 'volume', 'is_closed', 
                                                   'is_historical', 'enhanced', 'ultra_enriched']])
                logger.info(f"‚úÖ {symbol} {timeframe}: {len(enriched_points)} points avec {sample_indicators} indicateurs chacun")
            else:
                logger.warning(f"‚ö†Ô∏è Aucun point enrichi g√©n√©r√© pour {symbol} {timeframe}")
            
            return enriched_points
            
        except Exception as e:
            logger.error(f"‚ùå Erreur enrichissement s√©quentiel {symbol} {timeframe}: {e}")
            # En cas d'erreur globale, retourner au moins les donn√©es de base
            basic_points = []
            for kline in klines:
                basic_point = {
                    'symbol': symbol,
                    'interval': timeframe,
                    'start_time': kline[0],
                    'open_time': kline[0],
                    'close_time': kline[6],
                    'open': float(kline[1]),
                    'high': float(kline[2]),
                    'low': float(kline[3]),
                    'close': float(kline[4]),
                    'volume': float(kline[5]),
                    'is_closed': True,
                    'is_historical': True
                }
                basic_points.append(basic_point)
            return basic_points

    async def _calculate_point_indicators(self, close_price: float, high_price: float, low_price: float, volume: float, symbol: str, timeframe: str) -> Dict:
        """
        Calcule les indicateurs pour un point unique de mani√®re s√©quentielle.
        Maintient l'√©tat des indicateurs incr√©mentaux dans le cache global.
        
        Args:
            close_price: Prix de fermeture
            high_price: Prix le plus haut
            low_price: Prix le plus bas
            volume: Volume
            symbol: Symbole de trading
            timeframe: Timeframe des donn√©es
            
        Returns:
            Dictionnaire avec tous les indicateurs calcul√©s
        """
        from shared.src.technical_indicators import indicators
        
        try:
            # Cr√©er des listes avec ce point unique pour compatibilit√© avec les fonctions existantes
            prices = [close_price]
            highs = [high_price]
            lows = [low_price]
            volumes = [volume]
            
            # Donn√©es de base
            point_indicators = {
                'symbol': symbol,
                'timeframe': timeframe,
                'enhanced': True,
                'ultra_enriched': True,
                'close': close_price,
                'volume': volume,
                'timestamp': time.time(),
                'last_update': time.time()
            }
            
            # **CRITIQUE**: Calcul incr√©mental pour EMA/MACD (maintient la continuit√©)
            incremental_indicators = self._calculate_smooth_indicators_ultra(symbol, timeframe, prices, highs, lows, volumes)
            
            # Calcul des autres indicateurs avec les donn√©es actuelles
            # Note: Pour un seul point, certains indicateurs ne peuvent pas √™tre calcul√©s
            # mais les incr√©mentaux (EMA/MACD) peuvent l'√™tre gr√¢ce au cache
            if incremental_indicators:
                point_indicators.update(incremental_indicators)
                
                # Calculs simples possibles avec un point
                point_indicators['rsi_14'] = incremental_indicators.get('rsi_14', 50.0)  # Valeur par d√©faut neutre
                
                # **NOUVEAU**: Sauvegarder explicitement les indicateurs dans le cache persistant
                from shared.src.technical_indicators import indicator_cache
                for indicator_name, indicator_value in incremental_indicators.items():
                    # Sauvegarder seulement les indicateurs num√©riques valides
                    if isinstance(indicator_value, (int, float)) and not (isinstance(indicator_value, float) and indicator_value != indicator_value):
                        indicator_cache.set(symbol, timeframe, indicator_name, indicator_value)
                
                logger.debug(f"Point {symbol} {timeframe}: {len(point_indicators)} indicateurs calcul√©s et sauvegard√©s")
            
            return point_indicators
            
        except Exception as e:
            logger.warning(f"Erreur calcul indicateurs point {symbol} {timeframe}: {e}")
            # Retourner au moins les donn√©es de base
            return {
                'symbol': symbol,
                'timeframe': timeframe,
                'close': close_price,
                'volume': volume
            }

    async def _process_historical_klines(self, klines: List, symbol: str, timeframe: str) -> Dict:
        """Traite et enrichit un batch de klines historiques"""
        try:
            if not klines:
                return {}
            
            # Prendre la derni√®re kline comme donn√©es actuelles (plus r√©cente)
            latest_kline = klines[-1]
            
            # Traiter comme une kline normale pour obtenir les indicateurs
            enriched_data = await self._process_ultra_enriched_klines(klines, symbol, timeframe)
            
            # Ajouter les m√©tadonn√©es historiques
            enriched_data.update({
                'symbol': symbol,
                'timeframe': timeframe,
                'historical_count': len(klines),
                'oldest_timestamp': klines[0][0],
                'newest_timestamp': klines[-1][0],
                'is_historical': True
            })
            
            return enriched_data
            
        except Exception as e:
            logger.error(f"‚ùå Erreur traitement historique {symbol} {timeframe}: {e}")
            return {}

    def _calculate_smooth_indicators_ultra(self, symbol: str, timeframe: str, prices: List[float], highs: List[float], lows: List[float], volumes: List[float]) -> Dict:
        """
        üöÄ NOUVEAU : Calcule EMA/MACD avec s√©ries optimis√©es pour √©viter les dents de scie.
        Utilise le module technical_indicators pour un calcul lisse.
        
        Args:
            symbol: Symbole trad√©
            timeframe: Intervalle de temps  
            prices: Liste des prix de cl√¥ture
            highs: Liste des prix hauts
            lows: Liste des prix bas
            volumes: Liste des volumes
            
        Returns:
            Dict avec indicateurs EMA/MACD lisses
        """
        result = {}
        
        try:
            if len(prices) < 12:  # Pas assez de donn√©es
                return {}
            
            # üìà EMA 12, 26, 50 avec s√©ries optimis√©es (√©vite dents de scie)
            for period in [12, 26, 50]:
                if len(prices) >= period:
                    ema_key = f'ema_{period}'
                    
                    # Utiliser la m√©thode s√©rie du module indicators
                    ema_series = indicators.calculate_ema_series(prices, period)
                    
                    # Prendre la derni√®re valeur calcul√©e (la plus r√©cente)
                    if ema_series and len(ema_series) > 0 and ema_series[-1] is not None:
                        result[ema_key] = float(ema_series[-1])
            
            # üìä MACD avec s√©ries optimis√©es  
            if len(prices) >= 26:  # Assez de donn√©es pour MACD
                macd_data = indicators.calculate_macd_series(prices)
                
                if macd_data and isinstance(macd_data, dict):
                    # Prendre les derni√®res valeurs calcul√©es
                    if 'macd_line' in macd_data and len(macd_data['macd_line']) > 0 and macd_data['macd_line'][-1] is not None:
                        result['macd_line'] = float(macd_data['macd_line'][-1])
                    if 'macd_signal' in macd_data and len(macd_data['macd_signal']) > 0 and macd_data['macd_signal'][-1] is not None:
                        result['macd_signal'] = float(macd_data['macd_signal'][-1])
                    if 'macd_histogram' in macd_data and len(macd_data['macd_histogram']) > 0 and macd_data['macd_histogram'][-1] is not None:
                        result['macd_histogram'] = float(macd_data['macd_histogram'][-1])
            
            if result:
                logger.debug(f"üöÄ [ULTRA] EMA/MACD lisses calcul√©s pour {symbol} {timeframe}: "
                            f"EMA12={result.get('ema_12', 0):.4f}, "
                            f"MACD={result.get('macd_line', 0):.4f}")
            
        except Exception as e:
            logger.error(f"‚ùå [ULTRA] Erreur calcul indicateurs lisses {symbol} {timeframe}: {e}")
            # En cas d'erreur, retourner un dict vide (fallback vers calcul traditionnel)
            result = {}
        
        return result

# Point d'entr√©e pour test standalone
async def main():
    """Test standalone de l'UltraDataFetcher"""
    fetcher = UltraDataFetcher()
    await fetcher.start()

if __name__ == "__main__":
    asyncio.run(main())