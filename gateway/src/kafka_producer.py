"""
Module Kafka Producer pour le Gateway.
Convertit les donn√©es WebSocket Binance en messages Kafka.
"""

import logging
import os

# Importer les clients partag√©s
import sys
import time
from typing import Any

from shared.src.config import KAFKA_BROKER, KAFKA_TOPIC_MARKET_DATA
from shared.src.kafka_client import KafkaClient
from shared.src.redis_client import RedisClient

sys.path.append(
    os.path.abspath(
        os.path.join(
            os.path.dirname(__file__),
            "../../")))


# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s")
logger = logging.getLogger("kafka_producer")


class KafkaProducer:
    """
    Producteur Kafka pour le Gateway.
    Publie les donn√©es de march√© sur les topics Kafka.
    """

    def __init__(self, broker: str = KAFKA_BROKER):
        """
        Initialise le producteur Kafka.

        Args:
            broker: Adresse du broker Kafka (host:port)
        """
        self.client = KafkaClient(broker=broker)
        self.redis_client = RedisClient()
        logger.info(f"‚úÖ Producteur Kafka+Redis initialis√© pour {broker}")

    def publish_market_data(
        self, data: dict[str, Any], key: str | None = None
    ) -> None:
        """
        Publie les donn√©es de march√© sur le topic Kafka appropri√©.

        Args:
            data: Donn√©es de march√© √† publier
            key: Cl√© √† utiliser pour le partitionnement (g√©n√©ralement le symbole)
        """
        if not data or "symbol" not in data:
            logger.error(
                "‚ùå Donn√©es de march√© invalides, impossible de publier")
            return

        symbol = data["symbol"].lower()
        timeframe = data.get("timeframe", "1m")
        topic = f"{KAFKA_TOPIC_MARKET_DATA}.{symbol}.{timeframe}"

        try:
            # Utiliser le symbole comme cl√© si non fournie
            message_key = key or symbol

            # Publier le message sur Kafka
            self.client.produce(topic=topic, message=data, key=message_key)

            # Publier sur Redis avec le m√™me format que Kafka
            redis_channel = (
                f"roottrading:market:data:{symbol}:{data.get('timeframe', '1m')}"
            )
            self.redis_client.publish(redis_channel, data)

            # Log pour le d√©bogage (uniquement pour les chandeliers ferm√©s)
            if data.get("is_closed", False):
                logger.info(
                    f"üìä OHLCV brutes publi√©es {symbol.upper()}: {data['close']} [O:{data['open']} H:{data['high']} L:{data['low']} V:{data.get('volume', 'N/A')}]"
                )
            else:
                # Log plus discret pour les mises √† jour en cours
                logger.debug(
                    f"üîÑ Donn√©es en cours {symbol.upper()}: prix actuel {data['close']}"
                )
        except Exception as e:
            error_msg = str(e).replace("{", "{{").replace("}", "}}")
            logger.exception(
                "‚ùå Erreur lors de la publication sur Kafka: ")

    def publish_to_topic(
        self, topic: str, data: dict[str, Any], key: str | None = None
    ) -> None:
        """
        Publie des donn√©es sur un topic Kafka sp√©cifique.

        Args:
            topic: Nom du topic Kafka
            data: Donn√©es √† publier
            key: Cl√© pour le partitionnement
        """
        try:
            self.client.produce(topic=topic, message=data, key=key)

            # Log simple pour les donn√©es publi√©es
            if data.get("is_closed", False):
                symbol = data.get("symbol", "N/A")
                timeframe = data.get("timeframe", "N/A")
                price = data.get("close", "N/A")
                logger.info(
                    f"üìä Donn√©es publi√©es {symbol} {timeframe}: {price}")

        except Exception:
            logger.exception("‚ùå Erreur publication topic {topic}")

    def publish_account_data(
            self, data: dict[str, Any], data_type: str) -> None:
        """
        Publie les donn√©es de compte sur le topic Kafka appropri√©.

        Args:
            data: Donn√©es de compte √† publier
            data_type: Type de donn√©es (balances, orders, etc.)
        """
        topic = f"account.{data_type}"

        try:
            self.client.produce(topic=topic, message=data)
            logger.info(f"üí∞ Publi√© donn√©es de compte sur {topic}")
        except Exception as e:
            error_msg = str(e).replace("{", "{{").replace("}", "}}")
            logger.exception(
                "‚ùå Erreur lors de la publication des donn√©es de compte: "
            )

    def flush(self) -> None:
        """
        Force l'envoi de tous les messages en attente.
        """
        self.client.flush()
        logger.info("üîÑ Producteur Kafka vid√©")

    def close(self) -> None:
        """
        Ferme le producteur Kafka proprement.
        """
        self.flush()
        self.client.close()
        logger.info("üëã Producteur Kafka ferm√©")


# Fonction utilitaire pour cr√©er une instance singleton du producteur
_producer_instance = None


def get_producer() -> KafkaProducer:
    """
    Retourne l'instance singleton du producteur Kafka.

    Returns:
        Instance du producteur Kafka
    """
    global _producer_instance
    if _producer_instance is None:
        _producer_instance = KafkaProducer()
    return _producer_instance


# Point d'entr√©e pour les tests
if __name__ == "__main__":
    producer = get_producer()

    # Exemple de publication de donn√©es de march√©
    test_data = {
        "symbol": "BTCUSDC",
        "start_time": int(time.time() * 1000),
        "close_time": int(time.time() * 1000) + 60000,
        "open": 50000.0,
        "high": 50100.0,
        "low": 49900.0,
        "close": 50050.0,
        "volume": 2.5,
        "is_closed": True,
    }

    producer.publish_market_data(test_data)
    producer.flush()

    logger.info("‚úÖ Test de publication Kafka r√©ussi")
    producer.close()
