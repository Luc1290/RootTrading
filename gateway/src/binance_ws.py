"""
Module de connexion WebSocket √† Binance.
Re√ßoit les donn√©es de march√© en temps r√©el et les transmet au Kafka producer.
"""
import json
import logging
import time
from typing import Dict, Any, List, Optional, Tuple
import asyncio
import websockets
from websockets.exceptions import ConnectionClosed, InvalidStatus

# Importer les clients partag√©s
import sys
import os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "../../")))

from shared.src.config import SYMBOLS, INTERVAL
from shared.src.kafka_client import KafkaClient
from gateway.src.kafka_producer import get_producer   # NEW


# Configuration du logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger("binance_ws")

class BinanceWebSocket:
    """
    Gestionnaire de connexion WebSocket √† Binance pour les donn√©es de march√©.
    R√©cup√®re les donn√©es en temps r√©el via WebSocket et les transmet via Kafka.
    """
    
    def __init__(self, symbols: Optional[List[str]] = None, interval: str = INTERVAL, kafka_client: Optional[KafkaClient] = None):
        """
        Initialise la connexion WebSocket Binance.
        
        Args:
            symbols: Liste des symboles √† surveiller (ex: ['BTCUSDC', 'ETHUSDC'])
            interval: Intervalle des chandeliers (ex: '1m', '3m', '5m', '15m', 1d)
            kafka_client: Client Kafka pour la publication des donn√©es
        """
        self.symbols = symbols or SYMBOLS
        self.interval = interval
        self.kafka_client = kafka_client or get_producer()   # utilise le producteur singleton
        self.ws: Optional[websockets.WebSocketClientProtocol] = None  # type: ignore
        self.running = False
        self.reconnect_delay = 1  # Secondes, pour backoff exponentiel
        self.last_message_time = 0.0
        self.heartbeat_interval = 60  # Secondes
        
        # URL de l'API WebSocket Binance
        self.base_url = "wss://stream.binance.com:9443/ws"
        
        # Multi-timeframes et donn√©es enrichies pour trading pr√©cis
        self.timeframes = ['1m', '3m', '5m', '15m', '1d']  # Timeframes optimis√©s pour scalping
        self.stream_paths = []
        
        for symbol in self.symbols:
            symbol_lower = symbol.lower()
            # Klines multi-timeframes
            for tf in self.timeframes:
                self.stream_paths.append(f"{symbol_lower}@kline_{tf}")
            # Ticker 24h pour contexte
            self.stream_paths.append(f"{symbol_lower}@ticker")
            # Orderbook pour spread
            self.stream_paths.append(f"{symbol_lower}@depth5")
            
        # Cache pour donn√©es enrichies
        self.ticker_cache: Dict[str, Any] = {}
        self.orderbook_cache: Dict[str, Any] = {}
        
        # ULTRA-AVANC√â : Buffers pour calculs techniques en temps r√©el
        self.price_buffers: Dict[str, Dict[str, List[float]]] = {}
        self.volume_buffers: Dict[str, Dict[str, List[float]]] = {}
        self.high_buffers: Dict[str, Dict[str, List[float]]] = {}
        self.low_buffers: Dict[str, Dict[str, List[float]]] = {}
        self.rsi_buffers: Dict[str, Dict[str, List[float]]] = {}
        self.macd_buffers: Dict[str, Dict[str, Any]] = {}
        
        # üöÄ NOUVEAU : Cache pour indicateurs incr√©mentaux (√©vite dents de scie)
        self.incremental_cache: Dict[str, Dict[str, Any]] = {}
        
        # Initialiser les buffers pour chaque symbole/timeframe
        for symbol in self.symbols:
            self.price_buffers[symbol] = {}
            self.volume_buffers[symbol] = {}
            self.high_buffers[symbol] = {}
            self.low_buffers[symbol] = {}
            self.rsi_buffers[symbol] = {}
            self.macd_buffers[symbol] = {}
            
            # üöÄ NOUVEAU : Initialiser cache incr√©mental pour EMA/MACD lisses
            self.incremental_cache[symbol] = {}
            
            for tf in self.timeframes:
                self.price_buffers[symbol][tf] = []
                self.volume_buffers[symbol][tf] = []
                self.high_buffers[symbol][tf] = []
                self.low_buffers[symbol][tf] = []
                self.rsi_buffers[symbol][tf] = []
                self.macd_buffers[symbol][tf] = {'ema7': None, 'ema26': None, 'signal9': None}
                
                # Cache pour chaque timeframe
                self.incremental_cache[symbol][tf] = {}
                
        # Configuration pour sauvegarde p√©riodique des buffers
        self.redis_client = None
        self._init_redis_for_buffers()
        self.buffer_save_interval = 300  # 5 minutes
        self.last_buffer_save_time = 0.0
        
        logger.info(f"üî• Gateway ULTRA-AVANC√â : {len(self.stream_paths)} streams + indicateurs temps r√©el")
    
    async def _connect(self) -> None:
        """
        √âtablit la connexion WebSocket avec Binance et souscrit aux streams.
        """
        uri = self.base_url
        
        try:
            logger.info(f"Connexion √† Binance WebSocket: {uri}")
            self.ws = await websockets.connect(uri)
            
            # S'abonner aux streams
            subscribe_msg = {
                "method": "SUBSCRIBE",
                "params": self.stream_paths,
                "id": int(time.time() * 1000)
            }
            
            if self.ws:
                await self.ws.send(json.dumps(subscribe_msg))
            logger.info(f"Abonnement envoy√© pour: {', '.join(self.stream_paths)}")
            
            # Attendre la confirmation d'abonnement
            if self.ws:
                response = await self.ws.recv()
            else:
                return
            response_data = json.loads(response)
            
            if 'result' in response_data and response_data['result'] is None:
                logger.info("‚úÖ Abonnement aux streams Binance confirm√©")
            else:
                logger.warning(f"R√©ponse d'abonnement inattendue: {response_data}")
            
            # R√©initialiser le d√©lai de reconnexion
            self.reconnect_delay = 1
            
        except (ConnectionClosed, InvalidStatus) as e:
            logger.error(f"‚ùå Erreur lors de la connexion WebSocket: {str(e)}")
            await self._handle_reconnect()
        except Exception as e:
            logger.error(f"‚ùå Erreur inattendue lors de la connexion: {str(e)}")
            await self._handle_reconnect()
    
    async def _handle_reconnect(self) -> None:
        """
        G√®re la reconnexion en cas de perte de connexion.
        Utilise un backoff exponentiel pour √©viter de surcharger le serveur.
        """
        # V√©rifier si le service est encore en cours d'ex√©cution
        if not self.running:
            return
    
        # Limiter le nombre de tentatives
        max_retries = 20
        retry_count = 0
    
        while self.running and retry_count < max_retries:
            logger.warning(f"Tentative de reconnexion {retry_count+1}/{max_retries} dans {self.reconnect_delay} secondes...")
            await asyncio.sleep(self.reconnect_delay)
        
            # Backoff exponentiel avec plafond plus √©lev√©
            self.reconnect_delay = min(300, self.reconnect_delay * 2)  # 5 minutes max
        
            try:
                # Fermer proprement la connexion existante si elle existe
                if self.ws:
                    try:
                        await self.ws.close()
                    except Exception as e:
                        logger.warning(f"Erreur lors de la fermeture de la connexion existante: {str(e)}")
                
                # √âtablir une nouvelle connexion
                await self._connect()
                return  # Reconnexion r√©ussie
            except Exception as e:
                retry_count += 1
                logger.error(f"‚ùå √âchec de reconnexion: {str(e)}")
    
        logger.critical("Impossible de se reconnecter apr√®s plusieurs tentatives. Arr√™t du service.")
        # Signaler l'arr√™t au service principal
        self.running = False
    
    async def _heartbeat_check(self) -> None:
        """
        V√©rifie r√©guli√®rement si nous recevons toujours des messages.
        Reconnecte si aucun message n'a √©t√© re√ßu depuis trop longtemps.
        """
        while self.running:
            await asyncio.sleep(self.heartbeat_interval)
            
            # Si aucun message re√ßu depuis 2x l'intervalle de heartbeat
            if time.time() - self.last_message_time > self.heartbeat_interval * 2:
                logger.warning(f"‚ùó Aucun message re√ßu depuis {self.heartbeat_interval * 2} secondes. Reconnexion...")
                
                # Fermer la connexion existante
                if self.ws:
                    try:
                        await self.ws.close()
                    except Exception as e:
                        logger.warning(f"Erreur lors de la fermeture de la connexion: {str(e)}")
                
                # Reconnecter
                await self._connect()
    
    def _process_kline_message(self, message: Dict[str, Any]) -> Dict[str, Any]:
        """
        Traite un message de chandelier Binance avec donn√©es enrichies.
        
        Args:
            message: Message brut de Binance
            
        Returns:
            Message format√© enrichi pour le syst√®me RootTrading
        """
        # Extraire les donn√©es pertinentes
        kline = message['k']
        symbol = message['s']
        
        # Cr√©er un message enrichi
        processed_data = {
            # Donn√©es de base
            "symbol": symbol,
            "timeframe": kline['i'],
            "start_time": kline['t'],
            "close_time": kline['T'],
            "open": float(kline['o']),
            "high": float(kline['h']),
            "low": float(kline['l']),
            "close": float(kline['c']),
            "volume": float(kline['v']),
            "quote_volume": float(kline['q']),
            "trade_count": int(kline['n']),
            "taker_buy_volume": float(kline['V']),
            "taker_buy_quote_volume": float(kline['Q']),
            "is_closed": kline['x'],
            
            # Donn√©es enrichies si disponibles
            "enhanced": True
        }
        
        # Ajouter les donn√©es de ticker si disponibles
        if symbol in self.ticker_cache:
            ticker = self.ticker_cache[symbol]
            processed_data.update({
                "price_change_24h": ticker.get('priceChange', 0),
                "price_change_pct_24h": ticker.get('priceChangePercent', 0),
                "volume_24h": ticker.get('volume', 0),
                "high_24h": ticker.get('highPrice', 0),
                "low_24h": ticker.get('lowPrice', 0)
            })
            
        # Ajouter les donn√©es d'orderbook enrichies si disponibles
        if symbol in self.orderbook_cache:
            book = self.orderbook_cache[symbol]
            bids = book.get('bids', [])
            asks = book.get('asks', [])
            sentiment_analysis = book.get('sentiment_analysis', {})
            
            if bids and asks:
                bid_price = float(bids[0][0])
                ask_price = float(asks[0][0])
                mid_price = (bid_price + ask_price) / 2
                spread_pct = ((ask_price - bid_price) / mid_price) * 100
                
                processed_data.update({
                    "bid_price": bid_price,
                    "ask_price": ask_price,
                    "bid_qty": float(bids[0][1]),
                    "ask_qty": float(asks[0][1]),
                    "spread_pct": spread_pct,
                    
                    # ULTRA-AVANC√â : Sentiment analysis complete
                    "orderbook_sentiment": sentiment_analysis
                })
        
        # ULTRA-AVANC√â : Calculer tous les indicateurs techniques en temps r√©el
        if processed_data['is_closed']:
            self._update_technical_indicators(symbol, processed_data)
        
        return processed_data
        
    def _update_technical_indicators(self, symbol: str, candle_data: Dict) -> None:
        """ULTRA-AVANC√â : Met √† jour tous les indicateurs techniques en temps r√©el"""
        timeframe = candle_data['timeframe']
        close_price = candle_data['close']
        high_price = candle_data['high']
        low_price = candle_data['low']
        volume = candle_data['volume']
        
        # üîß FIX CRITIQUE: Mise √† jour ATOMIQUE des buffers pour √©viter d√©salignement
        try:
            # V√©rifier les longueurs actuelles avant modification
            price_len = len(self.price_buffers[symbol][timeframe])
            high_len = len(self.high_buffers[symbol][timeframe])
            low_len = len(self.low_buffers[symbol][timeframe])
            volume_len = len(self.volume_buffers[symbol][timeframe])
            
            # D√©tecter un d√©salignement existant et le corriger AVANT d'ajouter
            if not (price_len == high_len == low_len == volume_len):
                logger.warning(f"üîß D√©salignement d√©tect√© {symbol} {timeframe}: P:{price_len} H:{high_len} L:{low_len} V:{volume_len}")
                
                # Trouver la longueur minimum et aligner TOUS les buffers
                min_len = min(price_len, high_len, low_len, volume_len)
                self.price_buffers[symbol][timeframe] = self.price_buffers[symbol][timeframe][-min_len:]
                self.high_buffers[symbol][timeframe] = self.high_buffers[symbol][timeframe][-min_len:]
                self.low_buffers[symbol][timeframe] = self.low_buffers[symbol][timeframe][-min_len:]
                self.volume_buffers[symbol][timeframe] = self.volume_buffers[symbol][timeframe][-min_len:]
                
                logger.info(f"‚úÖ Buffers r√©align√©s √† {min_len} √©l√©ments pour {symbol} {timeframe}")
            
            # Mise √† jour ATOMIQUE : si un buffer doit √™tre tronqu√©, TOUS le sont
            max_buffer_size = 200
            current_size = len(self.price_buffers[symbol][timeframe])
            
            if current_size >= max_buffer_size:
                # Tronquer TOUS les buffers en m√™me temps
                self.price_buffers[symbol][timeframe].pop(0)
                self.high_buffers[symbol][timeframe].pop(0)
                self.low_buffers[symbol][timeframe].pop(0)
                self.volume_buffers[symbol][timeframe].pop(0)
            
            # Ajouter TOUS les nouveaux √©l√©ments en m√™me temps
            self.price_buffers[symbol][timeframe].append(close_price)
            self.high_buffers[symbol][timeframe].append(high_price)
            self.low_buffers[symbol][timeframe].append(low_price)
            self.volume_buffers[symbol][timeframe].append(volume)
            
            # V√©rification post-mise √† jour
            final_lengths = [
                len(self.price_buffers[symbol][timeframe]),
                len(self.high_buffers[symbol][timeframe]),
                len(self.low_buffers[symbol][timeframe]),
                len(self.volume_buffers[symbol][timeframe])
            ]
            
            if not all(length == final_lengths[0] for length in final_lengths):
                logger.error(f"‚ùå ERREUR CRITIQUE: Buffers encore d√©salign√©s apr√®s correction {symbol} {timeframe}: {final_lengths}")
                # Force synchronization as last resort
                min_final_len = min(final_lengths)
                self.price_buffers[symbol][timeframe] = self.price_buffers[symbol][timeframe][-min_final_len:]
                self.high_buffers[symbol][timeframe] = self.high_buffers[symbol][timeframe][-min_final_len:]
                self.low_buffers[symbol][timeframe] = self.low_buffers[symbol][timeframe][-min_final_len:]
                self.volume_buffers[symbol][timeframe] = self.volume_buffers[symbol][timeframe][-min_final_len:]
            
        except Exception as e:
            logger.error(f"‚ùå Erreur mise √† jour buffers {symbol} {timeframe}: {e}")
            # En cas d'erreur critique, recr√©er des buffers propres
            self.price_buffers[symbol][timeframe] = [close_price]
            self.high_buffers[symbol][timeframe] = [high_price]
            self.low_buffers[symbol][timeframe] = [low_price]
            self.volume_buffers[symbol][timeframe] = [volume]
        
        prices = self.price_buffers[symbol][timeframe]
        highs = self.high_buffers[symbol][timeframe]
        lows = self.low_buffers[symbol][timeframe]
        volumes = self.volume_buffers[symbol][timeframe]
        
        # üöÄ HYBRIDE UNIFI√â : Toujours calculer TOUS les indicateurs gr√¢ce aux donn√©es historiques
        if len(prices) >= 1:  # M√™me avec 1 seul point, on peut utiliser l'historique !
            # V√©rifier que tous les buffers sont bien align√©s
            all_lengths = [len(prices), len(highs), len(lows), len(volumes)]
            if all(length == all_lengths[0] for length in all_lengths):
                logger.info(f"üìä HYBRIDE WebSocket {symbol} {timeframe}: buffers=[P:{len(prices)},H:{len(highs)},L:{len(lows)},V:{len(volumes)}] ‚Üí calcul COMPLET avec historique ‚úÖ ALIGN√âS")
            else:
                logger.error(f"‚ùå BUFFERS D√âSALIGN√âS {symbol} {timeframe}: [P:{len(prices)},H:{len(highs)},L:{len(lows)},V:{len(volumes)}] - ARR√äT DU CALCUL")
                
                # üîß CORRECTION D'URGENCE: Force l'alignement imm√©diat
                min_buffer_len = min(all_lengths)
                self.price_buffers[symbol][timeframe] = self.price_buffers[symbol][timeframe][-min_buffer_len:]
                self.high_buffers[symbol][timeframe] = self.high_buffers[symbol][timeframe][-min_buffer_len:]
                self.low_buffers[symbol][timeframe] = self.low_buffers[symbol][timeframe][-min_buffer_len:]
                self.volume_buffers[symbol][timeframe] = self.volume_buffers[symbol][timeframe][-min_buffer_len:]
                
                logger.warning(f"üö® CORRECTION D'URGENCE APPLIQU√âE: Buffers forc√©s √† {min_buffer_len} √©l√©ments")
                return  # Skip calculation if buffers are misaligned despite correction
            
            # **NOUVEAU**: Compl√©ter avec les donn√©es historiques si pas assez de points (ULTRA-PR√âCIS)
            extended_prices, extended_highs, extended_lows, extended_volumes = self._get_extended_buffers_sync(
                symbol, timeframe, prices, highs, lows, volumes, min_required=200
            )
            
            # Import new centralized modules with cache
            from market_analyzer.indicators import (
                calculate_rsi, calculate_ema, calculate_macd,
                calculate_bollinger_bands, calculate_atr, calculate_adx,
                calculate_stochastic, calculate_williams_r, calculate_cci,
                calculate_vwap, calculate_obv,
                get_cached_indicators
            )
            
            # **CRITIQUE**: R√©cup√©rer TOUS les indicateurs calcul√©s avec cache unifi√©
            all_indicators = get_cached_indicators(
                highs=extended_highs,
                lows=extended_lows, 
                closes=extended_prices,
                volumes=extended_volumes,
                symbol=symbol,
                enable_cache=True
            )
            
            logger.debug(f"üîÑ {len(all_indicators)} indicateurs calcul√©s avec cache unifi√© pour {symbol} {timeframe}")
            
            # Cache is already handled by get_cached_indicators - no manual saving needed
            
            # Debug: v√©rifier si ADX est calcul√©
            if 'adx_14' in all_indicators and all_indicators['adx_14'] is not None:
                logger.info(f"‚úÖ ADX disponible dans WebSocket {symbol} {timeframe}: {all_indicators['adx_14']}")
            else:
                logger.warning(f"‚ùå ADX manquant dans WebSocket {symbol} {timeframe}")
            
            # üöÄ NOUVEAU : Calcul incr√©mental pour EMA/MACD (√©vite dents de scie)
            incremental_indicators = self._calculate_smooth_indicators(symbol, timeframe, candle_data, all_indicators)
            
            # FUSION INTELLIGENTE : Garder tous les indicateurs + override EMA/MACD avec versions lisses
            final_indicators = all_indicators.copy()
            # Override seulement les indicateurs EMA/MACD avec les versions incr√©mentales lisses
            for indicator_name, value in incremental_indicators.items():
                if value is not None and indicator_name in ['ema_7', 'ema_26', 'ema_99', 'macd_line', 'macd_signal', 'macd_histogram']:
                    final_indicators[indicator_name] = value
                    # Cache is already handled by the individual indicator functions
            
            # Ajouter tous les indicateurs calcul√©s
            for indicator_name, value in final_indicators.items():
                if value is not None:
                    candle_data[indicator_name] = value
            
            logger.info(f"‚úÖ {len(final_indicators)} indicateurs COMPLETS pour {symbol} {timeframe} (üöÄ {len(incremental_indicators)} incr√©mentaux)")
            
            # Marquer comme totalement enrichi
            candle_data['enhanced'] = True
            candle_data['ultra_enriched'] = True
                
    def _calculate_rsi(self, prices: List[float], period: int = 14) -> Optional[float]:
        """Calcule le RSI via le module centralis√©"""
        from market_analyzer.indicators.momentum.rsi import calculate_rsi
        return calculate_rsi(prices, period)
        
    def _calculate_stoch_rsi(self, prices: List[float], period: int = 14) -> Optional[float]:
        """Calcule le Stochastic RSI"""
        if len(prices) < period * 2:
            return None
            
        # Calculer RSI sur les derni√®res p√©riodes
        rsi_values = []
        for i in range(period, len(prices) + 1):
            rsi = self._calculate_rsi(prices[:i], period)
            if rsi:
                rsi_values.append(rsi)
                
        if len(rsi_values) < period:
            return None
            
        recent_rsi = rsi_values[-period:]
        min_rsi = min(recent_rsi)
        max_rsi = max(recent_rsi)
        current_rsi = rsi_values[-1]
        
        if max_rsi == min_rsi:
            return 50
            
        return ((current_rsi - min_rsi) / (max_rsi - min_rsi)) * 100
        
    def _calculate_ema(self, prices: List[float], period: int) -> float:
        """Calcule l'EMA via le module centralis√©"""
        from market_analyzer.indicators.trend.moving_averages import calculate_ema
        result = calculate_ema(prices, period)
        return result if result is not None else (prices[-1] if prices else 0)
        
    def _calculate_macd(self, prices: List[float]) -> Optional[Dict]:
        """Calcule MACD complet via le module centralis√©"""
        from market_analyzer.indicators.trend.macd import calculate_macd_series
        
        result = calculate_macd_series(prices)
        if result is None:
            return None
        
        # Find last valid values
        macd_line = None
        macd_signal = None
        macd_histogram = None
        
        for i in range(len(result) - 1, -1, -1):
            if result[i] is not None:
                macd_line = result[i]['macd_line']
                macd_signal = result[i]['macd_signal']
                macd_histogram = result[i]['macd_histogram']
                break
        
        if macd_line is None:
            return None
            
        return {
            'macd_line': macd_line,
            'macd_signal': macd_signal,
            'macd_histogram': macd_histogram
        }
        
    def _calculate_bollinger_bands(self, prices: List[float], period: int, std_dev: float) -> Optional[Dict]:
        """Calcule les Bollinger Bands via le module centralis√©"""
        from market_analyzer.indicators.volatility.bollinger import calculate_bollinger_bands_series
        
        result = calculate_bollinger_bands_series(prices, period, std_dev)
        if result is None:
            return None
        
        # Find last valid values
        bb_upper = None
        bb_middle = None
        bb_lower = None
        
        for i in range(len(result) - 1, -1, -1):
            if result[i] is not None:
                bb_upper = result[i]['upper']
                bb_middle = result[i]['middle']
                bb_lower = result[i]['lower']
                break
        
        if bb_upper is None:
            return None
            
        current_price = prices[-1]
        bb_position = ((current_price - bb_lower) / (bb_upper - bb_lower)) * 100 if bb_upper != bb_lower else 50
        bb_width = ((bb_upper - bb_lower) / bb_middle) * 100 if bb_middle != 0 else 0
            
        return {
            'bb_upper': bb_upper,
            'bb_middle': bb_middle, 
            'bb_lower': bb_lower,
            'bb_position': bb_position,
            'bb_width': bb_width
        }
        
    def _calculate_vwap(self, prices: List[float], volumes: List[float]) -> Optional[float]:
        """Calcule le VWAP (Volume Weighted Average Price)"""
        if len(prices) != len(volumes) or not prices:
            return None
            
        total_pv = sum(p * v for p, v in zip(prices, volumes))
        total_volume = sum(volumes)
        
        return total_pv / total_volume if total_volume > 0 else None
        
    def _analyze_volume_profile(self, volumes: List[float]) -> Dict:
        """Analyse avanc√©e du profil de volume"""
        if len(volumes) < 5:
            return {}
            
        recent_vol = volumes[-1]
        avg_vol = sum(volumes) / len(volumes)
        
        vol_ratio = recent_vol / avg_vol if avg_vol > 0 else 1
        
        # D√©tection de pic de volume
        vol_spike = vol_ratio > 2.0
        
        # Tendance du volume
        if len(volumes) >= 3:
            vol_trend = "increasing" if volumes[-1] > volumes[-2] > volumes[-3] else "decreasing"
        else:
            vol_trend = "stable"
            
        return {
            'volume_ratio': vol_ratio,
            'volume_spike': vol_spike,
            'volume_trend': vol_trend,
            'avg_volume': avg_vol
        }
        
    def _calculate_momentum(self, prices: List[float], period: int) -> Optional[float]:
        """Calcule le momentum"""
        if len(prices) < period + 1:
            return None
            
        return ((prices[-1] - prices[-period-1]) / prices[-period-1]) * 100
        
    def _calculate_atr(self, symbol: str, timeframe: str, period: int) -> Optional[float]:
        """Calcule l'ATR (Average True Range)"""
        # Pour l'ATR, on a besoin des high/low pr√©c√©dents, simplifi√© ici
        prices = self.price_buffers.get(symbol, {}).get(timeframe, [])
        if len(prices) < period + 1:
            return None
            
        # Approximation : utiliser les variations de prix comme proxy
        ranges = [abs(prices[i] - prices[i-1]) for i in range(1, len(prices))]
        if len(ranges) < period:
            return None
            
        return sum(ranges[-period:]) / period
        
    def _calculate_adx(self, symbol: str, timeframe: str, period: int) -> Optional[float]:
        """Calcule l'ADX en utilisant le module partag√©"""
        if symbol not in self.high_buffers or symbol not in self.low_buffers:
            return None
            
        if timeframe not in self.high_buffers[symbol] or timeframe not in self.low_buffers[symbol]:
            return None
            
        highs = self.high_buffers.get(symbol, {}).get(timeframe, [])
        lows = self.low_buffers.get(symbol, {}).get(timeframe, [])
        closes = self.price_buffers.get(symbol, {}).get(timeframe, [])
        
        if len(highs) < period + 1 or len(lows) < period + 1 or len(closes) < period + 1:
            return None
        
        try:
            # Utiliser le module centralis√© pour le calcul ADX
            from market_analyzer.indicators.trend.adx import calculate_adx
            
            return calculate_adx(highs, lows, closes, period)
            
        except Exception as e:
            logger.debug(f"Erreur calcul ADX pour {symbol}: {e}")
            return None
        
    def _calculate_williams_r(self, symbol: str, timeframe: str, period: int) -> Optional[float]:
        """Calcule Williams %R"""
        prices = self.price_buffers.get(symbol, {}).get(timeframe, [])
        if len(prices) < period:
            return None
            
        recent_prices = prices[-period:]
        highest = max(recent_prices)
        lowest = min(recent_prices)
        current = prices[-1]
        
        if highest == lowest:
            return -50
            
        return ((highest - current) / (highest - lowest)) * -100
        
    def _calculate_cci(self, symbol: str, timeframe: str, period: int) -> Optional[float]:
        """Calcule CCI (Commodity Channel Index)"""
        prices = self.price_buffers.get(symbol, {}).get(timeframe, [])
        if len(prices) < period:
            return None
            
        recent_prices = prices[-period:]
        typical_price = sum(recent_prices) / len(recent_prices)
        sma = typical_price
        
        # Approximation de l'√©cart moyen
        mean_deviation = sum(abs(p - sma) for p in recent_prices) / len(recent_prices)
        
        if mean_deviation == 0:
            return 0
            
        return (prices[-1] - sma) / (0.015 * mean_deviation)
        
    def _process_ticker_message(self, data: Dict[str, Any]) -> None:
        """Traite les donn√©es ticker 24h"""
        symbol = data['s']
        self.ticker_cache[symbol] = {
            'priceChange': float(data.get('P', 0)),
            'priceChangePercent': float(data.get('p', 0)),
            'volume': float(data.get('v', 0)),
            'highPrice': float(data.get('h', 0)),
            'lowPrice': float(data.get('l', 0))
        }
        
    def _process_depth_message(self, data: Dict[str, Any]) -> None:
        """Traite les donn√©es orderbook avec analyse de sentiment avanc√©e"""
        symbol = data['s']
        bids = data.get('b', [])
        asks = data.get('a', [])
        
        # Analyse avanc√©e du sentiment via orderbook
        sentiment_analysis = self._analyze_orderbook_sentiment(bids, asks)
        
        self.orderbook_cache[symbol] = {
            'bids': bids,
            'asks': asks,
            'sentiment_analysis': sentiment_analysis,
            'timestamp': time.time()
        }
        
    def _analyze_orderbook_sentiment(self, bids: List, asks: List) -> Dict:
        """ULTRA-AVANC√â : Analyse compl√®te du sentiment via l'orderbook"""
        try:
            if not bids or not asks:
                return {}
                
            # Convertir en float pour calculs
            bid_data = [(float(price), float(qty)) for price, qty in bids[:20]]  # Top 20
            ask_data = [(float(price), float(qty)) for price, qty in asks[:20]]
            
            if not bid_data or not ask_data:
                return {}
                
            # 1. Ratio bid/ask volume
            total_bid_volume = sum(qty for _, qty in bid_data)
            total_ask_volume = sum(qty for _, qty in ask_data)
            
            bid_ask_ratio = total_bid_volume / total_ask_volume if total_ask_volume > 0 else 1
            
            # 2. Spread analysis
            best_bid = bid_data[0][0]
            best_ask = ask_data[0][0]
            spread = best_ask - best_bid
            mid_price = (best_bid + best_ask) / 2
            spread_pct = (spread / mid_price) * 100
            
            # 3. Order book imbalance
            imbalance = (total_bid_volume - total_ask_volume) / (total_bid_volume + total_ask_volume)
            
            # 4. Depth analysis (support/resistance strength)
            bid_depth_strength = self._calculate_depth_strength(bid_data, best_bid, 'support')
            ask_depth_strength = self._calculate_depth_strength(ask_data, best_ask, 'resistance')
            
            # 5. Wall detection (gros ordres)
            bid_walls = self._detect_walls(bid_data, 'bid')
            ask_walls = self._detect_walls(ask_data, 'ask')
            
            # 6. Price level concentration
            bid_concentration = self._calculate_price_concentration(bid_data)
            ask_concentration = self._calculate_price_concentration(ask_data)
            
            # 7. Sentiment global
            sentiment_score = self._calculate_sentiment_score(
                bid_ask_ratio, imbalance, bid_depth_strength, ask_depth_strength,
                len(bid_walls), len(ask_walls)
            )
            
            # 8. Liquidit√© totale
            total_liquidity = total_bid_volume + total_ask_volume
            
            # 9. Ratio des 5 premiers niveaux vs le reste
            top5_bid_vol = sum(qty for _, qty in bid_data[:5])
            top5_ask_vol = sum(qty for _, qty in ask_data[:5])
            top5_ratio = (top5_bid_vol + top5_ask_vol) / total_liquidity if total_liquidity > 0 else 0
            
            return {
                'bid_ask_ratio': round(bid_ask_ratio, 3),
                'spread_pct': round(spread_pct, 4),
                'imbalance': round(imbalance, 3),
                'bid_depth_strength': round(bid_depth_strength, 3),
                'ask_depth_strength': round(ask_depth_strength, 3),
                'bid_walls': bid_walls,
                'ask_walls': ask_walls,
                'bid_concentration': round(bid_concentration, 3),
                'ask_concentration': round(ask_concentration, 3),
                'sentiment_score': round(sentiment_score, 3),
                'total_liquidity': round(total_liquidity, 2),
                'top5_concentration': round(top5_ratio, 3),
                'sentiment_signal': self._interpret_sentiment(sentiment_score, imbalance, bid_ask_ratio)
            }
            
        except Exception as e:
            logger.error(f"‚ùå Erreur analyse sentiment orderbook: {e}")
            return {}
            
    def _calculate_depth_strength(self, orders: List[Tuple], best_price: float, side: str) -> float:
        """Calcule la force de profondeur des ordres"""
        if not orders:
            return 0
            
        # Calculer la force bas√©e sur le volume et la distance du meilleur prix
        total_strength = 0
        
        for price, qty in orders:
            # Distance relative du meilleur prix
            if side == 'support':
                distance = (best_price - price) / best_price
            else:  # resistance
                distance = (price - best_price) / best_price
                
            # Force = volume / (1 + distance) - plus pr√®s = plus fort
            strength = qty / (1 + distance * 100)  # *100 pour normaliser
            total_strength += strength
            
        return total_strength
        
    def _detect_walls(self, orders: List[Tuple], side: str) -> List[Dict]:
        """D√©tecte les murs d'ordres (gros volumes)"""
        if not orders:
            return []
            
        # Calculer le volume moyen
        volumes = [qty for _, qty in orders]
        avg_volume = sum(volumes) / len(volumes)
        
        # D√©tecter les volumes > 3x la moyenne
        walls = []
        for price, qty in orders:
            if qty > avg_volume * 3:
                walls.append({
                    'price': price,
                    'volume': qty,
                    'ratio_to_avg': qty / avg_volume,
                    'side': side
                })
                
        return sorted(walls, key=lambda x: x['volume'], reverse=True)[:5]  # Top 5
        
    def _calculate_price_concentration(self, orders: List[Tuple]) -> float:
        """Calcule la concentration des ordres par niveau de prix"""
        if len(orders) < 3:
            return 0
            
        # Calculer l'√©cart-type des volumes pour mesurer la concentration
        volumes = [qty for _, qty in orders]
        avg_vol = sum(volumes) / len(volumes)
        
        variance = sum((vol - avg_vol) ** 2 for vol in volumes) / len(volumes)
        std_dev = variance ** 0.5
        
        # Normaliser : plus l'√©cart-type est √©lev√©, plus la concentration est in√©gale
        return std_dev / avg_vol if avg_vol > 0 else 0
        
    def _calculate_sentiment_score(self, bid_ask_ratio: float, imbalance: float, 
                                 bid_strength: float, ask_strength: float,
                                 bid_walls: int, ask_walls: int) -> float:
        """Calcule un score de sentiment global (-1 = tr√®s bearish, +1 = tr√®s bullish)"""
        
        # Composants du sentiment
        ratio_score = 0.0
        if bid_ask_ratio > 1.2:
            ratio_score = 0.3  # Plus de volume bid = bullish
        elif bid_ask_ratio < 0.8:
            ratio_score = -0.3  # Plus de volume ask = bearish
            
        # Imbalance score (directement utilisable entre -1 et 1)
        imbalance_score = imbalance * 0.4
        
        # Depth strength comparison
        if bid_strength > ask_strength * 1.2:
            depth_score = 0.2  # Support plus fort
        elif ask_strength > bid_strength * 1.2:
            depth_score = -0.2  # R√©sistance plus forte
        else:
            depth_score = 0.0
            
        # Wall advantage
        wall_score = 0.0
        if bid_walls > ask_walls:
            wall_score = 0.1  # Plus de murs bid
        elif ask_walls > bid_walls:
            wall_score = -0.1  # Plus de murs ask
            
        # Score final
        total_score = ratio_score + imbalance_score + depth_score + wall_score
        
        # Limiter entre -1 et 1
        return max(-1, min(1, total_score))
        
    def _interpret_sentiment(self, sentiment_score: float, imbalance: float, bid_ask_ratio: float) -> str:
        """Interpr√®te le score de sentiment en signal actionnable"""
        
        if sentiment_score > 0.5:
            return "VERY_BULLISH"
        elif sentiment_score > 0.2:
            return "BULLISH"
        elif sentiment_score < -0.5:
            return "VERY_BEARISH"
        elif sentiment_score < -0.2:
            return "BEARISH"
        else:
            return "NEUTRAL"
    
    async def _listen(self) -> None:
        """
        √âcoute les messages WebSocket et les traite.
        """
        while self.running:
            try:
                if not self.ws:
                    logger.warning("Connexion WebSocket perdue, reconnexion...")
                    await self._connect()
                    continue
                
                # Recevoir un message
                message = await self.ws.recv()
                self.last_message_time = time.time()
                
                # Traiter le message
                try:
                    data = json.loads(message)
                    
                    # V√©rifier le type de message
                    if 'e' in data:
                        event_type = data['e']
                        
                        # Traiter les donn√©es de chandelier
                        if event_type == 'kline':
                            processed_data = self._process_kline_message(data)

                            # üëâ on ignore les bougies encore ouvertes
                            if not processed_data['is_closed']:
                                continue

                            symbol = processed_data['symbol']
                            timeframe = processed_data['timeframe']
                            
                            # Topic sp√©cifique au timeframe
                            topic = f"market.data.{symbol.lower()}.{timeframe}"
                            key = f"{symbol}:{timeframe}:{processed_data['start_time']}"

                            # Nettoyer les donn√©es pour √©viter les erreurs de format
                            clean_data = {}
                            for k, v in processed_data.items():
                                if isinstance(v, str):
                                    # √âchapper les caract√®res de formatage probl√©matiques
                                    clean_data[k] = v.replace('{', '{{').replace('}', '}}') if '{' in v or '}' in v else v
                                else:
                                    clean_data[k] = v
                            
                            # Log des cl√©s pour d√©bogage
                            logger.debug(f"Donn√©es envoy√©es √† Kafka - Cl√©s: {list(clean_data.keys())}")

                            # Publier avec la m√©thode appropri√©e
                            if hasattr(self.kafka_client, 'publish_to_topic'):
                                self.kafka_client.publish_to_topic(
                                    topic=topic,
                                    data=clean_data,
                                    key=key
                                )
                            else:
                                # Fallback pour KafkaProducer - utiliser produce() au lieu de publish_market_data()
                                self.kafka_client.produce(
                                    topic="market_data",
                                    message=clean_data,
                                    key=symbol
                                )
        
                            # Log enrichi
                            spread_pct = processed_data.get('spread_pct', 0)
                            spread_info = f" Spread:{spread_pct:.3f}%" if isinstance(spread_pct, (int, float)) and 'spread_pct' in processed_data else ""
                            logger.info(f"üìä {symbol} {timeframe}: {processed_data['close']} "
                                      f"[O:{processed_data['open']} H:{processed_data['high']} L:{processed_data['low']}]"
                                      f"{spread_info}")
                                      
                        # Traiter les donn√©es ticker
                        elif event_type == '24hrTicker':
                            self._process_ticker_message(data)
                            
                        # Traiter les donn√©es orderbook
                        elif event_type == 'depthUpdate':
                            self._process_depth_message(data)
                except json.JSONDecodeError:
                    logger.error(f"Message non-JSON re√ßu: {message[:100]}...")
                except Exception as e:
                    error_msg = str(e).replace('{', '{{').replace('}', '}}')
                    logger.error(f"Erreur lors du traitement du message: {error_msg}")
                
            except ConnectionClosed as e:
                logger.warning(f"Connexion WebSocket ferm√©e: {str(e)}")
                await self._handle_reconnect()
            except Exception as e:
                logger.error(f"Erreur durant l'√©coute: {str(e)}")
                await asyncio.sleep(1)  # Pause pour √©viter une boucle d'erreur infinie
    
    async def start(self) -> None:
        """
        D√©marre la connexion WebSocket et l'√©coute des messages.
        """
        if self.running:
            logger.warning("Le WebSocket est d√©j√† en cours d'ex√©cution.")
            return
            
        self.running = True
        self.last_message_time = time.time()
        
        try:
            # üîÑ NOUVEAU : Restaurer les buffers WebSocket depuis Redis
            logger.info("üîÑ Restoration des buffers WebSocket depuis Redis...")
            restored_buffers = await self._restore_buffers_from_redis()
            
            if restored_buffers > 0:
                logger.info(f"‚úÖ {restored_buffers} buffers WebSocket restaur√©s - CONTINUIT√â PR√âSERV√âE")
            else:
                logger.info("üÜï Aucun buffer restaur√© - Premier d√©marrage ou cache vide")
            
            # üöÄ NOUVEAU : S'assurer que les buffers ont assez de donn√©es pour tous les indicateurs
            await self._ensure_sufficient_buffer_data()
            
            # üöÄ Initialiser le cache incr√©mental (compl√©ter si pas restaur√©)
            logger.info("üíæ Initialisation du cache incr√©mental pour EMA/MACD lisses...")
            for symbol in self.symbols:
                for timeframe in self.timeframes:
                    self._initialize_incremental_cache_simple(symbol, timeframe)
            logger.info("‚úÖ Cache incr√©mental initialis√© pour tous les symboles/timeframes")
            
            # D√©marrer la connexion
            await self._connect()
            
            # D√©marrer les t√¢ches parall√®les
            heartbeat_task = asyncio.create_task(self._heartbeat_check())
            buffer_save_task = asyncio.create_task(self._periodic_buffer_save())
            
            logger.info("üöÄ WebSocket + Sauvegarde p√©riodique d√©marr√©s")
            
            # √âcouter les messages
            await self._listen()
            
            # Annuler les t√¢ches parall√®les si on sort de la boucle
            heartbeat_task.cancel()
            buffer_save_task.cancel()
            
        except Exception as e:
            logger.error(f"Erreur critique dans la boucle WebSocket: {str(e)}")
            self.running = False
    
    async def stop(self) -> None:
        """
        Arr√™te la connexion WebSocket proprement.
        """
        logger.info("Arr√™t de la connexion WebSocket Binance...")
        self.running = False
        
        # üíæ Sauvegarder les buffers avant l'arr√™t pour pr√©server la continuit√©
        logger.info("üíæ Sauvegarde finale des buffers WebSocket...")
        try:
            await self._save_buffers_to_redis()
            logger.info("‚úÖ Buffers sauvegard√©s pour la prochaine restoration")
        except Exception as e:
            logger.warning(f"Erreur sauvegarde finale: {e}")
        
        if self.ws:
            try:
                # D√©sabonnement des streams
                unsubscribe_msg = {
                    "method": "UNSUBSCRIBE",
                    "params": self.stream_paths,
                    "id": int(time.time() * 1000)
                }
                
                await self.ws.send(json.dumps(unsubscribe_msg))
                logger.info("Message de d√©sabonnement envoy√©")
                
                # Fermer la connexion
                await self.ws.close()
                logger.info("Connexion WebSocket ferm√©e")
                
            except Exception as e:
                logger.error(f"Erreur lors de la fermeture du WebSocket: {str(e)}")
        
        # Fermer le client Kafka
        if self.kafka_client:
            self.kafka_client.flush()
            self.kafka_client.close()
            logger.info("Client Kafka ferm√©")

    def _calculate_smooth_indicators(self, symbol: str, timeframe: str, candle_data: Dict, all_indicators: Optional[Dict[Any, Any]] = None) -> Dict:
        """
        üöÄ NOUVEAU : Calcule EMA/MACD de mani√®re incr√©mentale pour √©viter les dents de scie.
        
        Args:
            symbol: Symbole trad√©
            timeframe: Intervalle de temps  
            candle_data: Donn√©es de la bougie actuelle
            
        Returns:
            Dict avec indicateurs EMA/MACD lisses
        """
        result = {}
        current_price = candle_data['close']
        
        try:
            # Cache pour ce symbole/timeframe
            cache = self.incremental_cache[symbol][timeframe]
            
            # üìà EMA 7, 26, 99 (incr√©mentaux avec initialisation intelligente)
            # Using incremental calculation directly
            
            for period in [7, 26, 99]:
                cache_ema_key = f'ema_{period}'
                prev_ema = cache.get(cache_ema_key)
                
                if prev_ema is None:
                    # Premi√®re fois : utiliser la valeur d√©j√† calcul√©e dans all_indicators
                    if all_indicators and cache_ema_key in all_indicators and all_indicators[cache_ema_key] is not None:
                        traditional_ema = all_indicators[cache_ema_key]
                        cache[cache_ema_key] = traditional_ema
                        result[cache_ema_key] = traditional_ema
                        logger.debug(f"üéØ EMA {period} initialis√©e depuis all_indicators: {traditional_ema:.4f}")
                        continue
                    else:
                        # **NOUVEAU**: Fallback vers le cache persistant si all_indicators manque
                        from shared.src.indicator_cache import get_indicator_cache
                        cache_client = get_indicator_cache()
                        cached_ema = cache_client.get(symbol, timeframe, cache_ema_key)
                        if cached_ema is not None:
                            cache[cache_ema_key] = cached_ema
                            prev_ema = cached_ema
                            logger.debug(f"üîÑ EMA {period} restaur√©e depuis cache persistant: {cached_ema:.4f}")
                        else:
                            logger.debug(f"‚ö†Ô∏è EMA {period} non disponible, skip incr√©mental")
                            continue
                
                # Calcul incr√©mental : EMA_new = Œ± √ó price + (1-Œ±) √ó EMA_prev
                alpha = 2 / (period + 1)
                new_ema = alpha * current_price + (1 - alpha) * prev_ema
                result[cache_ema_key] = new_ema
                
                # Mettre √† jour le cache
                cache[cache_ema_key] = new_ema
            
            # üìä MACD incr√©mental (bas√© sur EMA 7/26 du cache)
            # MACD incremental calculation using EMA values from cache
            
            prev_ema_fast = cache.get('macd_ema_fast')  # EMA 7 pour MACD
            prev_ema_slow = cache.get('macd_ema_slow')  # EMA 26 pour MACD  
            prev_macd_signal = cache.get('macd_signal')
            
            # Utiliser les EMA du cache si disponibles
            if cache.get('ema_7') is not None and cache.get('ema_26') is not None:
                # Synchroniser les EMA MACD avec les EMA principales
                if prev_ema_fast is None:
                    cache['macd_ema_fast'] = cache['ema_7']
                    prev_ema_fast = cache['ema_7']
                if prev_ema_slow is None:
                    cache['macd_ema_slow'] = cache['ema_26']
                    prev_ema_slow = cache['ema_26']
            else:
                # **NOUVEAU**: Fallback vers le cache persistant pour les EMA MACD
                from shared.src.indicator_cache import get_indicator_cache
                cache_client = get_indicator_cache()
                if prev_ema_fast is None:
                    # MIGRATION BINANCE: Utiliser directement ema_7
                    cached_fast = cache_client.get(symbol, timeframe, 'ema_7')
                    if cached_fast is not None:
                        cache['macd_ema_fast'] = cached_fast
                        prev_ema_fast = cached_fast
                if prev_ema_slow is None:
                    cached_slow = cache_client.get(symbol, timeframe, 'ema_26')
                    if cached_slow is not None:
                        cache['macd_ema_slow'] = cached_slow
                        prev_ema_slow = cached_slow
                if prev_macd_signal is None:
                    cached_signal = cache_client.get(symbol, timeframe, 'macd_signal')
                    if cached_signal is not None:
                        cache['macd_signal'] = cached_signal
                        prev_macd_signal = cached_signal
            
            # Calculate MACD components incrementally
            if prev_ema_fast is not None and prev_ema_slow is not None:
                # Update EMAs incrementally
                alpha_fast = 2 / (7 + 1)  # For EMA 7
                alpha_slow = 2 / (26 + 1)  # For EMA 26
                
                new_ema_fast = alpha_fast * current_price + (1 - alpha_fast) * prev_ema_fast
                new_ema_slow = alpha_slow * current_price + (1 - alpha_slow) * prev_ema_slow
                
                # MACD line
                new_macd_line = new_ema_fast - new_ema_slow
                
                # MACD signal (EMA 9 of MACD line)
                if prev_macd_signal is not None:
                    alpha_signal = 2 / (9 + 1)
                    new_macd_signal = alpha_signal * new_macd_line + (1 - alpha_signal) * prev_macd_signal
                else:
                    new_macd_signal = new_macd_line  # First value
                
                # MACD histogram
                new_macd_histogram = new_macd_line - new_macd_signal
                
                macd_result = {
                    'ema_fast': new_ema_fast,
                    'ema_slow': new_ema_slow,
                    'macd_line': new_macd_line,
                    'macd_signal': new_macd_signal,
                    'macd_histogram': new_macd_histogram
                }
            else:
                # Skip if no previous values
                macd_result = {
                    'ema_fast': None,
                    'ema_slow': None,
                    'macd_line': None,
                    'macd_signal': None,
                    'macd_histogram': None
                }
            
            if macd_result['macd_line'] is not None:
                result.update({
                    'macd_line': macd_result['macd_line'],
                    'macd_signal': macd_result['macd_signal'],
                    'macd_histogram': macd_result['macd_histogram']
                })
                
                # Mettre √† jour le cache MACD
                cache['macd_ema_fast'] = macd_result['ema_fast']
                cache['macd_ema_slow'] = macd_result['ema_slow'] 
                cache['macd_signal'] = macd_result['macd_signal']
            
            if result:
                logger.debug(f"üöÄ Indicateurs lisses calcul√©s pour {symbol} {timeframe}: "
                            f"EMA7={result.get('ema_7', 0):.4f}, "
                            f"MACD={result.get('macd_line', 0):.4f}")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur calcul indicateurs incr√©mentaux {symbol} {timeframe}: {e}")
            # En cas d'erreur, retourner un dict vide (fallback vers calcul traditionnel)
            result = {}
        
        return result

    def _initialize_incremental_cache_simple(self, symbol: str, timeframe: str):
        """
        üîÑ Initialise le cache incr√©mental avec restauration depuis le cache persistant.
        Assure la continuit√© entre donn√©es historiques et temps r√©el.
        """
        try:
            cache = self.incremental_cache[symbol][timeframe]
            
            # **NOUVEAU**: Restaurer depuis le cache persistant d'abord
            from shared.src.indicator_cache import get_indicator_cache
            cache_client = get_indicator_cache()
            
            restored_count = 0
            indicator_keys = ['ema_7', 'ema_26', 'ema_99', 'macd_line', 'macd_signal', 'macd_histogram']
            
            for indicator_key in indicator_keys:
                # Restaurer depuis Redis si disponible
                cached_value = cache_client.get(symbol, timeframe, indicator_key)
                if cached_value is not None:
                    # Mapper les indicateurs vers les cl√©s du cache incr√©mental
                    if indicator_key == 'macd_line':
                        # Pas stock√© directement dans le cache incr√©mental
                        pass
                    elif indicator_key == 'macd_histogram':
                        # Pas stock√© directement dans le cache incr√©mental
                        pass
                    else:
                        cache[indicator_key] = cached_value
                        restored_count += 1
                        logger.debug(f"üîÑ {indicator_key} restaur√©: {cached_value:.6f}")
            
            # Restaurer les EMA sp√©cifiques au MACD (synchronisation)
            if cache.get('ema_7') is not None:
                cache['macd_ema_fast'] = cache['ema_7']
                restored_count += 1
            # Si ema_7 n'est pas disponible, on ne fait rien (sera calcul√© la prochaine fois)
                restored_count += 1
            if cache.get('ema_26') is not None:
                cache['macd_ema_slow'] = cache['ema_26']
                restored_count += 1
            if cache_client.get(symbol, timeframe, 'macd_signal') is not None:
                cache['macd_signal'] = cache_client.get(symbol, timeframe, 'macd_signal')
                restored_count += 1
            
            # Initialiser √† None seulement si pas restaur√©
            for key in ['ema_7', 'ema_26', 'ema_99', 'macd_ema_fast', 'macd_ema_slow', 'macd_signal']:
                if key not in cache or cache[key] is None:
                    cache[key] = None
            
            if restored_count > 0:
                logger.info(f"‚úÖ Cache incr√©mental {symbol} {timeframe}: {restored_count} indicateurs restaur√©s (CONTINUIT√â PR√âSERV√âE)")
            else:
                logger.debug(f"üíæ Cache incr√©mental {symbol} {timeframe}: initialis√© vide (premier calcul)")
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erreur initialisation cache pour {symbol} {timeframe}: {e}")
            # Fallback: initialisation vide
            cache = self.incremental_cache[symbol][timeframe]
            for key in ['ema_7', 'ema_26', 'ema_99', 'macd_ema_fast', 'macd_ema_slow', 'macd_signal']:
                cache[key] = None
    
    def _init_redis_for_buffers(self):
        """Initialise Redis pour la sauvegarde des buffers WebSocket"""
        try:
            from shared.src.redis_client import RedisClient
            self.redis_client = RedisClient()
            logger.info("‚úÖ WebSocket connect√© √† Redis pour persistence des buffers")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è WebSocket sans Redis: {e}")
            self.redis_client = None
    
    def _get_extended_buffers_sync(self, symbol: str, timeframe: str, 
                                  current_prices: list, current_highs: list, 
                                  current_lows: list, current_volumes: list, 
                                  min_required: int = 50):
        """
        √âtend les buffers actuels avec les donn√©es historiques de la DB si n√©cessaire (version synchrone).
        
        Args:
            symbol: Symbole de trading
            timeframe: Intervalle de temps
            current_prices/highs/lows/volumes: Buffers actuels
            min_required: Nombre minimum de points requis
            
        Returns:
            Tuple (prices, highs, lows, volumes) √©tendus avec donn√©es historiques
        """
        try:
            # üîß CORRECTION CRITIQUE: Aligner les buffers actuels AVANT extension
            current_lengths = [len(current_prices), len(current_highs), len(current_lows), len(current_volumes)]
            if not all(length == current_lengths[0] for length in current_lengths):
                logger.warning(f"üîß D√©salignement d√©tect√© avant extension {symbol} {timeframe}: P:{len(current_prices)} H:{len(current_highs)} L:{len(current_lows)} V:{len(current_volumes)}")
                
                # Aligner √† la longueur minimum
                min_len = min(current_lengths)
                current_prices = current_prices[-min_len:]
                current_highs = current_highs[-min_len:]
                current_lows = current_lows[-min_len:]
                current_volumes = current_volumes[-min_len:]
                
                logger.info(f"‚úÖ Buffers actuels align√©s √† {min_len} √©l√©ments avant extension")
            
            # Si on a d√©j√† assez de points, retourner les buffers actuels (maintenant align√©s)
            if len(current_prices) >= min_required:
                return current_prices, current_highs, current_lows, current_volumes
            
            # Calculer combien de points historiques on doit r√©cup√©rer (ULTRA-PR√âCIS)
            needed_points = max(min_required - len(current_prices), 500)  # Minimum 500 points
            
            logger.debug(f"üîç Extension buffers {symbol} {timeframe}: {len(current_prices)} points actuels, r√©cup√©ration de {needed_points} points historiques")
            
            # R√©cup√©rer les donn√©es historiques depuis la DB (version synchrone)
            import psycopg2
            from shared.src.config import get_db_config
            
            db_config = get_db_config()
            connection = psycopg2.connect(
                host=db_config['host'],
                port=db_config['port'],
                database=db_config['database'],
                user=db_config['user'],
                password=db_config['password']
            )
            
            query = """
                SELECT high, low, close, volume 
                FROM market_data 
                WHERE symbol = %s AND timeframe = %s AND enhanced = true
                ORDER BY time DESC 
                LIMIT %s
            """
            
            cursor = connection.cursor()
            cursor.execute(query, (symbol, timeframe, needed_points))
            historical_data = cursor.fetchall()
            cursor.close()
            connection.close()
            
            if not historical_data:
                logger.warning(f"‚ö†Ô∏è Aucune donn√©e historique trouv√©e pour {symbol} {timeframe}")
                return current_prices, current_highs, current_lows, current_volumes
            
            # Inverser l'ordre (plus ancien en premier) et extraire les donn√©es
            historical_data.reverse()
            historical_highs = [float(row[0]) for row in historical_data]
            historical_lows = [float(row[1]) for row in historical_data]
            historical_prices = [float(row[2]) for row in historical_data]
            historical_volumes = [float(row[3]) for row in historical_data]
            
            # Combiner donn√©es historiques + buffers actuels
            extended_highs = historical_highs + current_highs
            extended_lows = historical_lows + current_lows
            extended_prices = historical_prices + current_prices
            extended_volumes = historical_volumes + current_volumes
            
            # üîß VALIDATION FINALE: V√©rifier l'alignement post-extension
            extended_lengths = [len(extended_prices), len(extended_highs), len(extended_lows), len(extended_volumes)]
            if not all(length == extended_lengths[0] for length in extended_lengths):
                logger.error(f"‚ùå D√âSALIGNEMENT CRITIQUE apr√®s extension {symbol} {timeframe}: P:{len(extended_prices)} H:{len(extended_highs)} L:{len(extended_lows)} V:{len(extended_volumes)}")
                
                # Force l'alignement final
                min_extended_len = min(extended_lengths)
                extended_prices = extended_prices[-min_extended_len:]
                extended_highs = extended_highs[-min_extended_len:]
                extended_lows = extended_lows[-min_extended_len:]
                extended_volumes = extended_volumes[-min_extended_len:]
                
                logger.warning(f"üîß CORRECTION APPLIQU√âE: Tous les buffers align√©s √† {min_extended_len} √©l√©ments")
            
            logger.debug(f"‚úÖ Buffers √©tendus {symbol} {timeframe}: {len(historical_data)} historiques + {len(current_prices)} actuels = {len(extended_prices)} total")
            
            return extended_prices, extended_highs, extended_lows, extended_volumes
            
        except Exception as e:
            logger.error(f"‚ùå Erreur extension buffers {symbol} {timeframe}: {e}")
            # En cas d'erreur, retourner les buffers actuels
            return current_prices, current_highs, current_lows, current_volumes

    async def _save_buffers_to_redis(self):
        """Sauvegarde p√©riodique des buffers WebSocket vers Redis"""
        if not self.redis_client:
            return
        
        try:
            saved_count = 0
            ttl = 24 * 3600  # 24h TTL pour buffers WebSocket
            
            for symbol in self.symbols:
                for timeframe in self.timeframes:
                    # Sauvegarder les price_buffers
                    if symbol in self.price_buffers and timeframe in self.price_buffers[symbol]:
                        if self.price_buffers[symbol][timeframe]:
                            key = f"ws_buffers:prices:{symbol}:{timeframe}"
                            buffer_data = {
                                'prices': self.price_buffers[symbol][timeframe][-100:],  # Garder 100 derniers
                                'timestamp': time.time()
                            }
                            self.redis_client.set(key, json.dumps(buffer_data), expiration=ttl)
                            saved_count += 1
                    
                    # Sauvegarder incremental_cache
                    if symbol in self.incremental_cache and timeframe in self.incremental_cache[symbol]:
                        if self.incremental_cache[symbol][timeframe]:
                            key = f"ws_buffers:incremental:{symbol}:{timeframe}"
                            cache_data = {
                                'cache': self.incremental_cache[symbol][timeframe],
                                'timestamp': time.time()
                            }
                            self.redis_client.set(key, json.dumps(cache_data), expiration=ttl)
                            saved_count += 1
            
            if saved_count > 0:
                logger.debug(f"üíæ {saved_count} buffers WebSocket sauvegard√©s vers Redis")
                
        except Exception as e:
            logger.warning(f"Erreur sauvegarde buffers: {e}")
    
    async def _restore_buffers_from_redis(self):
        """Restaure les buffers WebSocket depuis Redis au d√©marrage"""
        if not self.redis_client:
            return 0
        
        try:
            restored_count = 0
            
            for symbol in self.symbols:
                for timeframe in self.timeframes:
                    # Restaurer price_buffers
                    try:
                        key = f"ws_buffers:prices:{symbol}:{timeframe}"
                        buffer_data = self.redis_client.get(key)
                        if buffer_data:
                            # RedisClientPool fait d√©j√† le parsing JSON automatiquement
                            if isinstance(buffer_data, str):
                                buffer_data = json.loads(buffer_data)
                            self.price_buffers[symbol][timeframe] = buffer_data.get('prices', [])
                            restored_count += 1
                            logger.debug(f"üîÑ Prices buffer restaur√©: {symbol} {timeframe} ({len(self.price_buffers[symbol][timeframe])} prix)")
                    except Exception as e:
                        logger.warning(f"Erreur restoration prices {symbol} {timeframe}: {e}")
                    
                    # Restaurer incremental_cache
                    try:
                        key = f"ws_buffers:incremental:{symbol}:{timeframe}"
                        cache_data = self.redis_client.get(key)
                        if cache_data:
                            # RedisClientPool fait d√©j√† le parsing JSON automatiquement
                            if isinstance(cache_data, str):
                                cache_data = json.loads(cache_data)
                            self.incremental_cache[symbol][timeframe].update(cache_data.get('cache', {}))
                            restored_count += 1
                            logger.debug(f"üîÑ Cache incr√©mental restaur√©: {symbol} {timeframe}")
                    except Exception as e:
                        logger.warning(f"Erreur restoration cache {symbol} {timeframe}: {e}")
            
            if restored_count > 0:
                logger.info(f"üîÑ {restored_count} buffers WebSocket restaur√©s depuis Redis")
            
            return restored_count
            
        except Exception as e:
            logger.error(f"Erreur restoration buffers: {e}")
            return 0
    
    async def _periodic_buffer_save(self):
        """T√¢che p√©riodique de sauvegarde des buffers ET diagnostic de sant√©"""
        while self.running:
            try:
                current_time = time.time()
                
                # Sauvegarde p√©riodique
                if current_time - self.last_buffer_save_time > self.buffer_save_interval:
                    await self._save_buffers_to_redis()
                    self.last_buffer_save_time = current_time
                
                # üîß NOUVEAU: Diagnostic de sant√© des buffers toutes les 2 minutes
                if hasattr(self, 'last_health_check'):
                    time_since_check = current_time - self.last_health_check
                else:
                    time_since_check = float('inf')
                    self.last_health_check = current_time
                
                if time_since_check > 120:  # 2 minutes
                    self._diagnose_buffer_health()
                    self.last_health_check = current_time
                
                await asyncio.sleep(60)  # V√©rifier toutes les minutes
                
            except Exception as e:
                logger.warning(f"Erreur sauvegarde p√©riodique: {e}")
                await asyncio.sleep(60)

    async def _ensure_sufficient_buffer_data(self):
        """
        S'assure que les buffers WebSocket ont suffisamment de donn√©es (‚â•20 points)
        pour calculer imm√©diatement tous les indicateurs techniques.
        """
        try:
            import aiohttp
            
            total_fetched = 0
            
            for symbol in self.symbols:
                for timeframe in self.timeframes:
                    current_buffer_size = len(self.price_buffers[symbol][timeframe])
                    
                    # V√©rifier TOUS les buffers pour ce symbole/timeframe  
                    highs_size = len(self.high_buffers[symbol][timeframe])
                    lows_size = len(self.low_buffers[symbol][timeframe])
                    volumes_size = len(self.volume_buffers[symbol][timeframe])
                    min_buffer_size = min(current_buffer_size, highs_size, lows_size, volumes_size)
                    
                    if min_buffer_size < 20:
                        # Fetch recent klines from Binance API to fill buffers
                        needed_points = 25 - min_buffer_size  # Fetch a few extra
                        
                        try:
                            url = "https://api.binance.com/api/v3/klines"
                            params = {
                                'symbol': symbol,
                                'interval': timeframe,
                                'limit': needed_points
                            }
                            
                            async with aiohttp.ClientSession() as session:
                                async with session.get(url, params=params, timeout=10) as response:
                                    if response.status == 200:
                                        klines = await response.json()
                                        
                                        # Add to buffers (but don't exceed buffer limit)
                                        for kline in klines:
                                            if len(self.price_buffers[symbol][timeframe]) >= 100:
                                                break  # Respect buffer limit
                                                
                                            self.price_buffers[symbol][timeframe].append(float(kline[4]))  # close
                                            self.high_buffers[symbol][timeframe].append(float(kline[2]))   # high
                                            self.low_buffers[symbol][timeframe].append(float(kline[3]))    # low
                                            self.volume_buffers[symbol][timeframe].append(float(kline[5])) # volume
                                            total_fetched += 1
                                        
                                        logger.debug(f"üìä Pr√©-rempli {symbol} {timeframe}: +{len(klines)} points ‚Üí buffers=[P:{len(self.price_buffers[symbol][timeframe])},H:{len(self.high_buffers[symbol][timeframe])},L:{len(self.low_buffers[symbol][timeframe])},V:{len(self.volume_buffers[symbol][timeframe])}]")
                                    
                                    # Respect rate limits
                                    await asyncio.sleep(0.1)
                                    
                        except Exception as e:
                            logger.warning(f"Erreur pr√©-remplissage {symbol} {timeframe}: {e}")
            
            if total_fetched > 0:
                logger.info(f"üöÄ Buffers WebSocket pr√©-remplis: {total_fetched} points ajout√©s pour calcul imm√©diat des indicateurs")
            else:
                logger.info("üíæ Buffers WebSocket d√©j√† suffisamment remplis")
                
        except Exception as e:
            logger.error(f"‚ùå Erreur lors du pr√©-remplissage des buffers: {e}")

    def _diagnose_buffer_health(self):
        """
        üîß NOUVEAU: Diagnostic complet de la sant√© des buffers WebSocket.
        D√©tecte et corrige automatiquement les d√©salignements.
        """
        total_issues = 0
        total_corrections = 0
        
        try:
            logger.debug("üîç Diagnostic de sant√© des buffers WebSocket...")
            
            for symbol in self.symbols:
                for timeframe in self.timeframes:
                    # V√©rifier les longueurs
                    price_len = len(self.price_buffers.get(symbol, {}).get(timeframe, []))
                    high_len = len(self.high_buffers.get(symbol, {}).get(timeframe, []))
                    low_len = len(self.low_buffers.get(symbol, {}).get(timeframe, []))
                    volume_len = len(self.volume_buffers.get(symbol, {}).get(timeframe, []))
                    
                    lengths = [price_len, high_len, low_len, volume_len]
                    
                    # D√©tecter les probl√®mes
                    if not all(length == lengths[0] for length in lengths):
                        total_issues += 1
                        min_len = min(lengths)
                        max_len = max(lengths)
                        data_loss = max_len - min_len
                        
                        logger.warning(f"üîß SANT√â: D√©salignement {symbol} {timeframe} - "
                                     f"Longueurs: P:{price_len} H:{high_len} L:{low_len} V:{volume_len} "
                                     f"(perte: {data_loss} points)")
                        
                        # Correction automatique
                        if min_len > 0:
                            self.price_buffers[symbol][timeframe] = self.price_buffers[symbol][timeframe][-min_len:]
                            self.high_buffers[symbol][timeframe] = self.high_buffers[symbol][timeframe][-min_len:]
                            self.low_buffers[symbol][timeframe] = self.low_buffers[symbol][timeframe][-min_len:]
                            self.volume_buffers[symbol][timeframe] = self.volume_buffers[symbol][timeframe][-min_len:]
                            total_corrections += 1
                            logger.info(f"‚úÖ SANT√â: {symbol} {timeframe} corrig√© ‚Üí {min_len} √©l√©ments align√©s")
                        else:
                            # R√©initialiser les buffers vides
                            self.price_buffers[symbol][timeframe] = []
                            self.high_buffers[symbol][timeframe] = []
                            self.low_buffers[symbol][timeframe] = []
                            self.volume_buffers[symbol][timeframe] = []
                            logger.warning(f"üîÑ SANT√â: {symbol} {timeframe} r√©initialis√© (buffers vides)")
                    
                    # V√©rifier la taille maximum
                    elif lengths[0] > 250:  # Plus que la limite normale + marge
                        logger.warning(f"‚ö†Ô∏è SANT√â: {symbol} {timeframe} buffer trop grand ({lengths[0]} √©l√©ments)")
                        # Tronquer √† 200 √©l√©ments
                        self.price_buffers[symbol][timeframe] = self.price_buffers[symbol][timeframe][-200:]
                        self.high_buffers[symbol][timeframe] = self.high_buffers[symbol][timeframe][-200:]
                        self.low_buffers[symbol][timeframe] = self.low_buffers[symbol][timeframe][-200:]
                        self.volume_buffers[symbol][timeframe] = self.volume_buffers[symbol][timeframe][-200:]
                        total_corrections += 1
                        logger.info(f"‚úÖ SANT√â: {symbol} {timeframe} tronqu√© √† 200 √©l√©ments")
            
            # Rapport de sant√©
            if total_issues > 0:
                logger.warning(f"üîß SANT√â: {total_issues} probl√®mes d√©tect√©s, {total_corrections} corrections appliqu√©es")
            else:
                logger.debug("‚úÖ SANT√â: Tous les buffers sont en bonne sant√©")
                
        except Exception as e:
            logger.error(f"‚ùå Erreur diagnostic sant√© buffers: {e}")

# Fonction principale pour ex√©cuter le WebSocket de mani√®re asynchrone
async def run_binance_websocket():
    """
    Fonction principale pour ex√©cuter le WebSocket Binance.
    """
    # Cr√©er le client Kafka
    kafka_client = KafkaClient()
    
    # Cr√©er et d√©marrer le WebSocket
    ws_client = BinanceWebSocket(kafka_client=kafka_client)
    
    try:
        logger.info("üöÄ D√©marrage du WebSocket Binance...")
        await ws_client.start()
    except KeyboardInterrupt:
        logger.info("Interruption clavier d√©tect√©e")
    except Exception as e:
        logger.error(f"Erreur dans la boucle principale: {str(e)}")
    finally:
        logger.info("Arr√™t du WebSocket Binance...")
        await ws_client.stop()

# Point d'entr√©e pour ex√©cution directe
if __name__ == "__main__":
    try:
        asyncio.run(run_binance_websocket())
    except KeyboardInterrupt:
        logger.info("Programme interrompu par l'utilisateur")