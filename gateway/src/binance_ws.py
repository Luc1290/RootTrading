"""
Module de connexion WebSocket √† Binance.
Re√ßoit les donn√©es de march√© en temps r√©el et les transmet au Kafka producer.
"""
import json
import logging
import time
from typing import Dict, Any, Callable, List, Optional, Tuple
import asyncio
import websockets
from websockets.exceptions import ConnectionClosed, InvalidStatusCode

# Importer les clients partag√©s
import sys
import os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "../../")))

from shared.src.config import BINANCE_API_KEY, BINANCE_SECRET_KEY, SYMBOLS, INTERVAL, KAFKA_TOPIC_MARKET_DATA
from shared.src.kafka_client import KafkaClient
from gateway.src.kafka_producer import get_producer   # NEW


# Configuration du logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger("binance_ws")

class BinanceWebSocket:
    """
    Gestionnaire de connexion WebSocket √† Binance pour les donn√©es de march√©.
    R√©cup√®re les donn√©es en temps r√©el via WebSocket et les transmet via Kafka.
    """
    
    def __init__(self, symbols: List[str] = None, interval: str = INTERVAL, kafka_client: KafkaClient = None):
        """
        Initialise la connexion WebSocket Binance.
        
        Args:
            symbols: Liste des symboles √† surveiller (ex: ['BTCUSDC', 'ETHUSDC'])
            interval: Intervalle des chandeliers (ex: '1m', '5m', '1h')
            kafka_client: Client Kafka pour la publication des donn√©es
        """
        self.symbols = symbols or SYMBOLS
        self.interval = interval
        self.kafka_client = kafka_client or get_producer()   # utilise le producteur singleton
        self.ws = None
        self.running = False
        self.reconnect_delay = 1  # Secondes, pour backoff exponentiel
        self.last_message_time = 0
        self.heartbeat_interval = 60  # Secondes
        
        # URL de l'API WebSocket Binance
        self.base_url = "wss://stream.binance.com:9443/ws"
        
        # Multi-timeframes et donn√©es enrichies pour trading pr√©cis
        self.timeframes = ['1m', '5m', '15m', '1h', '4h']  # Timeframes multiples
        self.stream_paths = []
        
        for symbol in self.symbols:
            symbol_lower = symbol.lower()
            # Klines multi-timeframes
            for tf in self.timeframes:
                self.stream_paths.append(f"{symbol_lower}@kline_{tf}")
            # Ticker 24h pour contexte
            self.stream_paths.append(f"{symbol_lower}@ticker")
            # Orderbook pour spread
            self.stream_paths.append(f"{symbol_lower}@depth5")
            
        # Cache pour donn√©es enrichies
        self.ticker_cache = {}
        self.orderbook_cache = {}
        
        # ULTRA-AVANC√â : Buffers pour calculs techniques en temps r√©el
        self.price_buffers = {}
        self.volume_buffers = {}
        self.high_buffers = {}
        self.low_buffers = {}
        self.rsi_buffers = {}
        self.macd_buffers = {}
        
        # üöÄ NOUVEAU : Cache pour indicateurs incr√©mentaux (√©vite dents de scie)
        self.incremental_cache = {}
        
        # Initialiser les buffers pour chaque symbole/timeframe
        for symbol in self.symbols:
            self.price_buffers[symbol] = {}
            self.volume_buffers[symbol] = {}
            self.high_buffers[symbol] = {}
            self.low_buffers[symbol] = {}
            self.rsi_buffers[symbol] = {}
            self.macd_buffers[symbol] = {}
            
            # üöÄ NOUVEAU : Initialiser cache incr√©mental pour EMA/MACD lisses
            self.incremental_cache[symbol] = {}
            
            for tf in self.timeframes:
                self.price_buffers[symbol][tf] = []
                self.volume_buffers[symbol][tf] = []
                self.high_buffers[symbol][tf] = []
                self.low_buffers[symbol][tf] = []
                self.rsi_buffers[symbol][tf] = []
                self.macd_buffers[symbol][tf] = {'ema12': None, 'ema26': None, 'signal9': None}
                
                # Cache pour chaque timeframe
                self.incremental_cache[symbol][tf] = {}
                
        # Configuration pour sauvegarde p√©riodique des buffers
        self.redis_client = None
        self._init_redis_for_buffers()
        self.buffer_save_interval = 300  # 5 minutes
        self.last_buffer_save_time = 0
        
        logger.info(f"üî• Gateway ULTRA-AVANC√â : {len(self.stream_paths)} streams + indicateurs temps r√©el")
    
    async def _connect(self) -> None:
        """
        √âtablit la connexion WebSocket avec Binance et souscrit aux streams.
        """
        uri = self.base_url
        
        try:
            logger.info(f"Connexion √† Binance WebSocket: {uri}")
            self.ws = await websockets.connect(uri)
            
            # S'abonner aux streams
            subscribe_msg = {
                "method": "SUBSCRIBE",
                "params": self.stream_paths,
                "id": int(time.time() * 1000)
            }
            
            await self.ws.send(json.dumps(subscribe_msg))
            logger.info(f"Abonnement envoy√© pour: {', '.join(self.stream_paths)}")
            
            # Attendre la confirmation d'abonnement
            response = await self.ws.recv()
            response_data = json.loads(response)
            
            if 'result' in response_data and response_data['result'] is None:
                logger.info("‚úÖ Abonnement aux streams Binance confirm√©")
            else:
                logger.warning(f"R√©ponse d'abonnement inattendue: {response_data}")
            
            # R√©initialiser le d√©lai de reconnexion
            self.reconnect_delay = 1
            
        except (ConnectionClosed, InvalidStatusCode) as e:
            logger.error(f"‚ùå Erreur lors de la connexion WebSocket: {str(e)}")
            await self._handle_reconnect()
        except Exception as e:
            logger.error(f"‚ùå Erreur inattendue lors de la connexion: {str(e)}")
            await self._handle_reconnect()
    
    async def _handle_reconnect(self) -> None:
        """
        G√®re la reconnexion en cas de perte de connexion.
        Utilise un backoff exponentiel pour √©viter de surcharger le serveur.
        """
        # V√©rifier si le service est encore en cours d'ex√©cution
        if not self.running:
            return
    
        # Limiter le nombre de tentatives
        max_retries = 20
        retry_count = 0
    
        while self.running and retry_count < max_retries:
            logger.warning(f"Tentative de reconnexion {retry_count+1}/{max_retries} dans {self.reconnect_delay} secondes...")
            await asyncio.sleep(self.reconnect_delay)
        
            # Backoff exponentiel avec plafond plus √©lev√©
            self.reconnect_delay = min(300, self.reconnect_delay * 2)  # 5 minutes max
        
            try:
                # Fermer proprement la connexion existante si elle existe
                if self.ws:
                    try:
                        await self.ws.close()
                    except Exception as e:
                        logger.warning(f"Erreur lors de la fermeture de la connexion existante: {str(e)}")
                
                # √âtablir une nouvelle connexion
                await self._connect()
                return  # Reconnexion r√©ussie
            except Exception as e:
                retry_count += 1
                logger.error(f"‚ùå √âchec de reconnexion: {str(e)}")
    
        logger.critical("Impossible de se reconnecter apr√®s plusieurs tentatives. Arr√™t du service.")
        # Signaler l'arr√™t au service principal
        self.running = False
    
    async def _heartbeat_check(self) -> None:
        """
        V√©rifie r√©guli√®rement si nous recevons toujours des messages.
        Reconnecte si aucun message n'a √©t√© re√ßu depuis trop longtemps.
        """
        while self.running:
            await asyncio.sleep(self.heartbeat_interval)
            
            # Si aucun message re√ßu depuis 2x l'intervalle de heartbeat
            if time.time() - self.last_message_time > self.heartbeat_interval * 2:
                logger.warning(f"‚ùó Aucun message re√ßu depuis {self.heartbeat_interval * 2} secondes. Reconnexion...")
                
                # Fermer la connexion existante
                if self.ws:
                    try:
                        await self.ws.close()
                    except Exception as e:
                        logger.warning(f"Erreur lors de la fermeture de la connexion: {str(e)}")
                
                # Reconnecter
                await self._connect()
    
    def _process_kline_message(self, message: Dict[str, Any]) -> Dict[str, Any]:
        """
        Traite un message de chandelier Binance avec donn√©es enrichies.
        
        Args:
            message: Message brut de Binance
            
        Returns:
            Message format√© enrichi pour le syst√®me RootTrading
        """
        # Extraire les donn√©es pertinentes
        kline = message['k']
        symbol = message['s']
        
        # Cr√©er un message enrichi
        processed_data = {
            # Donn√©es de base
            "symbol": symbol,
            "timeframe": kline['i'],
            "start_time": kline['t'],
            "close_time": kline['T'],
            "open": float(kline['o']),
            "high": float(kline['h']),
            "low": float(kline['l']),
            "close": float(kline['c']),
            "volume": float(kline['v']),
            "quote_volume": float(kline['q']),
            "trade_count": int(kline['n']),
            "taker_buy_volume": float(kline['V']),
            "taker_buy_quote_volume": float(kline['Q']),
            "is_closed": kline['x'],
            
            # Donn√©es enrichies si disponibles
            "enhanced": True
        }
        
        # Ajouter les donn√©es de ticker si disponibles
        if symbol in self.ticker_cache:
            ticker = self.ticker_cache[symbol]
            processed_data.update({
                "price_change_24h": ticker.get('priceChange', 0),
                "price_change_pct_24h": ticker.get('priceChangePercent', 0),
                "volume_24h": ticker.get('volume', 0),
                "high_24h": ticker.get('highPrice', 0),
                "low_24h": ticker.get('lowPrice', 0)
            })
            
        # Ajouter les donn√©es d'orderbook enrichies si disponibles
        if symbol in self.orderbook_cache:
            book = self.orderbook_cache[symbol]
            bids = book.get('bids', [])
            asks = book.get('asks', [])
            sentiment_analysis = book.get('sentiment_analysis', {})
            
            if bids and asks:
                bid_price = float(bids[0][0])
                ask_price = float(asks[0][0])
                mid_price = (bid_price + ask_price) / 2
                spread_pct = ((ask_price - bid_price) / mid_price) * 100
                
                processed_data.update({
                    "bid_price": bid_price,
                    "ask_price": ask_price,
                    "bid_qty": float(bids[0][1]),
                    "ask_qty": float(asks[0][1]),
                    "spread_pct": spread_pct,
                    
                    # ULTRA-AVANC√â : Sentiment analysis complete
                    "orderbook_sentiment": sentiment_analysis
                })
        
        # ULTRA-AVANC√â : Calculer tous les indicateurs techniques en temps r√©el
        if processed_data['is_closed']:
            self._update_technical_indicators(symbol, processed_data)
        
        return processed_data
        
    def _update_technical_indicators(self, symbol: str, candle_data: Dict) -> None:
        """ULTRA-AVANC√â : Met √† jour tous les indicateurs techniques en temps r√©el"""
        timeframe = candle_data['timeframe']
        close_price = candle_data['close']
        high_price = candle_data['high']
        low_price = candle_data['low']
        volume = candle_data['volume']
        
        # Ajouter aux buffers (garder 200 derni√®res valeurs max)
        if len(self.price_buffers[symbol][timeframe]) >= 200:
            self.price_buffers[symbol][timeframe].pop(0)
        self.price_buffers[symbol][timeframe].append(close_price)
        
        if len(self.high_buffers[symbol][timeframe]) >= 200:
            self.high_buffers[symbol][timeframe].pop(0)
        self.high_buffers[symbol][timeframe].append(high_price)
        
        if len(self.low_buffers[symbol][timeframe]) >= 200:
            self.low_buffers[symbol][timeframe].pop(0)
        self.low_buffers[symbol][timeframe].append(low_price)
        
        if len(self.volume_buffers[symbol][timeframe]) >= 200:
            self.volume_buffers[symbol][timeframe].pop(0)
        self.volume_buffers[symbol][timeframe].append(volume)
        
        prices = self.price_buffers[symbol][timeframe]
        highs = self.high_buffers[symbol][timeframe]
        lows = self.low_buffers[symbol][timeframe]
        volumes = self.volume_buffers[symbol][timeframe]
        
        # üöÄ HYBRIDE : Calcul incr√©mental pour EMA/MACD + traditionnel pour le reste
        if len(prices) >= 20 and len(highs) >= 20 and len(lows) >= 20 and len(volumes) >= 20:
            logger.info(f"üìä HYBRIDE WebSocket {symbol} {timeframe}: buffers=[P:{len(prices)},H:{len(highs)},L:{len(lows)},V:{len(volumes)}] ‚Üí calcul des 33 indicateurs")
            
            # üìä TRADITIONNEL : Calculer TOUS les indicateurs d'abord
            from shared.src.technical_indicators import indicators
            all_indicators = indicators.calculate_all_indicators(highs, lows, prices, volumes)
            
            # Debug: v√©rifier si ADX est calcul√©
            if 'adx_14' in all_indicators and all_indicators['adx_14'] is not None:
                logger.info(f"‚úÖ ADX calcul√© dans WebSocket {symbol} {timeframe}: {all_indicators['adx_14']}")
            else:
                logger.warning(f"‚ùå ADX manquant dans WebSocket {symbol} {timeframe}, buffer sizes: H={len(highs)}, L={len(lows)}, C={len(prices)}")
                if 'adx_14' in all_indicators:
                    logger.warning(f"ADX value is None: {all_indicators['adx_14']}")
            
            # üöÄ NOUVEAU : Calcul incr√©mental pour EMA/MACD (√©vite dents de scie)
            incremental_indicators = self._calculate_smooth_indicators(symbol, timeframe, candle_data, all_indicators)
            
            # FUSION INTELLIGENTE : Garder tous les traditionnels + override seulement EMA/MACD avec versions lisses
            final_indicators = all_indicators.copy()
            # Override seulement les indicateurs EMA/MACD avec les versions incr√©mentales lisses
            for indicator_name, value in incremental_indicators.items():
                if value is not None and indicator_name in ['ema_12', 'ema_26', 'ema_50', 'macd_line', 'macd_signal', 'macd_histogram']:
                    final_indicators[indicator_name] = value
            
            # Ajouter tous les indicateurs calcul√©s
            for indicator_name, value in final_indicators.items():
                if value is not None:
                    candle_data[indicator_name] = value
            
            logger.debug(f"‚úÖ {len(final_indicators)} indicateurs calcul√©s pour {symbol} (üöÄ {len(incremental_indicators)} EMA/MACD incr√©mentaux, {len(all_indicators)} traditionnels)")
            
            # Calculer aussi les indicateurs custom non inclus dans calculate_all_indicators
            # Stochastic RSI (pas dans calculate_all_indicators)
            stoch_rsi = self._calculate_stoch_rsi(prices, 14)
            if stoch_rsi:
                candle_data['stoch_rsi'] = stoch_rsi
                
            # VWAP 10 (custom, pas dans calculate_all_indicators)
            if len(volumes) >= 10:
                vwap = self._calculate_vwap(prices[-10:], volumes[-10:])
                if vwap:
                    candle_data['vwap_10'] = vwap
                    
            # Williams %R (custom implementation)
            williams_r = self._calculate_williams_r(symbol, timeframe, 14)
            if williams_r:
                candle_data['williams_r'] = williams_r
                
            # CCI 20 (custom implementation)
            cci = self._calculate_cci(symbol, timeframe, 20)
            if cci:
                candle_data['cci_20'] = cci
                
            # Marquer les donn√©es comme enrichies
            candle_data['enhanced'] = True
            candle_data['ultra_enriched'] = True
            
        elif len(prices) >= 14:
            # Log pour debug: pourquoi on n'atteint pas les 20 points
            logger.info(f"üîç WebSocket {symbol} {timeframe}: buffers=[P:{len(prices)},H:{len(highs)},L:{len(lows)},V:{len(volumes)}] ‚Üí calcul PARTIEL seulement")
            # Calcul partiel pour avoir au moins RSI et quelques indicateurs de base
            logger.debug(f"üìä Calcul partiel des indicateurs pour {symbol} {timeframe}")
            
            # üöÄ NOUVEAU : M√™me pour calcul partiel, utiliser les EMA incr√©mentales si possible
            incremental_indicators = self._calculate_smooth_indicators(symbol, timeframe, candle_data)
            for indicator_name, value in incremental_indicators.items():
                if value is not None:
                    candle_data[indicator_name] = value
            
            # RSI seul si pas assez de donn√©es pour tous les indicateurs
            rsi = self._calculate_rsi(prices, 14)
            if rsi:
                candle_data['rsi_14'] = rsi
                
            # Marquer comme partiellement enrichi
            candle_data['enhanced'] = True
            candle_data['ultra_enriched'] = False
                
    def _calculate_rsi(self, prices: List[float], period: int = 14) -> Optional[float]:
        """Calcule le RSI via le module centralis√©"""
        from shared.src.technical_indicators import calculate_rsi
        return calculate_rsi(prices, period)
        
    def _calculate_stoch_rsi(self, prices: List[float], period: int = 14) -> Optional[float]:
        """Calcule le Stochastic RSI"""
        if len(prices) < period * 2:
            return None
            
        # Calculer RSI sur les derni√®res p√©riodes
        rsi_values = []
        for i in range(period, len(prices) + 1):
            rsi = self._calculate_rsi(prices[:i], period)
            if rsi:
                rsi_values.append(rsi)
                
        if len(rsi_values) < period:
            return None
            
        recent_rsi = rsi_values[-period:]
        min_rsi = min(recent_rsi)
        max_rsi = max(recent_rsi)
        current_rsi = rsi_values[-1]
        
        if max_rsi == min_rsi:
            return 50
            
        return ((current_rsi - min_rsi) / (max_rsi - min_rsi)) * 100
        
    def _calculate_ema(self, prices: List[float], period: int) -> float:
        """Calcule l'EMA via le module centralis√©"""
        from shared.src.technical_indicators import calculate_ema
        result = calculate_ema(prices, period)
        return result if result is not None else (prices[-1] if prices else 0)
        
    def _calculate_macd(self, prices: List[float]) -> Optional[Dict]:
        """Calcule MACD complet via le module centralis√©"""
        from shared.src.technical_indicators import calculate_macd
        
        result = calculate_macd(prices)
        if result is None or any(v is None for v in result.values()):
            return None
            
        return {
            'macd_line': result['macd_line'],
            'macd_signal': result['macd_signal'],
            'macd_histogram': result['macd_histogram']
        }
        
    def _calculate_bollinger_bands(self, prices: List[float], period: int, std_dev: float) -> Optional[Dict]:
        """Calcule les Bollinger Bands via le module centralis√©"""
        from shared.src.technical_indicators import calculate_bollinger_bands
        
        result = calculate_bollinger_bands(prices, period, std_dev)
        if result is None or any(v is None for v in result.values()):
            return None
            
        return {
            'bb_upper': result['bb_upper'],
            'bb_middle': result['bb_middle'], 
            'bb_lower': result['bb_lower'],
            'bb_position': result['bb_position'],
            'bb_width': result['bb_width']
        }
        
    def _calculate_vwap(self, prices: List[float], volumes: List[float]) -> Optional[float]:
        """Calcule le VWAP (Volume Weighted Average Price)"""
        if len(prices) != len(volumes) or not prices:
            return None
            
        total_pv = sum(p * v for p, v in zip(prices, volumes))
        total_volume = sum(volumes)
        
        return total_pv / total_volume if total_volume > 0 else None
        
    def _analyze_volume_profile(self, volumes: List[float]) -> Dict:
        """Analyse avanc√©e du profil de volume"""
        if len(volumes) < 5:
            return {}
            
        recent_vol = volumes[-1]
        avg_vol = sum(volumes) / len(volumes)
        
        vol_ratio = recent_vol / avg_vol if avg_vol > 0 else 1
        
        # D√©tection de pic de volume
        vol_spike = vol_ratio > 2.0
        
        # Tendance du volume
        if len(volumes) >= 3:
            vol_trend = "increasing" if volumes[-1] > volumes[-2] > volumes[-3] else "decreasing"
        else:
            vol_trend = "stable"
            
        return {
            'volume_ratio': vol_ratio,
            'volume_spike': vol_spike,
            'volume_trend': vol_trend,
            'avg_volume': avg_vol
        }
        
    def _calculate_momentum(self, prices: List[float], period: int) -> Optional[float]:
        """Calcule le momentum"""
        if len(prices) < period + 1:
            return None
            
        return ((prices[-1] - prices[-period-1]) / prices[-period-1]) * 100
        
    def _calculate_atr(self, symbol: str, timeframe: str, period: int) -> Optional[float]:
        """Calcule l'ATR (Average True Range)"""
        # Pour l'ATR, on a besoin des high/low pr√©c√©dents, simplifi√© ici
        prices = self.price_buffers[symbol][timeframe]
        if len(prices) < period + 1:
            return None
            
        # Approximation : utiliser les variations de prix comme proxy
        ranges = [abs(prices[i] - prices[i-1]) for i in range(1, len(prices))]
        if len(ranges) < period:
            return None
            
        return sum(ranges[-period:]) / period
        
    def _calculate_adx(self, symbol: str, timeframe: str, period: int) -> Optional[float]:
        """Calcule l'ADX en utilisant le module partag√©"""
        if symbol not in self.high_buffers or symbol not in self.low_buffers:
            return None
            
        if timeframe not in self.high_buffers[symbol] or timeframe not in self.low_buffers[symbol]:
            return None
            
        highs = self.high_buffers[symbol][timeframe]
        lows = self.low_buffers[symbol][timeframe]
        closes = self.price_buffers[symbol][timeframe]
        
        if len(highs) < period + 1 or len(lows) < period + 1 or len(closes) < period + 1:
            return None
        
        try:
            # Utiliser le module partag√© pour le calcul ADX
            from shared.src.technical_indicators import TechnicalIndicators
            indicators = TechnicalIndicators()
            
            adx, plus_di, minus_di = indicators.calculate_adx(highs, lows, closes, period)
            return adx  # Retourner seulement la valeur ADX
            
        except Exception as e:
            logger.debug(f"Erreur calcul ADX pour {symbol}: {e}")
            return None
        
    def _calculate_williams_r(self, symbol: str, timeframe: str, period: int) -> Optional[float]:
        """Calcule Williams %R"""
        prices = self.price_buffers[symbol][timeframe]
        if len(prices) < period:
            return None
            
        recent_prices = prices[-period:]
        highest = max(recent_prices)
        lowest = min(recent_prices)
        current = prices[-1]
        
        if highest == lowest:
            return -50
            
        return ((highest - current) / (highest - lowest)) * -100
        
    def _calculate_cci(self, symbol: str, timeframe: str, period: int) -> Optional[float]:
        """Calcule CCI (Commodity Channel Index)"""
        prices = self.price_buffers[symbol][timeframe]
        if len(prices) < period:
            return None
            
        recent_prices = prices[-period:]
        typical_price = sum(recent_prices) / len(recent_prices)
        sma = typical_price
        
        # Approximation de l'√©cart moyen
        mean_deviation = sum(abs(p - sma) for p in recent_prices) / len(recent_prices)
        
        if mean_deviation == 0:
            return 0
            
        return (prices[-1] - sma) / (0.015 * mean_deviation)
        
    def _process_ticker_message(self, data: Dict[str, Any]) -> None:
        """Traite les donn√©es ticker 24h"""
        symbol = data['s']
        self.ticker_cache[symbol] = {
            'priceChange': float(data.get('P', 0)),
            'priceChangePercent': float(data.get('p', 0)),
            'volume': float(data.get('v', 0)),
            'highPrice': float(data.get('h', 0)),
            'lowPrice': float(data.get('l', 0))
        }
        
    def _process_depth_message(self, data: Dict[str, Any]) -> None:
        """Traite les donn√©es orderbook avec analyse de sentiment avanc√©e"""
        symbol = data['s']
        bids = data.get('b', [])
        asks = data.get('a', [])
        
        # Analyse avanc√©e du sentiment via orderbook
        sentiment_analysis = self._analyze_orderbook_sentiment(bids, asks)
        
        self.orderbook_cache[symbol] = {
            'bids': bids,
            'asks': asks,
            'sentiment_analysis': sentiment_analysis,
            'timestamp': time.time()
        }
        
    def _analyze_orderbook_sentiment(self, bids: List, asks: List) -> Dict:
        """ULTRA-AVANC√â : Analyse compl√®te du sentiment via l'orderbook"""
        try:
            if not bids or not asks:
                return {}
                
            # Convertir en float pour calculs
            bid_data = [(float(price), float(qty)) for price, qty in bids[:20]]  # Top 20
            ask_data = [(float(price), float(qty)) for price, qty in asks[:20]]
            
            if not bid_data or not ask_data:
                return {}
                
            # 1. Ratio bid/ask volume
            total_bid_volume = sum(qty for _, qty in bid_data)
            total_ask_volume = sum(qty for _, qty in ask_data)
            
            bid_ask_ratio = total_bid_volume / total_ask_volume if total_ask_volume > 0 else 1
            
            # 2. Spread analysis
            best_bid = bid_data[0][0]
            best_ask = ask_data[0][0]
            spread = best_ask - best_bid
            mid_price = (best_bid + best_ask) / 2
            spread_pct = (spread / mid_price) * 100
            
            # 3. Order book imbalance
            imbalance = (total_bid_volume - total_ask_volume) / (total_bid_volume + total_ask_volume)
            
            # 4. Depth analysis (support/resistance strength)
            bid_depth_strength = self._calculate_depth_strength(bid_data, best_bid, 'support')
            ask_depth_strength = self._calculate_depth_strength(ask_data, best_ask, 'resistance')
            
            # 5. Wall detection (gros ordres)
            bid_walls = self._detect_walls(bid_data, 'bid')
            ask_walls = self._detect_walls(ask_data, 'ask')
            
            # 6. Price level concentration
            bid_concentration = self._calculate_price_concentration(bid_data)
            ask_concentration = self._calculate_price_concentration(ask_data)
            
            # 7. Sentiment global
            sentiment_score = self._calculate_sentiment_score(
                bid_ask_ratio, imbalance, bid_depth_strength, ask_depth_strength,
                len(bid_walls), len(ask_walls)
            )
            
            # 8. Liquidit√© totale
            total_liquidity = total_bid_volume + total_ask_volume
            
            # 9. Ratio des 5 premiers niveaux vs le reste
            top5_bid_vol = sum(qty for _, qty in bid_data[:5])
            top5_ask_vol = sum(qty for _, qty in ask_data[:5])
            top5_ratio = (top5_bid_vol + top5_ask_vol) / total_liquidity if total_liquidity > 0 else 0
            
            return {
                'bid_ask_ratio': round(bid_ask_ratio, 3),
                'spread_pct': round(spread_pct, 4),
                'imbalance': round(imbalance, 3),
                'bid_depth_strength': round(bid_depth_strength, 3),
                'ask_depth_strength': round(ask_depth_strength, 3),
                'bid_walls': bid_walls,
                'ask_walls': ask_walls,
                'bid_concentration': round(bid_concentration, 3),
                'ask_concentration': round(ask_concentration, 3),
                'sentiment_score': round(sentiment_score, 3),
                'total_liquidity': round(total_liquidity, 2),
                'top5_concentration': round(top5_ratio, 3),
                'sentiment_signal': self._interpret_sentiment(sentiment_score, imbalance, bid_ask_ratio)
            }
            
        except Exception as e:
            logger.error(f"‚ùå Erreur analyse sentiment orderbook: {e}")
            return {}
            
    def _calculate_depth_strength(self, orders: List[Tuple], best_price: float, side: str) -> float:
        """Calcule la force de profondeur des ordres"""
        if not orders:
            return 0
            
        # Calculer la force bas√©e sur le volume et la distance du meilleur prix
        total_strength = 0
        
        for price, qty in orders:
            # Distance relative du meilleur prix
            if side == 'support':
                distance = (best_price - price) / best_price
            else:  # resistance
                distance = (price - best_price) / best_price
                
            # Force = volume / (1 + distance) - plus pr√®s = plus fort
            strength = qty / (1 + distance * 100)  # *100 pour normaliser
            total_strength += strength
            
        return total_strength
        
    def _detect_walls(self, orders: List[Tuple], side: str) -> List[Dict]:
        """D√©tecte les murs d'ordres (gros volumes)"""
        if not orders:
            return []
            
        # Calculer le volume moyen
        volumes = [qty for _, qty in orders]
        avg_volume = sum(volumes) / len(volumes)
        
        # D√©tecter les volumes > 3x la moyenne
        walls = []
        for price, qty in orders:
            if qty > avg_volume * 3:
                walls.append({
                    'price': price,
                    'volume': qty,
                    'ratio_to_avg': qty / avg_volume,
                    'side': side
                })
                
        return sorted(walls, key=lambda x: x['volume'], reverse=True)[:5]  # Top 5
        
    def _calculate_price_concentration(self, orders: List[Tuple]) -> float:
        """Calcule la concentration des ordres par niveau de prix"""
        if len(orders) < 3:
            return 0
            
        # Calculer l'√©cart-type des volumes pour mesurer la concentration
        volumes = [qty for _, qty in orders]
        avg_vol = sum(volumes) / len(volumes)
        
        variance = sum((vol - avg_vol) ** 2 for vol in volumes) / len(volumes)
        std_dev = variance ** 0.5
        
        # Normaliser : plus l'√©cart-type est √©lev√©, plus la concentration est in√©gale
        return std_dev / avg_vol if avg_vol > 0 else 0
        
    def _calculate_sentiment_score(self, bid_ask_ratio: float, imbalance: float, 
                                 bid_strength: float, ask_strength: float,
                                 bid_walls: int, ask_walls: int) -> float:
        """Calcule un score de sentiment global (-1 = tr√®s bearish, +1 = tr√®s bullish)"""
        
        # Composants du sentiment
        ratio_score = 0
        if bid_ask_ratio > 1.2:
            ratio_score = 0.3  # Plus de volume bid = bullish
        elif bid_ask_ratio < 0.8:
            ratio_score = -0.3  # Plus de volume ask = bearish
            
        # Imbalance score (directement utilisable entre -1 et 1)
        imbalance_score = imbalance * 0.4
        
        # Depth strength comparison
        if bid_strength > ask_strength * 1.2:
            depth_score = 0.2  # Support plus fort
        elif ask_strength > bid_strength * 1.2:
            depth_score = -0.2  # R√©sistance plus forte
        else:
            depth_score = 0
            
        # Wall advantage
        wall_score = 0
        if bid_walls > ask_walls:
            wall_score = 0.1  # Plus de murs bid
        elif ask_walls > bid_walls:
            wall_score = -0.1  # Plus de murs ask
            
        # Score final
        total_score = ratio_score + imbalance_score + depth_score + wall_score
        
        # Limiter entre -1 et 1
        return max(-1, min(1, total_score))
        
    def _interpret_sentiment(self, sentiment_score: float, imbalance: float, bid_ask_ratio: float) -> str:
        """Interpr√®te le score de sentiment en signal actionnable"""
        
        if sentiment_score > 0.5:
            return "VERY_BULLISH"
        elif sentiment_score > 0.2:
            return "BULLISH"
        elif sentiment_score < -0.5:
            return "VERY_BEARISH"
        elif sentiment_score < -0.2:
            return "BEARISH"
        else:
            return "NEUTRAL"
    
    async def _listen(self) -> None:
        """
        √âcoute les messages WebSocket et les traite.
        """
        while self.running:
            try:
                if not self.ws:
                    logger.warning("Connexion WebSocket perdue, reconnexion...")
                    await self._connect()
                    continue
                
                # Recevoir un message
                message = await self.ws.recv()
                self.last_message_time = time.time()
                
                # Traiter le message
                try:
                    data = json.loads(message)
                    
                    # V√©rifier le type de message
                    if 'e' in data:
                        event_type = data['e']
                        
                        # Traiter les donn√©es de chandelier
                        if event_type == 'kline':
                            processed_data = self._process_kline_message(data)

                            # üëâ on ignore les bougies encore ouvertes
                            if not processed_data['is_closed']:
                                continue

                            symbol = processed_data['symbol']
                            timeframe = processed_data['timeframe']
                            
                            # Topic sp√©cifique au timeframe
                            topic = f"market.data.{symbol.lower()}.{timeframe}"
                            key = f"{symbol}:{timeframe}:{processed_data['start_time']}"

                            # Nettoyer les donn√©es pour √©viter les erreurs de format
                            clean_data = {}
                            for k, v in processed_data.items():
                                if isinstance(v, str):
                                    # √âchapper les caract√®res de formatage probl√©matiques
                                    clean_data[k] = v.replace('{', '{{').replace('}', '}}') if '{' in v or '}' in v else v
                                else:
                                    clean_data[k] = v
                            
                            # Log des cl√©s pour d√©bogage
                            logger.debug(f"Donn√©es envoy√©es √† Kafka - Cl√©s: {list(clean_data.keys())}")

                            # Publier avec la nouvelle m√©thode
                            self.kafka_client.publish_to_topic(
                                topic=topic,
                                data=clean_data,
                                key=key
                            )
        
                            # Log enrichi
                            spread_pct = processed_data.get('spread_pct', 0)
                            spread_info = f" Spread:{spread_pct:.3f}%" if isinstance(spread_pct, (int, float)) and 'spread_pct' in processed_data else ""
                            logger.info(f"üìä {symbol} {timeframe}: {processed_data['close']} "
                                      f"[O:{processed_data['open']} H:{processed_data['high']} L:{processed_data['low']}]"
                                      f"{spread_info}")
                                      
                        # Traiter les donn√©es ticker
                        elif event_type == '24hrTicker':
                            self._process_ticker_message(data)
                            
                        # Traiter les donn√©es orderbook
                        elif event_type == 'depthUpdate':
                            self._process_depth_message(data)
                except json.JSONDecodeError:
                    logger.error(f"Message non-JSON re√ßu: {message[:100]}...")
                except Exception as e:
                    error_msg = str(e).replace('{', '{{').replace('}', '}}')
                    logger.error(f"Erreur lors du traitement du message: {error_msg}")
                
            except ConnectionClosed as e:
                logger.warning(f"Connexion WebSocket ferm√©e: {str(e)}")
                await self._handle_reconnect()
            except Exception as e:
                logger.error(f"Erreur durant l'√©coute: {str(e)}")
                await asyncio.sleep(1)  # Pause pour √©viter une boucle d'erreur infinie
    
    async def start(self) -> None:
        """
        D√©marre la connexion WebSocket et l'√©coute des messages.
        """
        if self.running:
            logger.warning("Le WebSocket est d√©j√† en cours d'ex√©cution.")
            return
            
        self.running = True
        self.last_message_time = time.time()
        
        try:
            # üîÑ NOUVEAU : Restaurer les buffers WebSocket depuis Redis
            logger.info("üîÑ Restoration des buffers WebSocket depuis Redis...")
            restored_buffers = await self._restore_buffers_from_redis()
            
            if restored_buffers > 0:
                logger.info(f"‚úÖ {restored_buffers} buffers WebSocket restaur√©s - CONTINUIT√â PR√âSERV√âE")
            else:
                logger.info("üÜï Aucun buffer restaur√© - Premier d√©marrage ou cache vide")
            
            # üöÄ NOUVEAU : S'assurer que les buffers ont assez de donn√©es pour tous les indicateurs
            await self._ensure_sufficient_buffer_data()
            
            # üöÄ Initialiser le cache incr√©mental (compl√©ter si pas restaur√©)
            logger.info("üíæ Initialisation du cache incr√©mental pour EMA/MACD lisses...")
            for symbol in self.symbols:
                for timeframe in self.timeframes:
                    self._initialize_incremental_cache_simple(symbol, timeframe)
            logger.info("‚úÖ Cache incr√©mental initialis√© pour tous les symboles/timeframes")
            
            # D√©marrer la connexion
            await self._connect()
            
            # D√©marrer les t√¢ches parall√®les
            heartbeat_task = asyncio.create_task(self._heartbeat_check())
            buffer_save_task = asyncio.create_task(self._periodic_buffer_save())
            
            logger.info("üöÄ WebSocket + Sauvegarde p√©riodique d√©marr√©s")
            
            # √âcouter les messages
            await self._listen()
            
            # Annuler les t√¢ches parall√®les si on sort de la boucle
            heartbeat_task.cancel()
            buffer_save_task.cancel()
            
        except Exception as e:
            logger.error(f"Erreur critique dans la boucle WebSocket: {str(e)}")
            self.running = False
    
    async def stop(self) -> None:
        """
        Arr√™te la connexion WebSocket proprement.
        """
        logger.info("Arr√™t de la connexion WebSocket Binance...")
        self.running = False
        
        # üíæ Sauvegarder les buffers avant l'arr√™t pour pr√©server la continuit√©
        logger.info("üíæ Sauvegarde finale des buffers WebSocket...")
        try:
            await self._save_buffers_to_redis()
            logger.info("‚úÖ Buffers sauvegard√©s pour la prochaine restoration")
        except Exception as e:
            logger.warning(f"Erreur sauvegarde finale: {e}")
        
        if self.ws:
            try:
                # D√©sabonnement des streams
                unsubscribe_msg = {
                    "method": "UNSUBSCRIBE",
                    "params": self.stream_paths,
                    "id": int(time.time() * 1000)
                }
                
                await self.ws.send(json.dumps(unsubscribe_msg))
                logger.info("Message de d√©sabonnement envoy√©")
                
                # Fermer la connexion
                await self.ws.close()
                logger.info("Connexion WebSocket ferm√©e")
                
            except Exception as e:
                logger.error(f"Erreur lors de la fermeture du WebSocket: {str(e)}")
        
        # Fermer le client Kafka
        if self.kafka_client:
            self.kafka_client.flush()
            self.kafka_client.close()
            logger.info("Client Kafka ferm√©")

    def _calculate_smooth_indicators(self, symbol: str, timeframe: str, candle_data: Dict, all_indicators: Dict = None) -> Dict:
        """
        üöÄ NOUVEAU : Calcule EMA/MACD de mani√®re incr√©mentale pour √©viter les dents de scie.
        
        Args:
            symbol: Symbole trad√©
            timeframe: Intervalle de temps  
            candle_data: Donn√©es de la bougie actuelle
            
        Returns:
            Dict avec indicateurs EMA/MACD lisses
        """
        result = {}
        current_price = candle_data['close']
        
        try:
            # Cache pour ce symbole/timeframe
            cache = self.incremental_cache[symbol][timeframe]
            
            # üìà EMA 12, 26, 50 (incr√©mentaux avec initialisation intelligente)
            from shared.src.technical_indicators import calculate_ema_incremental
            
            for period in [12, 26, 50]:
                cache_ema_key = f'ema_{period}'
                prev_ema = cache.get(cache_ema_key)
                
                if prev_ema is None:
                    # Premi√®re fois : utiliser la valeur d√©j√† calcul√©e dans all_indicators
                    if all_indicators and cache_ema_key in all_indicators and all_indicators[cache_ema_key] is not None:
                        traditional_ema = all_indicators[cache_ema_key]
                        cache[cache_ema_key] = traditional_ema
                        result[cache_ema_key] = traditional_ema
                        logger.debug(f"üéØ EMA {period} initialis√©e depuis all_indicators: {traditional_ema:.4f}")
                        continue
                    else:
                        logger.debug(f"‚ö†Ô∏è EMA {period} non disponible dans all_indicators, skip incr√©mental")
                
                # Calcul incr√©mental : EMA_new = Œ± √ó price + (1-Œ±) √ó EMA_prev
                new_ema = calculate_ema_incremental(current_price, prev_ema, period)
                result[cache_ema_key] = new_ema
                
                # Mettre √† jour le cache
                cache[cache_ema_key] = new_ema
            
            # üìä MACD incr√©mental (bas√© sur EMA 12/26 du cache)
            from shared.src.technical_indicators import calculate_macd_incremental
            
            prev_ema_fast = cache.get('macd_ema_fast')  # EMA 12 pour MACD
            prev_ema_slow = cache.get('macd_ema_slow')  # EMA 26 pour MACD  
            prev_macd_signal = cache.get('macd_signal')
            
            # Utiliser les EMA du cache si disponibles
            if cache.get('ema_12') is not None and cache.get('ema_26') is not None:
                # Synchroniser les EMA MACD avec les EMA principales
                if prev_ema_fast is None:
                    cache['macd_ema_fast'] = cache['ema_12']
                    prev_ema_fast = cache['ema_12']
                if prev_ema_slow is None:
                    cache['macd_ema_slow'] = cache['ema_26']
                    prev_ema_slow = cache['ema_26']
            
            macd_result = calculate_macd_incremental(
                current_price, prev_ema_fast, prev_ema_slow, prev_macd_signal
            )
            
            result.update({
                'macd_line': macd_result['macd_line'],
                'macd_signal': macd_result['macd_signal'],
                'macd_histogram': macd_result['macd_histogram']
            })
            
            # Mettre √† jour le cache MACD
            cache['macd_ema_fast'] = macd_result['ema_fast']
            cache['macd_ema_slow'] = macd_result['ema_slow'] 
            cache['macd_signal'] = macd_result['macd_signal']
            
            if result:
                logger.debug(f"üöÄ Indicateurs lisses calcul√©s pour {symbol} {timeframe}: "
                            f"EMA12={result.get('ema_12', 0):.4f}, "
                            f"MACD={result.get('macd_line', 0):.4f}")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur calcul indicateurs incr√©mentaux {symbol} {timeframe}: {e}")
            # En cas d'erreur, retourner un dict vide (fallback vers calcul traditionnel)
            result = {}
        
        return result

    def _initialize_incremental_cache_simple(self, symbol: str, timeframe: str):
        """
        üîÑ Initialise le cache incr√©mental de mani√®re simple.
        Les premi√®res EMA seront calcul√©es normalement, puis continu√©es de mani√®re incr√©mentale.
        """
        try:
            # Pour l'instant, initialisation simple : le cache commence vide
            # Les premi√®res valeurs EMA/MACD seront calcul√©es par la m√©thode traditionnelle
            # puis les suivantes seront incr√©mentales et lisses
            
            cache = self.incremental_cache[symbol][timeframe]
            
            # Cache initialement vide - sera rempli au premier calcul
            cache['ema_12'] = None
            cache['ema_26'] = None
            cache['ema_50'] = None
            cache['macd_ema_fast'] = None
            cache['macd_ema_slow'] = None
            cache['macd_signal'] = None
            
            logger.debug(f"üíæ Cache incr√©mental initialis√© (vide) pour {symbol} {timeframe}")
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erreur initialisation cache pour {symbol} {timeframe}: {e}")
    
    def _init_redis_for_buffers(self):
        """Initialise Redis pour la sauvegarde des buffers WebSocket"""
        try:
            from shared.src.redis_client import RedisClient
            self.redis_client = RedisClient()
            logger.info("‚úÖ WebSocket connect√© √† Redis pour persistence des buffers")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è WebSocket sans Redis: {e}")
            self.redis_client = None
    
    async def _save_buffers_to_redis(self):
        """Sauvegarde p√©riodique des buffers WebSocket vers Redis"""
        if not self.redis_client:
            return
        
        try:
            saved_count = 0
            ttl = 24 * 3600  # 24h TTL pour buffers WebSocket
            
            for symbol in self.symbols:
                for timeframe in self.timeframes:
                    # Sauvegarder les price_buffers
                    if symbol in self.price_buffers and timeframe in self.price_buffers[symbol]:
                        if self.price_buffers[symbol][timeframe]:
                            key = f"ws_buffers:prices:{symbol}:{timeframe}"
                            buffer_data = {
                                'prices': self.price_buffers[symbol][timeframe][-100:],  # Garder 100 derniers
                                'timestamp': time.time()
                            }
                            self.redis_client.set(key, json.dumps(buffer_data), expiration=ttl)
                            saved_count += 1
                    
                    # Sauvegarder incremental_cache
                    if symbol in self.incremental_cache and timeframe in self.incremental_cache[symbol]:
                        if self.incremental_cache[symbol][timeframe]:
                            key = f"ws_buffers:incremental:{symbol}:{timeframe}"
                            cache_data = {
                                'cache': self.incremental_cache[symbol][timeframe],
                                'timestamp': time.time()
                            }
                            self.redis_client.set(key, json.dumps(cache_data), expiration=ttl)
                            saved_count += 1
            
            if saved_count > 0:
                logger.debug(f"üíæ {saved_count} buffers WebSocket sauvegard√©s vers Redis")
                
        except Exception as e:
            logger.warning(f"Erreur sauvegarde buffers: {e}")
    
    async def _restore_buffers_from_redis(self):
        """Restaure les buffers WebSocket depuis Redis au d√©marrage"""
        if not self.redis_client:
            return 0
        
        try:
            restored_count = 0
            
            for symbol in self.symbols:
                for timeframe in self.timeframes:
                    # Restaurer price_buffers
                    try:
                        key = f"ws_buffers:prices:{symbol}:{timeframe}"
                        buffer_data = self.redis_client.get(key)
                        if buffer_data:
                            # RedisClientPool fait d√©j√† le parsing JSON automatiquement
                            if isinstance(buffer_data, str):
                                buffer_data = json.loads(buffer_data)
                            self.price_buffers[symbol][timeframe] = buffer_data.get('prices', [])
                            restored_count += 1
                            logger.debug(f"üîÑ Prices buffer restaur√©: {symbol} {timeframe} ({len(self.price_buffers[symbol][timeframe])} prix)")
                    except Exception as e:
                        logger.warning(f"Erreur restoration prices {symbol} {timeframe}: {e}")
                    
                    # Restaurer incremental_cache
                    try:
                        key = f"ws_buffers:incremental:{symbol}:{timeframe}"
                        cache_data = self.redis_client.get(key)
                        if cache_data:
                            # RedisClientPool fait d√©j√† le parsing JSON automatiquement
                            if isinstance(cache_data, str):
                                cache_data = json.loads(cache_data)
                            self.incremental_cache[symbol][timeframe].update(cache_data.get('cache', {}))
                            restored_count += 1
                            logger.debug(f"üîÑ Cache incr√©mental restaur√©: {symbol} {timeframe}")
                    except Exception as e:
                        logger.warning(f"Erreur restoration cache {symbol} {timeframe}: {e}")
            
            if restored_count > 0:
                logger.info(f"üîÑ {restored_count} buffers WebSocket restaur√©s depuis Redis")
            
            return restored_count
            
        except Exception as e:
            logger.error(f"Erreur restoration buffers: {e}")
            return 0
    
    async def _periodic_buffer_save(self):
        """T√¢che p√©riodique de sauvegarde des buffers"""
        while self.running:
            try:
                current_time = time.time()
                if current_time - self.last_buffer_save_time > self.buffer_save_interval:
                    await self._save_buffers_to_redis()
                    self.last_buffer_save_time = current_time
                
                await asyncio.sleep(60)  # V√©rifier toutes les minutes
                
            except Exception as e:
                logger.warning(f"Erreur sauvegarde p√©riodique: {e}")
                await asyncio.sleep(60)

    async def _ensure_sufficient_buffer_data(self):
        """
        S'assure que les buffers WebSocket ont suffisamment de donn√©es (‚â•20 points)
        pour calculer imm√©diatement tous les 33 indicateurs techniques.
        """
        try:
            import aiohttp
            
            total_fetched = 0
            
            for symbol in self.symbols:
                for timeframe in self.timeframes:
                    current_buffer_size = len(self.price_buffers[symbol][timeframe])
                    
                    # V√©rifier TOUS les buffers pour ce symbole/timeframe  
                    highs_size = len(self.high_buffers[symbol][timeframe])
                    lows_size = len(self.low_buffers[symbol][timeframe])
                    volumes_size = len(self.volume_buffers[symbol][timeframe])
                    min_buffer_size = min(current_buffer_size, highs_size, lows_size, volumes_size)
                    
                    if min_buffer_size < 20:
                        # Fetch recent klines from Binance API to fill buffers
                        needed_points = 25 - min_buffer_size  # Fetch a few extra
                        
                        try:
                            url = "https://api.binance.com/api/v3/klines"
                            params = {
                                'symbol': symbol,
                                'interval': timeframe,
                                'limit': needed_points
                            }
                            
                            async with aiohttp.ClientSession() as session:
                                async with session.get(url, params=params, timeout=10) as response:
                                    if response.status == 200:
                                        klines = await response.json()
                                        
                                        # Add to buffers (but don't exceed buffer limit)
                                        for kline in klines:
                                            if len(self.price_buffers[symbol][timeframe]) >= 100:
                                                break  # Respect buffer limit
                                                
                                            self.price_buffers[symbol][timeframe].append(float(kline[4]))  # close
                                            self.high_buffers[symbol][timeframe].append(float(kline[2]))   # high
                                            self.low_buffers[symbol][timeframe].append(float(kline[3]))    # low
                                            self.volume_buffers[symbol][timeframe].append(float(kline[5])) # volume
                                            total_fetched += 1
                                        
                                        logger.debug(f"üìä Pr√©-rempli {symbol} {timeframe}: +{len(klines)} points ‚Üí buffers=[P:{len(self.price_buffers[symbol][timeframe])},H:{len(self.high_buffers[symbol][timeframe])},L:{len(self.low_buffers[symbol][timeframe])},V:{len(self.volume_buffers[symbol][timeframe])}]")
                                    
                                    # Respect rate limits
                                    await asyncio.sleep(0.1)
                                    
                        except Exception as e:
                            logger.warning(f"Erreur pr√©-remplissage {symbol} {timeframe}: {e}")
            
            if total_fetched > 0:
                logger.info(f"üöÄ Buffers WebSocket pr√©-remplis: {total_fetched} points ajout√©s pour calcul imm√©diat des 33 indicateurs")
            else:
                logger.info("üíæ Buffers WebSocket d√©j√† suffisamment remplis")
                
        except Exception as e:
            logger.error(f"‚ùå Erreur lors du pr√©-remplissage des buffers: {e}")

# Fonction principale pour ex√©cuter le WebSocket de mani√®re asynchrone
async def run_binance_websocket():
    """
    Fonction principale pour ex√©cuter le WebSocket Binance.
    """
    # Cr√©er le client Kafka
    kafka_client = KafkaClient()
    
    # Cr√©er et d√©marrer le WebSocket
    ws_client = BinanceWebSocket(kafka_client=kafka_client)
    
    try:
        logger.info("üöÄ D√©marrage du WebSocket Binance...")
        await ws_client.start()
    except KeyboardInterrupt:
        logger.info("Interruption clavier d√©tect√©e")
    except Exception as e:
        logger.error(f"Erreur dans la boucle principale: {str(e)}")
    finally:
        logger.info("Arr√™t du WebSocket Binance...")
        await ws_client.stop()

# Point d'entr√©e pour ex√©cution directe
if __name__ == "__main__":
    try:
        asyncio.run(run_binance_websocket())
    except KeyboardInterrupt:
        logger.info("Programme interrompu par l'utilisateur")