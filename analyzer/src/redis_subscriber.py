"""
Module de gestion des abonnements Redis pour l'analyzer.
S'abonne aux canaux Redis pour recevoir les donnÃ©es de marchÃ© et publier les signaux.
"""
import datetime
import json
import logging
import threading
import time
from typing import Dict, Any, Callable, List, Optional
import queue

# Ajouter le rÃ©pertoire parent au path pour les imports
import sys
import os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "../../")))

from shared.src.config import REDIS_HOST, REDIS_PORT, REDIS_PASSWORD, REDIS_DB, CHANNEL_PREFIX, SYMBOLS
from shared.src.redis_client import RedisClient
from shared.src.kafka_client import KafkaClient
from shared.src.schemas import StrategySignal
from analyzer.src.bar_aggregator import BarAggregator

# Configuration du logging
logger = logging.getLogger(__name__)

class RedisSubscriber:
    """
    Gestionnaire d'abonnements Redis pour l'analyzer.
    ReÃ§oit les donnÃ©es de marchÃ© et publie les signaux de trading.
    """
    
    def __init__(self, symbols: List[str] = None):
        """
        Initialise le subscriber Redis.
        
        Args:
            symbols: Liste des symboles Ã  surveiller
        """
        self.symbols = symbols or SYMBOLS
        self.redis_client = RedisClient()
        self.kafka_client = KafkaClient()
        self.market_data_channels = [f"{CHANNEL_PREFIX}:market:data:{symbol.lower()}" for symbol in self.symbols]
        # AgrÃ©gateurs par symbole
        self.aggregators = {sym: BarAggregator() for sym in self.symbols}
        self.signal_channel = f"{CHANNEL_PREFIX}:analyze:signal"
        
        # File d'attente thread-safe pour les donnÃ©es de marchÃ©
        self.market_data_queue = queue.Queue()
        
        # Thread pour le traitement des donnÃ©es
        self.processing_thread = None
        self.stop_event = threading.Event()
        
        logger.info(f"âœ… RedisSubscriber initialisÃ© pour {len(self.symbols)} symboles: {', '.join(self.symbols)}")
    
    def _process_market_data(self, channel: str, data: Dict[str, Any]) -> None:
        """
        Callback pour traiter les donnÃ©es de marchÃ© reÃ§ues de Redis.
        Ajoute les donnÃ©es Ã  la file d'attente pour traitement.
        
        Args:
            channel: Canal Redis d'oÃ¹ proviennent les donnÃ©es
            data: DonnÃ©es de marchÃ©
        """
        try:
            # AgrÃ©gation : ne pousser que les bougies fermÃ©es
            symbol = data.get("symbol")
            if symbol not in self.aggregators:
                return

            bar = self.aggregators[symbol].add(data)
            if bar is None:
                return          # bougie pas encore fermÃ©e

            self.market_data_queue.put((channel, bar))

            # Log uniquement sur bougie fermÃ©e
            if bar.get("is_closed", False):
                logger.info(f"ğŸ“Š {symbol} : bougie 1 min close={bar['close']}")

        except Exception as e:
            logger.error(f"âŒ Erreur lors du traitement des donnÃ©es de marchÃ©: {str(e)}")
    
    def start_listening(self, callback: Callable[[Dict[str, Any]], None]) -> None:
        """
        DÃ©marre l'Ã©coute des canaux Redis pour les donnÃ©es de marchÃ©.
        
        Args:
            callback: Fonction appelÃ©e pour chaque donnÃ©e de marchÃ© reÃ§ue
        """
        try:
            # S'abonner aux canaux de donnÃ©es de marchÃ©
            self.redis_client.subscribe(self.market_data_channels, self._process_market_data)
            logger.info(f"âœ… AbonnÃ© aux canaux Redis: {', '.join(self.market_data_channels)}")
            
            # DÃ©marrer le thread de traitement des donnÃ©es
            self.stop_event.clear()
            self.processing_thread = threading.Thread(
                target=self._processing_loop,
                args=(callback,),
                daemon=True
            )
            self.processing_thread.start()
            logger.info("âœ… Thread de traitement des donnÃ©es dÃ©marrÃ©")
            
        except Exception as e:
            logger.error(f"âŒ Erreur lors du dÃ©marrage de l'Ã©coute Redis: {str(e)}")
            raise
    
    def _processing_loop(self, callback: Callable[[Dict[str, Any]], None]) -> None:
        """
        Boucle de traitement des donnÃ©es de marchÃ©.
        Cette mÃ©thode s'exÃ©cute dans un thread sÃ©parÃ©.
        
        Args:
            callback: Fonction appelÃ©e pour chaque donnÃ©e de marchÃ©
        """
        while not self.stop_event.is_set():
            try:
                # RÃ©cupÃ©rer une donnÃ©e de la file d'attente avec timeout
                try:
                    channel, data = self.market_data_queue.get(timeout=1.0)
                except queue.Empty:
                    continue
                
                # Appeler le callback avec les donnÃ©es
                callback(data)
                
                # Marquer la tÃ¢che comme terminÃ©e
                self.market_data_queue.task_done()
                
            except Exception as e:
                logger.error(f"âŒ Erreur dans la boucle de traitement: {str(e)}")
                time.sleep(1)  # Pause pour Ã©viter une boucle d'erreur infinie
    
    def publish_signal(self, signal: StrategySignal) -> None:
        try:
            # VÃ©rifier que tous les champs requis sont prÃ©sents
            required_fields = ['symbol', 'strategy', 'side', 'timestamp', 'price']
            missing_fields = [field for field in required_fields if not hasattr(signal, field) or getattr(signal, field) is None]
        
            if missing_fields:
                logger.error(f"âŒ Signal incomplet, ne sera pas publiÃ©. Champs manquants: {missing_fields}")
                return
            
            # Convertir le signal en dictionnaire
            signal_dict = signal.dict()
        
            # S'assurer que timestamp est converti en chaÃ®ne ISO
            if "timestamp" in signal_dict and isinstance(signal_dict["timestamp"], datetime.datetime):
                signal_dict["timestamp"] = signal_dict["timestamp"].isoformat()
        
            # S'assurer que les Ã©nums sont convertis en chaÃ®nes
            if "side" in signal_dict and not isinstance(signal_dict["side"], str):
                signal_dict["side"] = signal_dict["side"].value
        
            if "strength" in signal_dict and not isinstance(signal_dict["strength"], str):
                signal_dict["strength"] = signal_dict["strength"].value
        
            # VÃ©rification finale avant publication
            for field in required_fields:
                if field not in signal_dict or signal_dict[field] is None:
                    logger.error(f"âŒ Champ {field} manquant ou nul aprÃ¨s conversion")
                    return
        
            # Publier sur Redis (pour le coordinator actuel)
            self.redis_client.publish(self.signal_channel, signal_dict)
            
            # Publier aussi sur Kafka (pour le signal_aggregator)
            try:
                self.kafka_client.produce('analyzer.signals', signal_dict)
                logger.info(f"âœ… Signal publiÃ© sur Redis et Kafka: {signal.side} pour {signal.symbol} @ {signal.price}")
            except Exception as e:
                logger.warning(f"âš ï¸ Erreur publication Kafka: {e}. Signal publiÃ© sur Redis seulement.")
                logger.info(f"âœ… Signal publiÃ© sur {self.signal_channel}: {signal.side} pour {signal.symbol} @ {signal.price}")
    
        except Exception as e:
            logger.error(f"âŒ Erreur lors de la publication du signal: {str(e)}")
    
    def stop(self) -> None:
        """
        ArrÃªte l'Ã©coute Redis et nettoie les ressources.
        """
        logger.info("ArrÃªt du subscriber Redis...")
        
        # Signaler aux threads de s'arrÃªter
        self.stop_event.set()
        
        # Attendre que le thread de traitement se termine
        if self.processing_thread and self.processing_thread.is_alive():
            self.processing_thread.join(timeout=5.0)
            logger.info("Thread de traitement arrÃªtÃ©")
        
        # Se dÃ©sabonner des canaux Redis
        self.redis_client.unsubscribe()
        
        # Fermer la connexion Redis
        self.redis_client.close()
        logger.info("âœ… Subscriber Redis arrÃªtÃ©")