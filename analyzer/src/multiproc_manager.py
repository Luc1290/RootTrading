"""
Gestionnaire de processus multiples pour l'analyzer.
Permet d'ex√©cuter plusieurs strat√©gies en parall√®le sur diff√©rents c≈ìurs CPU.
"""
import logging
import multiprocessing as mp
import os
import sys
import time
import datetime
import signal
import threading
from typing import Dict, List, Any, Optional, Callable
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed
import queue
from functools import partial
from multiprocessing import Manager

# Ajouter le r√©pertoire parent au path pour les imports
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "../../")))

from shared.src.config import SYMBOLS
from shared.src.schemas import StrategySignal

from analyzer.src.strategy_loader import StrategyLoader, get_strategy_loader
from analyzer.src.redis_subscriber import RedisSubscriber

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('analyzer.log')
    ]
)
logger = logging.getLogger(__name__)

class AnalyzerManager:
    """
    Gestionnaire de processus pour l'analyzer.
    Distribue l'analyse des donn√©es de march√© sur plusieurs processus/threads.
    """
    
    def __init__(self, symbols: List[str] = None, max_workers: int = None, use_threads: bool = False):
        """
        Initialise le gestionnaire d'analyse.
        
        Args:
            symbols: Liste des symboles √† analyser
            max_workers: Nombre maximum de workers (processus/threads)
            use_threads: Utiliser des threads au lieu de processus
        """
        # Filtrer les symboles vides ou uniquement des espaces
        self.symbols = [s.strip() for s in (symbols or SYMBOLS) if s and s.strip()]
    
        # Log pour d√©boguer
        logger.info(f"Symboles apr√®s filtrage: {self.symbols}")

        if max_workers is None:
            max_workers = max(1, int(mp.cpu_count() * 0.75))
        self.max_workers = max_workers
        self.use_threads = use_threads

        # Regrouper les symboles par worker pour r√©duire la surcharge
        symbols_per_worker = max(1, len(self.symbols) // self.max_workers)
        self.symbol_groups = [
            self.symbols[i:i+symbols_per_worker] 
            for i in range(0, len(self.symbols), symbols_per_worker)
        ]

        # Cr√©er un Manager pour partager les files d'attente entre processus
        if not use_threads:
            self.mp_manager = Manager()
            self.data_queue = self.mp_manager.Queue()
            self.signal_queue = self.mp_manager.Queue()
        else:
            self.data_queue = queue.Queue()
            self.signal_queue = queue.Queue()
    
        # Ajuster le nombre de workers en fonction des groupes
        self.max_workers = min(self.max_workers, len(self.symbol_groups))       
             
        # Cr√©er le chargeur de strat√©gies
        self.strategy_loader = get_strategy_loader()
        
        # √âv√©nement d'arr√™t
        self.stop_event = mp.Event() if not use_threads else threading.Event()
        
        # Subscriber Redis
        self.redis_subscriber = RedisSubscriber(symbols=self.symbols)
        
        # Pool de workers
        self.executor = None
        
        logger.info(f"‚úÖ AnalyzerManager initialis√© avec {self.max_workers} workers "
                   f"({'threads' if use_threads else 'processus'}) pour {len(self.symbols)} symboles")
    
    def _worker_analyze(self, data: Dict[str, Any]) -> List[StrategySignal]:
        """
        Fonction de worker qui analyse les donn√©es de march√©.
        
        Args:
            data: Donn√©es de march√© √† analyser
            
        Returns:
            Liste des signaux g√©n√©r√©s
        """
        try:
            # R√©cup√©rer le chargeur de strat√©gies (local √† chaque processus)
            strategy_loader = get_strategy_loader()
            
            # Traiter les donn√©es avec toutes les strat√©gies
            signals = strategy_loader.process_market_data(data)
            
            return signals
        
        except Exception as e:
            logger.error(f"‚ùå Erreur dans le worker d'analyse: {str(e)}")
            return []
    
    def _handle_market_data(self, data: Dict[str, Any]) -> None:
        """
        Callback appel√© pour chaque donn√©e de march√© re√ßue.
        Ajoute les donn√©es √† la file d'attente pour analyse.
        
        Args:
            data: Donn√©es de march√©
        """
        # Ajouter √† la file d'attente d'analyse
        self.data_queue.put(data)
    
    def _handle_future_result(self, future_result):
        """
        M√©thode statique pour traiter les r√©sultats futurs sans capturer self
        """
        # Extraire les signaux du r√©sultat
        signals = future_result
    
        # Au lieu de publier directement, mettre dans une file d'attente que le processus principal va g√©rer
        self.signal_queue.put(signals)

    def _process_signal_queue(self):
        """
        Processus/thread qui traite la file d'attente des signaux
        """
        logger.info("D√©marrage du processeur de file d'attente des signaux")
    
        # Importer localement pour √©viter les probl√®mes de pickling
        from shared.src.schemas import StrategySignal
        from shared.src.enums import OrderSide, SignalStrength
        from datetime import datetime

        while not self.stop_event.is_set():
            try:
                # R√©cup√©rer les signaux avec timeout
                try:
                    signal_dicts = self.signal_queue.get(timeout=0.1)
                except (queue.Empty, mp.queues.Empty):
                    continue
        
                if signal_dicts:
                    logger.info(f"Traitement de {len(signal_dicts)} signal(s) re√ßus")
                
                    # Traiter chaque dictionnaire de signal
                    for signal_dict in signal_dicts:
                        try:
                            # Convertir les cha√Ænes en objets enum
                            if isinstance(signal_dict.get('side'), str):
                                try:
                                    signal_dict['side'] = OrderSide(signal_dict['side'])
                                except (ValueError, TypeError):
                                    signal_dict['side'] = OrderSide.BUY  # Valeur par d√©faut
                        
                            if isinstance(signal_dict.get('strength'), str):
                                try:
                                    signal_dict['strength'] = SignalStrength(signal_dict['strength'])
                                except (ValueError, TypeError):
                                    signal_dict['strength'] = SignalStrength.MODERATE  # Valeur par d√©faut
                        
                            # Convertir le timestamp
                            if isinstance(signal_dict.get('timestamp'), str):
                                try:
                                    signal_dict['timestamp'] = datetime.fromisoformat(signal_dict['timestamp'])
                                except (ValueError, TypeError):
                                    signal_dict['timestamp'] = datetime.now()
                        
                            # Recr√©er l'objet StrategySignal
                            signal = StrategySignal(**signal_dict)
                        
                            # V√©rifier les champs obligatoires
                            required_fields = ['symbol', 'strategy', 'side', 'timestamp', 'price']
                            missing_fields = [field for field in required_fields 
                                            if not hasattr(signal, field) or getattr(signal, field) is None]
                        
                            if missing_fields:
                                logger.info(f"‚ùå Signal incomplet, ne sera pas publi√©. Champs manquants: {missing_fields}")
                                continue
                        
                            # Publier le signal si valide
                            self.redis_subscriber.publish_signal(signal)
                            logger.info(f"‚úÖ Signal publi√©: {signal.side} pour {signal.symbol} @ {signal.price}")
                    
                        except Exception as e:
                            logger.info(f"‚ùå Erreur lors du traitement du signal: {str(e)}", exc_info=True)
            
                # Marquer comme trait√©
                self.signal_queue.task_done()
        
            except Exception as e:
                logger.info(f"‚ùå Erreur dans le processeur de file d'attente des signaux: {str(e)}")
                time.sleep(0.1)

        logger.info("Processeur de file d'attente des signaux arr√™t√©")

    def _publish_signals(self, signals: List[StrategySignal]) -> None:
        """
        Publie les signaux g√©n√©r√©s sur Redis.
        
        Args:
            signals: Liste des signaux √† publier
        """
        if not signals:
            return
        
        for signal in signals:
            self.redis_subscriber.publish_signal(signal)
    
    def _process_data_queue(self) -> None:
        """
        Processus/thread qui traite la file d'attente de donn√©es.
        Cette version n'utilise pas de pool d'executors pour √©viter les probl√®mes de pickling.
        """
        logger.info("D√©marrage du processeur de file d'attente de donn√©es")
    
        # Cr√©er un loader de strat√©gies local √† ce processus
        local_strategy_loader = None
    
        while not self.stop_event.is_set():
            try:
                # Ne pas bloquer ind√©finiment pour pouvoir v√©rifier stop_event
                try:
                    # R√©cup√©rer les donn√©es avec timeout
                    data = self.data_queue.get(timeout=0.1)
                except (queue.Empty, mp.queues.Empty):
                    continue
            
                # Ne traiter que les chandeliers ferm√©s
                if not data.get('is_closed', False):
                    continue

                # Ajouter ces logs pour d√©boguer
                logger.info(f"Donn√©es re√ßues pour {data.get('symbol')}: is_closed={data.get('is_closed', False)}, close={data.get('close')}")
            
                # Extraire uniquement les donn√©es n√©cessaires
                analysis_data = {
                    'symbol': data.get('symbol', ''),
                    'open': data.get('open', 0.0),
                    'high': data.get('high', 0.0),
                    'low': data.get('low', 0.0),
                    'close': data.get('close', 0.0),
                    'volume': data.get('volume', 0.0),
                    'timestamp': data.get('timestamp', ''),
                    'start_time': data.get('start_time', 0),
                    'interval': data.get('interval', ''),
                    'is_closed': data.get('is_closed', True)
                }
            
                # Initialiser le strategy loader au besoin (seulement au premier appel)
                if local_strategy_loader is None:
                    from analyzer.src.strategy_loader import get_strategy_loader
                    local_strategy_loader = get_strategy_loader()
                    logger.info("Loader de strat√©gies local cr√©√© dans le processus d'analyse")
            
                # Analyser les donn√©es directement dans ce processus
                try:
                    signals = local_strategy_loader.process_market_data(analysis_data)
                
                    # Si des signaux sont g√©n√©r√©s, les convertir en dictionnaires et les envoyer
                    if signals:
                        # Convertir les signaux en dictionnaires pour √©viter les probl√®mes de pickling
                        signal_dicts = []
                        for signal in signals:
                            try:
                                # Assurez-vous que tous les champs requis sont pr√©sents et correctement format√©s
                                side_value = signal.side.value if hasattr(signal.side, 'value') else str(signal.side)
                                strength_value = signal.strength.value if hasattr(signal.strength, 'value') else "MODERATE"
                                timestamp_value = signal.timestamp.isoformat() if hasattr(signal.timestamp, 'isoformat') else datetime.now().isoformat()
            
                                # Cr√©er un dictionnaire avec les donn√©es du signal
                                signal_dict = {
                                    'symbol': signal.symbol,
                                    'strategy': signal.strategy,
                                    'side': side_value,
                                    'timestamp': timestamp_value,
                                    'price': float(signal.price),
                                    'confidence': float(signal.confidence) if hasattr(signal, 'confidence') else 0.5,
                                    'strength': strength_value,
                                    'metadata': dict(signal.metadata) if hasattr(signal, 'metadata') and signal.metadata else {}
                                }
            
                                # V√©rifier explicitement que tous les champs requis sont pr√©sents
                                required_fields = ['symbol', 'strategy', 'side', 'timestamp', 'price']
                                missing_fields = [field for field in required_fields if field not in signal_dict or signal_dict[field] is None]
            
                                if missing_fields:
                                    logger.warning(f"Signal incomplet, ne sera pas ajout√©. Champs manquants: {missing_fields}")
                                    continue
                
                                signal_dicts.append(signal_dict)
                            except Exception as e:
                                logger.error(f"Erreur lors de la conversion du signal: {str(e)}")
                    
                            # Mettre les dictionnaires sur la file d'attente
                            if signal_dicts:
                                self.signal_queue.put(signal_dicts)
                            logger.info(f"Mis {len(signal_dicts)} signaux sur la file d'attente")
            
                except Exception as e:
                    logger.info(f"‚ùå Erreur lors de l'analyse des donn√©es: {str(e)}")
        
            except Exception as e:
                logger.error(f"‚ùå Erreur dans le processeur de file d'attente: {str(e)}")
                time.sleep(0.1)  # Pause pour √©viter une boucle d'erreur infinie
    
        logger.info("Processeur de file d'attente de donn√©es arr√™t√©")
      
    def start(self):
        """
        D√©marre le gestionnaire d'analyse.
        """
        logger.info("üöÄ D√©marrage du gestionnaire d'analyse...")
    
        # R√©initialiser l'√©v√©nement d'arr√™t
        self.stop_event.clear()
    
        # D√©marrer le subscriber Redis
        self.redis_subscriber.start_listening(self._handle_market_data)
    
        # D√©marrer le processus/thread de traitement de file d'attente des donn√©es
        if self.use_threads:
            self.queue_processor = threading.Thread(
                target=self._process_data_queue,
                daemon=True
            )
        
            # Ajouter le processeur de signaux
            self.signal_processor = threading.Thread(
                target=self._process_signal_queue,
                daemon=True
            )
        else:
            self.queue_processor = mp.Process(
                target=self._process_data_queue,
                daemon=False
            )
        
            # Ajouter le processeur de signaux
            self.signal_processor = threading.Thread(  # Utiliser un thread m√™me en mode processus
                target=self._process_signal_queue,
                daemon=True
            )
    
        self.queue_processor.start()
        self.signal_processor.start()
        logger.info("‚úÖ Gestionnaire d'analyse d√©marr√©")
    
    def stop(self):
        """
        Arr√™te le gestionnaire d'analyse.
        """
        logger.info("Arr√™t du gestionnaire d'analyse...")
    
        # Signaler l'arr√™t
        self.stop_event.set()
    
        # Attendre que les processeurs se terminent
        if self.queue_processor.is_alive():
            if self.use_threads:
                self.queue_processor.join(timeout=5.0)
            else:
                self.queue_processor.join(timeout=5.0)
                if self.queue_processor.is_alive():
                    self.queue_processor.terminate()
        
            logger.info("Processeur de file d'attente de donn√©es arr√™t√©")
    
        # Attendre que le processeur de signaux se termine
        if self.signal_processor.is_alive():
            self.signal_processor.join(timeout=5.0)
            logger.info("Processeur de file d'attente arr√™t√©")
    
        # Arr√™ter le subscriber Redis
        self.redis_subscriber.stop()
    
        # Vider les files d'attente
        while not self.data_queue.empty():
            try:
                self.data_queue.get_nowait()
            except (queue.Empty, mp.queues.Empty):
                break
    
        while not self.signal_queue.empty():
            try:
                self.signal_queue.get_nowait()
            except (queue.Empty, mp.queues.Empty):
                break
    
        logger.info("‚úÖ Gestionnaire d'analyse arr√™t√©")

# Point d'entr√©e pour ex√©cution directe
if __name__ == "__main__":
    try:
        # Configurer le gestionnaire de signaux pour l'arr√™t propre
        stop_event = mp.Event()
        
        def signal_handler(sig, frame):
            logger.info(f"Signal {sig} re√ßu, arr√™t en cours...")
            stop_event.set()
        
        signal.signal(signal.SIGINT, signal_handler)
        signal.signal(signal.SIGTERM, signal_handler)
        
        # Cr√©er et d√©marrer le gestionnaire d'analyse
        manager = AnalyzerManager()
        manager.start()
        
        # Attendre le signal d'arr√™t
        while not stop_event.is_set():
            time.sleep(1.0)
        
        # Arr√™ter le gestionnaire
        manager.stop()
        logger.info("Fin du programme")
        
    except KeyboardInterrupt:
        logger.info("Programme interrompu par l'utilisateur")
    except Exception as e:
        logger.error(f"Erreur critique: {str(e)}")