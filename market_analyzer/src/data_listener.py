"""
Market Data Listener
√âcoute les nouvelles donn√©es dans market_data et d√©clenche les calculs.
Syst√®me trigger-based pour traitement temps r√©el.
"""

import asyncio
import builtins
import contextlib
import json
import logging
import sys
from datetime import datetime, timezone
# Ajouter les chemins pour les imports
from pathlib import Path
from typing import Any

import asyncpg  # type: ignore

from shared.src.config import get_db_config

from .indicator_processor import IndicatorProcessor

sys.path.append(str((Path(__file__).parent / "../../").resolve()))


logger = logging.getLogger(__name__)


class DataListener:
    """
    √âcoute les changements dans market_data et d√©clenche les calculs automatiquement.
    Utilise PostgreSQL LISTEN/NOTIFY pour traitement temps r√©el.
    """

    def __init__(self):
        self.db_pool = None
        self.listen_conn = None
        self.running = False
        self.processed_count = 0
        self.indicator_processor = IndicatorProcessor()
        # Semaphore pour limiter les connexions DB concurrentes
        self.db_semaphore = asyncio.Semaphore(
            15)  # Max 15 connexions simultan√©es

        logger.info("üì° DataListener initialis√©")

    async def initialize(self):
        """Initialise les connexions et le moteur de calcul."""
        try:
            db_config = get_db_config()

            # Pool principal pour les requ√™tes - AUGMENT√â pour traitement
            # historique
            self.db_pool = await asyncpg.create_pool(
                host=db_config["host"],
                port=db_config["port"],
                database=db_config["database"],
                user=db_config["user"],
                password=db_config["password"],
                min_size=5,
                max_size=25,  # Augment√© de 10 √† 25
                command_timeout=30,  # Timeout requ√™tes longues
                server_settings={
                    "application_name": "market_analyzer_pool",
                    "statement_timeout": "30000",  # 30s timeout pour statements
                    # Tuer les "idle in transaction" apr√®s 60s
                    "idle_in_transaction_session_timeout": "60000",
                },
            )

            # Connexion d√©di√©e pour LISTEN
            self.listen_conn = await asyncpg.connect(
                host=db_config["host"],
                port=db_config["port"],
                database=db_config["database"],
                user=db_config["user"],
                password=db_config["password"],
                server_settings={
                    "application_name": "market_analyzer_listener",
                    "statement_timeout": "60000",  # 60s timeout pour LISTEN
                    "idle_in_transaction_session_timeout": "120000",  # 2min pour listener
                },
            )

            # Initialiser le processeur d'indicateurs
            await self.indicator_processor.initialize()

            # Cr√©er le trigger si n√©cessaire
            await self._setup_database_trigger()

            logger.info("‚úÖ DataListener connect√© et pr√™t")

        except Exception:
            logger.exception("‚ùå Erreur initialisation DataListener")
            raise

    async def _setup_database_trigger(self):
        """
        Configure le trigger PostgreSQL pour notifier les nouveaux inserts.

        IDEMPOTENT: Nettoie d'abord les anciens triggers/fonctions avant de recr√©er.
        Safe pour rebuild/restart sans intervention manuelle.
        """

        # √âTAPE 1: Nettoyer proprement les anciens triggers et fonctions
        cleanup_sql = """
            -- Supprimer le trigger s'il existe
            DROP TRIGGER IF EXISTS market_data_change_trigger ON market_data;

            -- Supprimer l'ancienne fonction (CASCADE pour forcer si r√©f√©renc√©e)
            DROP FUNCTION IF EXISTS notify_market_data_change() CASCADE;
        """

        # √âTAPE 2: Cr√©er la nouvelle fonction trigger
        trigger_function = """
            CREATE OR REPLACE FUNCTION notify_market_data_change()
            RETURNS TRIGGER AS $$
            BEGIN
                -- Envoyer notification avec les d√©tails de la nouvelle donn√©e
                PERFORM pg_notify(
                    'market_data_change',
                    json_build_object(
                        'symbol', NEW.symbol,
                        'timeframe', NEW.timeframe,
                        'time', extract(epoch from NEW.time)::bigint,
                        'action', TG_OP
                    )::text
                );
                RETURN NEW;
            END;
            $$ LANGUAGE plpgsql;
        """

        # √âTAPE 3: Cr√©er le trigger
        trigger_definition = """
            CREATE TRIGGER market_data_change_trigger
                AFTER INSERT OR UPDATE ON market_data
                FOR EACH ROW
                EXECUTE FUNCTION notify_market_data_change();
        """

        try:
            if not self.db_pool:
                raise RuntimeError("db_pool not initialized")
            async with self.db_pool.acquire() as conn:
                # Ex√©cuter le nettoyage d'abord
                logger.info("üßπ Nettoyage des anciens triggers...")
                await conn.execute(cleanup_sql)

                # Cr√©er la fonction
                logger.info("üîß Cr√©ation de la fonction trigger...")
                await conn.execute(trigger_function)

                # Cr√©er le trigger
                logger.info("üéØ Cr√©ation du trigger...")
                await conn.execute(trigger_definition)

            logger.info("‚úÖ Trigger PostgreSQL configur√© pour market_data")

        except Exception:
            logger.exception("‚ùå Erreur configuration trigger")
            # Ne pas raise pour permettre au service de d√©marrer quand m√™me
            logger.warning("‚ö†Ô∏è Service d√©marr√© sans trigger (mode d√©grad√©)")
            # raise  # Comment√© pour ne pas bloquer le d√©marrage

    async def start_listening(self):
        """D√©marre l'√©coute des notifications."""
        self.running = True
        logger.info("üéß D√©marrage de l'√©coute des changements market_data...")

        try:
            # S'abonner aux notifications
            if not self.listen_conn:
                raise RuntimeError("listen_conn not initialized")
            await self.listen_conn.add_listener(
                "market_data_change", self._handle_notification
            )

            logger.info("‚úÖ √âcoute active - en attente de nouvelles donn√©es...")

            # Boucle principale d'√©coute
            while self.running:
                try:
                    # Attendre les notifications (bloquant)
                    # Petite pause pour √©viter de surcharger
                    await asyncio.sleep(0.1)

                except asyncio.CancelledError:
                    logger.info("üõë √âcoute interrompue")
                    break
                except Exception:
                    logger.exception("‚ùå Erreur dans la boucle d'√©coute")
                    await asyncio.sleep(1)  # Attendre avant de retry

        except Exception:
            logger.exception("‚ùå Erreur critique dans l'√©coute")
        finally:
            await self._cleanup()

    async def _handle_notification(self, _connection, _pid, _channel, payload):
        """
        Gestionnaire appel√© quand une notification est re√ßue.

        Args:
            payload: JSON avec symbol, timeframe, time, action
        """
        try:
            # Parser la notification
            data = json.loads(payload)
            symbol = data["symbol"]
            timeframe = data["timeframe"]
            timestamp = datetime.fromtimestamp(data["time"], tz=timezone.utc)
            action = data["action"]

            logger.debug(
                f"üì¨ Notification re√ßue: {action} {symbol} {timeframe} @ {timestamp}"
            )

            # Ne traiter que les INSERT (nouvelles donn√©es)
            if action == "INSERT":
                await self._process_new_data(symbol, timeframe, timestamp)
                self.processed_count += 1

                if self.processed_count % 10 == 0:
                    logger.info(
                        f"üìä {self.processed_count} analyses compl√©t√©es")

        except Exception:
            logger.exception("‚ùå Erreur traitement notification")
            logger.exception("Payload: ")

    async def _process_new_data(
            self,
            symbol: str,
            timeframe: str,
            timestamp: datetime):
        """
        Traite une nouvelle donn√©e en lan√ßant les calculs.

        Args:
            symbol: Symbole (ex: BTCUSDC)
            timeframe: Timeframe (ex: 1m, 5m)
            timestamp: Timestamp de la nouvelle donn√©e
        """
        try:
            # V√©rifier si on a d√©j√† analys√© cette donn√©e
            if await self._is_already_analyzed(symbol, timeframe, timestamp):
                logger.debug(
                    f"‚è≠Ô∏è D√©j√† analys√©: {symbol} {timeframe} @ {timestamp}")
                return

            # Appeler le processeur d'indicateurs
            await self.indicator_processor.process_new_data(
                symbol, timeframe, timestamp
            )

            logger.debug(f"‚úÖ Traitement termin√©: {symbol} {timeframe}")

        except Exception:
            logger.exception("‚ùå Erreur traitement donn√©es")

    async def _is_already_analyzed(
        self, symbol: str, timeframe: str, timestamp: datetime
    ) -> bool:
        """V√©rifie si cette donn√©e a d√©j√† √©t√© analys√©e."""

        query = """
            SELECT 1 FROM analyzer_data
            WHERE symbol = $1 AND timeframe = $2 AND time = $3
            LIMIT 1
        """

        try:
            if not self.db_pool:
                raise RuntimeError("db_pool not initialized")
            async with self.db_pool.acquire() as conn:
                result = await asyncio.wait_for(
                    conn.fetchval(query, symbol, timeframe, timestamp), timeout=5.0
                )
                return result is not None

        except Exception:
            logger.exception("‚ùå Erreur v√©rification analyse")
            return False

    async def process_historical_optimized(
        self,
        symbol: str | None = None,
        timeframe: str | None = None,
        limit: int = 1000000,
    ):
        """
        Traite les donn√©es historiques de mani√®re OPTIMIS√âE.
        - Traite par symbole
        - Dans l'ordre chronologique
        - R√©utilise les calculs pr√©c√©dents
        """
        logger.info(
            "üöÄ D√©marrage traitement optimis√© des donn√©es non analys√©es...")

        # Si pas de symbole sp√©cifique, traiter chaque symbole s√©par√©ment
        if symbol is None:
            symbols_to_process = await self._get_symbols_with_gaps()
            logger.info(f"üìä Symboles √† traiter: {symbols_to_process}")

            total_processed = 0
            for sym in symbols_to_process:
                processed = await self._process_symbol_historical(sym, timeframe, limit)
                total_processed += processed
                # Petite pause entre chaque symbole
                await asyncio.sleep(1)

            logger.info(
                f"‚úÖ Traitement optimis√© termin√©: {total_processed} donn√©es analys√©es"
            )
        else:
            processed = await self._process_symbol_historical(symbol, timeframe, limit)
            logger.info(
                f"‚úÖ Traitement optimis√© termin√© pour {symbol}: {processed} donn√©es analys√©es"
            )

    async def _get_symbols_with_gaps(self) -> list:
        """R√©cup√®re la liste des symboles ayant des donn√©es non analys√©es."""
        query = """
            SELECT DISTINCT md.symbol
            FROM market_data md
            LEFT JOIN analyzer_data ad ON (
                md.symbol = ad.symbol AND
                md.timeframe = ad.timeframe AND
                md.time = ad.time
            )
            WHERE ad.time IS NULL
            ORDER BY md.symbol
        """

        if not self.db_pool:
            raise RuntimeError("db_pool not initialized")
        async with self.db_pool.acquire() as conn:
            rows = await conn.fetch(query)
            return [row["symbol"] for row in rows]

    async def _process_symbol_historical(
        self, symbol: str, timeframe: str | None = None, limit: int = 1000000
    ) -> int:
        """Traite l'historique d'un symbole sp√©cifique dans l'ordre chronologique."""
        logger.info(f"üîÑ Traitement historique optimis√© pour {symbol}...")

        # Requ√™te optimis√©e : EXISTS plus rapide que LEFT JOIN
        query = """
            SELECT DISTINCT md.timeframe, md.time
            FROM market_data md
            WHERE md.symbol = $1
            AND NOT EXISTS (
                SELECT 1 FROM analyzer_data ad
                WHERE ad.symbol = md.symbol
                AND ad.timeframe = md.timeframe
                AND ad.time = md.time
            )
        """

        params: list[Any] = [symbol]

        if timeframe:
            query += " AND md.timeframe = $2"
            params.append(timeframe)

        query += " ORDER BY md.timeframe, md.time ASC LIMIT $" + \
            str(len(params) + 1)
        params.append(limit)

        total_processed = 0

        try:
            if not self.db_pool:
                raise RuntimeError("db_pool not initialized")
            async with self.db_pool.acquire() as conn:
                rows = await asyncio.wait_for(conn.fetch(query, *params), timeout=10.0)

            if not rows:
                logger.info(f"‚úÖ {symbol}: Aucune donn√©e non analys√©e")
                return 0

            logger.info(f"üìä {symbol}: {len(rows)} donn√©es √† analyser")

            # Grouper par timeframe pour optimiser
            timeframes: dict[str, list] = {}
            for row in rows:
                tf = row["timeframe"]
                if tf not in timeframes:
                    timeframes[tf] = []
                timeframes[tf].append(row["time"])

            # Traiter chaque timeframe s√©par√©ment dans l'ordre chronologique
            for tf, timestamps in timeframes.items():
                logger.info(
                    f"‚è±Ô∏è {symbol} {tf}: {len(timestamps)} donn√©es √† traiter")

                processed = 0
                errors = 0

                # Traiter dans l'ordre chronologique
                for timestamp in timestamps:
                    try:
                        async with self.db_semaphore:  # Limiter les connexions concurrentes
                            await self.indicator_processor.process_new_data(
                                symbol, tf, timestamp
                            )
                        processed += 1

                        # Log de progression
                        if processed % 100 == 0:
                            percent = (processed / len(timestamps)) * 100
                            logger.info(
                                f"üìà {symbol} {tf}: {processed}/{len(timestamps)} ({percent:.1f}%)"
                            )

                        # Pause raisonnable tous les 10 √©l√©ments pour √©viter
                        # saturation DB
                        if processed % 10 == 0:
                            await asyncio.sleep(0.01)  # 10ms

                    except Exception:
                        errors += 1
                        if errors <= 5:  # Limiter les logs d'erreur
                            logger.exception(
                                "‚ùå Erreur {symbol} {tf} @ {timestamp}")
                        continue

                logger.info(
                    f"‚úÖ {symbol} {tf}: {processed} donn√©es trait√©es ({errors} erreurs)"
                )
                total_processed += processed

            logger.info(
                f"‚úÖ {symbol}: Traitement termin√© - {total_processed} donn√©es analys√©es"
            )

        except Exception:
            logger.exception("‚ùå Erreur traitement {symbol}")

        return total_processed

    async def get_stats(self) -> dict[str, Any]:
        """Retourne les statistiques du listener."""

        # Compter les donn√©es avec focus sur les gaps r√©cents (24h)
        stats_query = """
            SELECT
                (SELECT COUNT(*) FROM market_data) as total_market_data,
                (SELECT COUNT(*) FROM analyzer_data) as total_analyzer_data,
                (SELECT COUNT(DISTINCT symbol) FROM market_data) as symbols_count,
                (SELECT COUNT(DISTINCT timeframe) FROM market_data) as timeframes_count,
                (SELECT COUNT(*) FROM market_data md
                 LEFT JOIN analyzer_data ad ON (
                     md.symbol = ad.symbol AND
                     md.timeframe = ad.timeframe AND
                     md.time = ad.time
                 )
                 WHERE ad.time IS NULL
                 AND md.time >= NOW() - INTERVAL '24 hours'
                ) as recent_gaps_24h
        """

        try:
            if not self.db_pool:
                raise RuntimeError("db_pool not initialized")
            async with self.db_pool.acquire() as conn:
                row = await conn.fetchrow(stats_query)

                coverage_percent = 0
                if row["total_market_data"] > 0:
                    coverage_percent = (
                        row["total_analyzer_data"] / row["total_market_data"]
                    ) * 100

                return {
                    "running": self.running,
                    "processed_count": self.processed_count,
                    "total_market_data": row["total_market_data"],
                    "total_analyzer_data": row["total_analyzer_data"],
                    "coverage_percent": round(coverage_percent, 2),
                    "symbols_count": row["symbols_count"],
                    "timeframes_count": row["timeframes_count"],
                    "missing_analyses": row[
                        "recent_gaps_24h"
                    ],  # Maintenant seulement les gaps r√©cents
                    "total_historical_gaps": row["total_market_data"]
                    - row["total_analyzer_data"],
                }

        except Exception as e:
            logger.exception("‚ùå Erreur r√©cup√©ration stats")
            return {
                "running": self.running,
                "processed_count": self.processed_count,
                "error": str(e),
            }

    async def _cleanup(self):
        """
        Nettoie les ressources proprement.

        IMPORTANT: Ferme toutes les connexions pour √©viter les orphelins
        lors d'un rebuild du container.
        """
        try:
            # Stopper l'√©coute des notifications
            if self.listen_conn:
                try:
                    await self.listen_conn.remove_listener(
                        "market_data_change", self._handle_notification
                    )
                    await self.listen_conn.close()
                    logger.info("‚úÖ Connexion LISTEN ferm√©e")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Erreur fermeture LISTEN: {e}")

            # Fermer le processeur d'indicateurs
            try:
                await self.indicator_processor.close()
                logger.info("‚úÖ Indicator processor ferm√©")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Erreur fermeture processor: {e}")

            # Fermer le pool de connexions
            if self.db_pool:
                try:
                    # Attendre que toutes les connexions se terminent (timeout
                    # 5s)
                    await asyncio.wait_for(self.db_pool.close(), timeout=5.0)
                    logger.info("‚úÖ Pool DB ferm√© proprement")
                except asyncio.TimeoutError:
                    logger.warning(
                        "‚ö†Ô∏è Timeout fermeture pool - connexions forc√©es")
                    # Force terminate si timeout
                    await self.db_pool.terminate()
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Erreur fermeture pool: {e}")

            logger.info("üßπ DataListener nettoy√© compl√®tement")

        except Exception:
            logger.exception("‚ùå Erreur critique nettoyage")
            # Toujours essayer de terminer le pool
            if self.db_pool:
                with contextlib.suppress(builtins.BaseException):
                    await self.db_pool.terminate()

    async def stop(self):
        """Arr√™te l'√©coute."""
        logger.info("üõë Arr√™t du DataListener...")
        self.running = False
        await self._cleanup()
