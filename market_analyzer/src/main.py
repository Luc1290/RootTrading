"""
Market Analyzer Service - Point d'entrÃ©e principal
Service qui calcule TOUS les indicateurs techniques Ã  partir des donnÃ©es OHLCV.
Architecture propre : Gateway â†’ market_data â†’ Market Analyzer â†’ analyzer_data
"""

import asyncio
import signal
import sys
import time
from pathlib import Path

from aiohttp import web

from market_analyzer.src.data_listener import DataListener
from shared.src.config import SYMBOLS

# Ajouter les chemins pour les imports
sys.path.append(str(Path(__file__).parent / "../../"))


# Configuration du logging centralisÃ©e
from shared.logging_config import setup_logging

logger = setup_logging("market_analyzer", log_level="INFO")

# Variables globales
data_listener = None
running = True
start_time = time.time()

# Routes pour l'API de diagnostic
routes = web.RouteTableDef()


@routes.get("/health")
async def health_check(_request):
    """Point de terminaison santÃ© du service."""
    uptime = time.time() - start_time
    return web.json_response(
        {
            "status": "ok",
            "timestamp": time.time(),
            "uptime": uptime,
            "service": "market_analyzer",
            "mode": "active" if running else "stopping",
            "symbols": SYMBOLS,
            "architecture": "clean_calculation_engine",
        }
    )


@routes.get("/stats")
async def get_stats(_request):
    """Statistiques du Market Analyzer."""
    if data_listener:
        stats = await data_listener.get_stats()
        return web.json_response(stats)

    return web.json_response(
        {"error": "DataListener not initialized", "status": "not_running"},
        status=503,
    )


@routes.post("/process-historical")
async def process_historical(request):
    """API pour traiter les donnÃ©es historiques."""
    if not data_listener:
        return web.json_response({"error": "DataListener not initialized"}, status=503)

    try:
        # Parser les paramÃ¨tres
        data = (
            await request.json() if request.content_type == "application/json" else {}
        )
        symbol = data.get("symbol")
        timeframe = data.get("timeframe")
        limit = data.get("limit", 1000)

        # Lancer le traitement optimisÃ© en arriÃ¨re-plan
        _task = asyncio.create_task(
            data_listener.process_historical_optimized(symbol, timeframe, limit)
        )

        return web.json_response(
            {
                "status": "started",
                "message": f"Traitement historique lancÃ© (limit: {limit})",
                "symbol": symbol,
                "timeframe": timeframe,
            }
        )

    except Exception as e:
        return web.json_response(
            {"error": f"Erreur lancement traitement: {e}"}, status=500
        )


@routes.get("/coverage")
async def get_coverage(_request):
    """Analyse de couverture des donnÃ©es."""
    if not data_listener:
        return web.json_response({"error": "DataListener not initialized"}, status=503)

    try:
        # Statistiques par symbole/timeframe
        coverage_query = """
            SELECT
                md.symbol,
                md.timeframe,
                COUNT(md.*) as market_data_count,
                COUNT(ad.*) as analyzer_data_count,
                ROUND((COUNT(ad.*)::FLOAT / COUNT(md.*)) * 100, 2) as coverage_percent
            FROM market_data md
            LEFT JOIN analyzer_data ad ON (
                md.symbol = ad.symbol AND
                md.timeframe = ad.timeframe AND
                md.time = ad.time
            )
            GROUP BY md.symbol, md.timeframe
            ORDER BY md.symbol, md.timeframe
        """

        async with data_listener.db_pool.acquire() as conn:
            rows = await conn.fetch(coverage_query)

        coverage_data = []
        for row in rows:
            coverage_data.append(
                {
                    "symbol": row["symbol"],
                    "timeframe": row["timeframe"],
                    "market_data_count": row["market_data_count"],
                    "analyzer_data_count": row["analyzer_data_count"],
                    "coverage_percent": float(row["coverage_percent"]),
                    "missing_count": row["market_data_count"]
                    - row["analyzer_data_count"],
                }
            )

        return web.json_response(
            {
                "coverage_by_asset": coverage_data,
                "total_assets": len({row["symbol"] for row in coverage_data}),
                "total_timeframes": len({row["timeframe"] for row in coverage_data}),
            }
        )

    except Exception as e:
        return web.json_response(
            {"error": f"Erreur analyse couverture: {e}"}, status=500
        )


async def start_http_server():
    """DÃ©marre le serveur HTTP pour l'API."""
    app = web.Application()
    app.add_routes(routes)

    runner = web.AppRunner(app)
    await runner.setup()

    # Port 5020 pour le Market Analyzer
    site = web.TCPSite(runner, "0.0.0.0", 5020)
    await site.start()

    logger.info("âœ… API HTTP dÃ©marrÃ©e sur le port 5020")
    return runner


async def shutdown(signal_type, loop):
    """GÃ¨re l'arrÃªt propre du service."""
    global running

    logger.info(f"Signal {signal_type.name} reÃ§u, arrÃªt en cours...")
    running = False

    # ArrÃªter le DataListener AVANT de fermer la boucle
    if data_listener:
        try:
            await data_listener.stop()
            logger.info("âœ… DataListener arrÃªtÃ© proprement")
        except Exception:
            logger.exception("âŒ Erreur arrÃªt DataListener")

    # Attendre plus longtemps pour que toutes les connexions DB se ferment
    await asyncio.sleep(1.0)

    # Annuler toutes les tÃ¢ches pendantes
    try:
        pending_tasks = [
            task
            for task in asyncio.all_tasks(loop)
            if not task.done() and task != asyncio.current_task()
        ]
        if pending_tasks:
            logger.info(f"Annulation de {len(pending_tasks)} tÃ¢ches pendantes...")
            for task in pending_tasks:
                task.cancel()
            # Attendre que les tÃ¢ches se terminent
            await asyncio.gather(*pending_tasks, return_exceptions=True)
    except Exception as e:
        logger.warning(f"Erreur lors de l'annulation des tÃ¢ches: {e}")

    logger.info("Market Analyzer terminÃ©")

    # NE PAS appeler loop.stop() ici - laisser le main() se terminer
    # naturellement


async def main():
    """Fonction principale du Market Analyzer."""
    global data_listener

    logger.info("ğŸš€ DÃ©marrage du Market Analyzer Service")
    logger.info("ğŸ¯ Architecture: Calcul TOUS les indicateurs depuis market_data")
    logger.info(f"ğŸ“Š Symboles surveillÃ©s: {', '.join(SYMBOLS)}")

    # DÃ©marrer le serveur HTTP
    http_runner = await start_http_server()

    try:
        # Initialiser le DataListener
        data_listener = DataListener()
        await data_listener.initialize()

        logger.info("âœ… Market Analyzer initialisÃ©")

        # DÃ©marrer l'Ã©coute temps rÃ©el IMMÃ‰DIATEMENT (non-bloquant)
        logger.info("ğŸ§ DÃ©marrage de l'Ã©coute temps rÃ©el...")
        _listening_task = asyncio.create_task(data_listener.start_listening())

        # Proposer de traiter l'historique en parallÃ¨le
        logger.info("ğŸ” VÃ©rification de la couverture des donnÃ©es...")
        stats = await data_listener.get_stats()

        if stats["missing_analyses"] > 0:
            logger.info(
                f"âš ï¸ {stats['missing_analyses']} donnÃ©es non analysÃ©es dÃ©tectÃ©es"
            )
            logger.info("ğŸ’¡ DÃ©marrage du traitement historique automatique...")

            # LIMITÃ‰ Ã  100k donnÃ©es max au dÃ©marrage pour Ã©viter saturation DB
            await data_listener.process_historical_optimized(limit=100000)
        else:
            logger.info("âœ… Toutes les donnÃ©es sont analysÃ©es")

        # Afficher les statistiques finales
        final_stats = await data_listener.get_stats()
        logger.info(
            f"ğŸ“Š Couverture: {final_stats['coverage_percent']}% ({final_stats['total_analyzer_data']}/{final_stats['total_market_data']})"
        )

        # Attendre que l'Ã©coute temps rÃ©el continue ou que l'arrÃªt soit demandÃ©
        logger.info("âœ… Traitement historique terminÃ© - Ã©coute temps rÃ©el active")

        # Boucle d'attente au lieu d'attendre le listening_task qui peut ne
        # jamais se terminer
        while running:
            await asyncio.sleep(1)

        logger.info("ğŸ“Ÿ ArrÃªt demandÃ© - terminaison en cours...")

    except Exception:
        logger.exception("âŒ Erreur critique")
    finally:
        if data_listener:
            await data_listener.stop()

        # ArrÃªter le serveur HTTP
        await http_runner.cleanup()

        logger.info("Market Analyzer terminÃ©")


if __name__ == "__main__":
    # Obtenir la boucle asyncio
    loop = asyncio.get_event_loop()

    # Configurer les gestionnaires de signaux
    for sig in (signal.SIGINT, signal.SIGTERM):
        loop.add_signal_handler(
            sig, lambda s=sig: asyncio.create_task(shutdown(s, loop))  # type: ignore
        )

    try:
        # ExÃ©cuter la fonction principale
        loop.run_until_complete(main())
    except KeyboardInterrupt:
        logger.info("Interruption clavier dÃ©tectÃ©e")
    finally:
        # Fermer la boucle asyncio
        loop.close()
        logger.info("Boucle asyncio fermÃ©e")
