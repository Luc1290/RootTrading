"""
Market Analyzer Service - Point d'entr√©e principal
Service qui calcule TOUS les indicateurs techniques √† partir des donn√©es OHLCV.
Architecture propre : Gateway ‚Üí market_data ‚Üí Market Analyzer ‚Üí analyzer_data
"""

import asyncio
import signal
import sys
import time
from pathlib import Path

from aiohttp import web

from market_analyzer.src.data_listener import DataListener
from shared.src.config import SYMBOLS

# Ajouter les chemins pour les imports
sys.path.append(str(Path(__file__).parent / "../../"))


# Configuration du logging centralis√©e
from shared.logging_config import setup_logging

logger = setup_logging("market_analyzer", log_level="INFO")

# Variables globales
data_listener = None
running = True
start_time = time.time()

# Routes pour l'API de diagnostic
routes = web.RouteTableDef()


@routes.get("/health")
async def health_check(_request):
    """Point de terminaison sant√© du service."""
    uptime = time.time() - start_time
    return web.json_response(
        {
            "status": "ok",
            "timestamp": time.time(),
            "uptime": uptime,
            "service": "market_analyzer",
            "mode": "active" if running else "stopping",
            "symbols": SYMBOLS,
            "architecture": "clean_calculation_engine",
        }
    )


@routes.get("/stats")
async def get_stats(_request):
    """Statistiques du Market Analyzer."""
    if data_listener:
        stats = await data_listener.get_stats()
        return web.json_response(stats)

    return web.json_response(
        {"error": "DataListener not initialized", "status": "not_running"},
        status=503,
    )


@routes.post("/process-historical")
async def process_historical(request):
    """API pour traiter les donn√©es historiques."""
    if not data_listener:
        return web.json_response(
            {"error": "DataListener not initialized"}, status=503)

    try:
        # Parser les param√®tres
        data = (
            await request.json() if request.content_type == "application/json" else {}
        )
        symbol = data.get("symbol")
        timeframe = data.get("timeframe")
        limit = data.get("limit", 1000)

        # Lancer le traitement optimis√© en arri√®re-plan
        _task = asyncio.create_task(
            data_listener.process_historical_optimized(
                symbol, timeframe, limit))

        return web.json_response(
            {
                "status": "started",
                "message": f"Traitement historique lanc√© (limit: {limit})",
                "symbol": symbol,
                "timeframe": timeframe,
            }
        )

    except Exception as e:
        return web.json_response(
            {"error": f"Erreur lancement traitement: {e}"}, status=500
        )
    return None  # type: ignore


@routes.get("/coverage")
async def get_coverage(_request):
    """Analyse de couverture des donn√©es."""
    if not data_listener:
        return web.json_response(
            {"error": "DataListener not initialized"}, status=503)

    try:
        # Statistiques par symbole/timeframe
        coverage_query = """
            SELECT
                md.symbol,
                md.timeframe,
                COUNT(md.*) as market_data_count,
                COUNT(ad.*) as analyzer_data_count,
                ROUND((COUNT(ad.*)::FLOAT / COUNT(md.*)) * 100, 2) as coverage_percent
            FROM market_data md
            LEFT JOIN analyzer_data ad ON (
                md.symbol = ad.symbol AND
                md.timeframe = ad.timeframe AND
                md.time = ad.time
            )
            GROUP BY md.symbol, md.timeframe
            ORDER BY md.symbol, md.timeframe
        """

        async with data_listener.db_pool.acquire() as conn:
            rows = await conn.fetch(coverage_query)

        coverage_data = []
        for row in rows:
            coverage_data.append(
                {
                    "symbol": row["symbol"],
                    "timeframe": row["timeframe"],
                    "market_data_count": row["market_data_count"],
                    "analyzer_data_count": row["analyzer_data_count"],
                    "coverage_percent": float(row["coverage_percent"]),
                    "missing_count": row["market_data_count"]
                    - row["analyzer_data_count"],
                }
            )

        return web.json_response(
            {
                "coverage_by_asset": coverage_data,
                "total_assets": len({row["symbol"] for row in coverage_data}),
                "total_timeframes": len({row["timeframe"] for row in coverage_data}),
            }
        )

    except Exception as e:
        return web.json_response(
            {"error": f"Erreur analyse couverture: {e}"}, status=500
        )
    return None  # type: ignore


async def start_http_server():
    """D√©marre le serveur HTTP pour l'API."""
    app = web.Application()
    app.add_routes(routes)

    runner = web.AppRunner(app)
    await runner.setup()

    # Port 5020 pour le Market Analyzer
    site = web.TCPSite(runner, "0.0.0.0", 5020)
    await site.start()

    logger.info("‚úÖ API HTTP d√©marr√©e sur le port 5020")
    return runner


async def shutdown(signal_type, loop):
    """G√®re l'arr√™t propre du service."""
    global running

    logger.info(f"Signal {signal_type.name} re√ßu, arr√™t en cours...")
    running = False

    # Arr√™ter le DataListener AVANT de fermer la boucle
    if data_listener:
        try:
            await data_listener.stop()
            logger.info("‚úÖ DataListener arr√™t√© proprement")
        except Exception:
            logger.exception("‚ùå Erreur arr√™t DataListener")

    # Attendre plus longtemps pour que toutes les connexions DB se ferment
    await asyncio.sleep(1.0)

    # Annuler toutes les t√¢ches pendantes
    try:
        pending_tasks = [
            task
            for task in asyncio.all_tasks(loop)
            if not task.done() and task != asyncio.current_task()
        ]
        if pending_tasks:
            logger.info(
                f"Annulation de {len(pending_tasks)} t√¢ches pendantes...")
            for task in pending_tasks:
                task.cancel()
            # Attendre que les t√¢ches se terminent
            await asyncio.gather(*pending_tasks, return_exceptions=True)
    except Exception as e:
        logger.warning(f"Erreur lors de l'annulation des t√¢ches: {e}")

    logger.info("Market Analyzer termin√©")

    # NE PAS appeler loop.stop() ici - laisser le main() se terminer
    # naturellement


async def main():
    """Fonction principale du Market Analyzer."""
    global data_listener

    logger.info("üöÄ D√©marrage du Market Analyzer Service")
    logger.info("üéØ Architecture: Calcul TOUS les indicateurs depuis market_data")
    logger.info(f"üìä Symboles surveill√©s: {', '.join(SYMBOLS)}")

    # D√©marrer le serveur HTTP
    http_runner = await start_http_server()

    try:
        # Initialiser le DataListener
        data_listener = DataListener()
        await data_listener.initialize()

        logger.info("‚úÖ Market Analyzer initialis√©")

        # D√©marrer l'√©coute temps r√©el IMM√âDIATEMENT (non-bloquant)
        logger.info("üéß D√©marrage de l'√©coute temps r√©el...")
        _listening_task = asyncio.create_task(data_listener.start_listening())

        # Proposer de traiter l'historique en parall√®le
        logger.info("üîç V√©rification de la couverture des donn√©es...")
        stats = await data_listener.get_stats()

        if stats["missing_analyses"] > 0:
            logger.info(
                f"‚ö†Ô∏è {stats['missing_analyses']} donn√©es non analys√©es d√©tect√©es"
            )
            logger.info("üí° D√©marrage du traitement historique automatique...")

            # LIMIT√â √† 100k donn√©es max au d√©marrage pour √©viter saturation DB
            await data_listener.process_historical_optimized(limit=100000)
        else:
            logger.info("‚úÖ Toutes les donn√©es sont analys√©es")

        # Afficher les statistiques finales
        final_stats = await data_listener.get_stats()
        logger.info(
            f"üìä Couverture: {final_stats['coverage_percent']}% ({final_stats['total_analyzer_data']}/{final_stats['total_market_data']})"
        )

        # Attendre que l'√©coute temps r√©el continue ou que l'arr√™t soit demand√©
        logger.info(
            "‚úÖ Traitement historique termin√© - √©coute temps r√©el active")

        # Boucle d'attente au lieu d'attendre le listening_task qui peut ne
        # jamais se terminer
        while running:
            await asyncio.sleep(1)

        logger.info("üìü Arr√™t demand√© - terminaison en cours...")

    except Exception:
        logger.exception("‚ùå Erreur critique")
    finally:
        if data_listener:
            await data_listener.stop()

        # Arr√™ter le serveur HTTP
        await http_runner.cleanup()

        logger.info("Market Analyzer termin√©")


if __name__ == "__main__":
    # Obtenir la boucle asyncio
    loop = asyncio.get_event_loop()

    # Configurer les gestionnaires de signaux
    for sig in (signal.SIGINT, signal.SIGTERM):
        loop.add_signal_handler(
            sig, lambda s=sig: asyncio.create_task(
                shutdown(s, loop))  # type: ignore
        )

    try:
        # Ex√©cuter la fonction principale
        loop.run_until_complete(main())
    except KeyboardInterrupt:
        logger.info("Interruption clavier d√©tect√©e")
    finally:
        # Fermer la boucle asyncio
        loop.close()
        logger.info("Boucle asyncio ferm√©e")
